abstract
"Models of social learning feature either binary signals or abstract signal structures often deprived of micro-foundations. Both models are limited when analyzing interim results or performing empirical analysis. We present a method of generating signal structures which are richer than the binary model, yet are tractable enough to perform simulations and empirical analysis. We demonstrate the method's usability by revisiting two classical papers: (1) we discuss the economic significance of unbounded signals Smith and Sorensen (2000); (2) we use experimental data from Anderson and Holt (1997) to perform econometric analysis. Additionally, we provide a necessary and sufficient condition for the occurrence of action cascades."
"This paper derives conditions under which preferences and technology are nonparametrically identified in hedonic equilibrium models, where products are differentiated along more than one dimension and agents are characterized by several dimensions of unobserved heterogeneity. With products differentiated along a quality index and agents characterized by scalar unobserved heterogeneity, single crossing conditions on preferences and technology provide identifying restrictions in Ekeland, Heckman and Nesheim (2004) and Heckman, Matzkin and Nesheim (2010). We develop similar shape restrictions in the multi-attribute case. These shape restrictions, which are based on optimal transport theory and generalized convexity, allow us to identify preferences for goods differentiated along multiple dimensions, from the observation of a single market. We thereby derive nonparametric identification results for nonseparable simultaneous equations and multi-attribute hedonic equilibrium models with (possibly) multiple dimensions of unobserved heterogeneity. One of our results is a proof of absolute continuity of the distribution of endogenously traded qualities, which is of independent interest."
"To determine the welfare implications of price changes in demand data, we introduce a revealed preference relation over prices. We show that the absence of cycles in this relation characterizes a consumer who trades off the utility of consumption against the disutility of expenditure. Our model can be applied whenever a consumer's demand over a strict subset of all available goods is being analyzed; it can also be extended to settings with discrete goods and nonlinear prices. To illustrate its use, we apply our model to a single-agent data set and to a data set with repeated cross-sections. We develop a novel test of linear hypotheses on partially identified parameters to estimate the proportion of the population who are revealed better off due to a price change in the latter application. This new technique can be used for nonparametric counterfactual analysis more broadly."
"Consider a predictor who ranks eventualities on the basis of past cases: for instance a search engine ranking webpages given past searches. Resampling past cases leads to different rankings and the extraction of deeper information. Yet a rich database, with sufficiently diverse rankings, is often beyond reach. Inexperience demands either ""on the fly"" learning-by-doing or prudence: the arrival of a novel case does not force (i) a revision of current rankings, (ii) dogmatism towards new rankings, or (iii) intransitivity. For this higher-order framework of inductive inference, we derive a suitably unique numerical representation of these rankings via a matrix on eventualities x cases and describe a robust test of prudence. Applications include: the success/failure of startups; the veracity of fake news; and novel conditions for the existence of a yield curve that is robustly arbitrage-free."
"Timing decisions are common: when to file your taxes, finish a referee report, or complete a task at work. We ask whether time preferences can be inferred when \textsl{only} task completion is observed. To answer this question, we analyze the following model: each period a decision maker faces the choice whether to complete the task today or to postpone it to later. Cost and benefits of task completion cannot be directly observed by the analyst, but the analyst knows that net benefits are drawn independently between periods from a time-invariant distribution and that the agent has time-separable utility. Furthermore, we suppose the analyst can observe the agent's exact stopping probability. We establish that for any agent with quasi-hyperbolic $\beta,\delta$-preferences and given level of partial naivete $\hat{\beta}$, the probability of completing the task conditional on not having done it earlier increases towards the deadline. And conversely, for any given preference parameters $\beta,\delta$ and (weakly increasing) profile of task completion probability, there exists a stationary payoff distribution that rationalizes her behavior as long as the agent is either sophisticated or fully naive. An immediate corollary being that, without parametric assumptions, it is impossible to rule out time-consistency even when imposing an a priori assumption on the permissible long-run discount factor. We also provide an exact partial identification result when the analyst can, in addition to the stopping probability, observe the agent's continuation value."
"We analyze the household savings problem in a general setting where returns on assets, non-financial income and impatience are all state dependent and fluctuate over time. All three processes can be serially correlated and mutually dependent. Rewards can be bounded or unbounded and wealth can be arbitrarily large. Extending classic results from an earlier literature, we determine conditions under which (a) solutions exist, are unique and are globally computable, (b) the resulting wealth dynamics are stationary, ergodic and geometrically mixing, and (c) the wealth distribution has a Pareto tail. We show how these results can be used to extend recent studies of the wealth distribution. Our conditions have natural economic interpretations in terms of asymptotic growth rates for discounting and return on savings."
"We offer a rationalization of the weak generalized axiom of revealed preference (WGARP) for both finite and infinite data sets of consumer choice. We call it maximin rationalization, in which each pairwise choice is associated with a ""local"" utility function. We develop its associated weak revealed-preference theory. We show that preference recoverability and welfare analysis \`a la Varian (1982) may not be informative enough, when the weak axiom holds, but when consumers are not utility maximizers. We clarify the reasons for this failure and provide new informative bounds for the consumer's true preferences."
"We propose a framework for nonparametric identification and estimation of discrete choice models with unobserved choice sets. We recover the joint distribution of choice sets and preferences from a panel dataset on choices. We assume that either the latent choice sets are sparse or that the panel is sufficiently long. Sparsity requires the number of possible choice sets to be relatively small. It is satisfied, for instance, when the choice sets are nested, or when they form a partition. Our estimation procedure is computationally fast and uses mixed-integer optimization to recover the sparse support of choice sets. Analyzing the ready-to-eat cereal industry using a household scanner dataset, we find that ignoring the unobservability of choice sets can lead to biased estimates of preferences due to significant latent heterogeneity in choice sets."
"The drift diffusion model (DDM) is a model of sequential sampling with diffusion (Brownian) signals, where the decision maker accumulates evidence until the process hits a stopping boundary, and then stops and chooses the alternative that corresponds to that boundary. This model has been widely used in psychology, neuroeconomics, and neuroscience to explain the observed patterns of choice and response times in a range of binary choice decision problems. This paper provides a statistical test for DDM's with general boundaries. We first prove a characterization theorem: we find a condition on choice probabilities that is satisfied if and only if the choice probabilities are generated by some DDM. Moreover, we show that the drift and the boundary are uniquely identified. We then use our condition to nonparametrically estimate the drift and the boundary and construct a test statistic."
"We study preferences estimated from finite choice experiments and provide sufficient conditions for convergence to a unique underlying ""true"" preference. Our conditions are weak, and therefore valid in a wide range of economic environments. We develop applications to expected utility theory, choice over consumption bundles, menu choice and intertemporal consumption. Our framework unifies the revealed preference tradition with models that allow for errors."
"The empirical analysis of discrete complete-information games has relied on behavioral restrictions in the form of solution concepts, such as Nash equilibrium. Choosing the right solution concept is crucial not just for identification of payoff parameters, but also for the validity and informativeness of counterfactual exercises and policy implications. We say that a solution concept is discernible if it is possible to determine whether it generated the observed data on the players' behavior and covariates. We propose a set of conditions that make it possible to discern solution concepts. In particular, our conditions are sufficient to tell whether the players' choices emerged from Nash equilibria. We can also discern between rationalizable behavior, maxmin behavior, and collusive behavior. Finally, we identify the correlation structure of unobserved shocks in our model using a novel approach."
"To what extent can agents with misspecified subjective models predict false correlations? We study an ""analyst"" who utilizes models that take the form of a recursive system of linear regression equations. The analyst fits each equation to minimize the sum of squared errors against an arbitrarily large sample. We characterize the maximal pairwise correlation that the analyst can predict given a generic objective covariance matrix, subject to the constraint that the estimated model does not distort the mean and variance of individual variables. We show that as the number of variables in the model grows, the false pairwise correlation can become arbitrarily close to one, regardless of the true correlation."
"We consider a small set of axioms for income averaging -- recursivity, continuity, and the boundary condition for the present. These properties yield a unique averaging function that is the density of the reflected Brownian motion with a drift started at the current income and moving over the past incomes. When averaging is done over the short past, the weighting function is asymptotically converging to a Gaussian. When averaging is done over the long horizon, the weighing function converges to the exponential distribution. For all intermediate averaging scales, we derive an explicit solution that interpolates between the two."
"Complexity of the problem of choosing among uncertain acts is a salient feature of many of the environments in which departures from expected utility theory are observed. I propose and axiomatize a model of choice under uncertainty in which the size of the partition with respect to which an act is measurable arises endogenously as a measure of subjective complexity. I derive a representation of incomplete Simple Bounds preferences in which acts that are complex from the perspective of the decision maker are bracketed by simple acts to which they are related by statewise dominance. The key axioms are motivated by a model of learning from limited data. I then consider choice behavior characterized by a ""cautious completion"" of Simple Bounds preferences, and discuss the relationship between this model and models of ambiguity aversion. I develop general comparative statics results, and explore applications to portfolio choice, contracting, and insurance choice."
"An asset pricing model using long-run capital share growth risk has recently been found to successfully explain U.S. stock returns. Our paper adopts a recursive preference utility framework to derive an heterogeneous asset pricing model with capital share risks.While modeling capital share risks, we account for the elevated consumption volatility of high income stockholders. Capital risks have strong volatility effects in our recursive asset pricing model. Empirical evidence is presented in which capital share growth is also a source of risk for stock return volatility. We uncover contrasting unconditional and conditional asset pricing evidence for capital share risks."
"Becker (1973) presents a bilateral matching model in which scalar types describe agents. For this framework, he establishes the conditions under which positive sorting between agents' attributes is the unique market outcome. Becker's celebrated sorting result has been applied to address many economic questions. However, recent empirical studies in the fields of health, household, and labor economics suggest that agents have multiple outcome-relevant attributes. In this paper, I study a matching model with multidimensional types. I offer multidimensional generalizations of concordance and supermodularity to construct three multidimensional sorting patterns and two classes of multidimensional complementarities. For each of these sorting patterns, I identify the sufficient conditions which guarantee its optimality. In practice, we observe sorting patterns between observed attributes that are aggregated over unobserved characteristics. To reconcile theory with practice, I establish the link between production complementarities and the aggregated sorting patterns. Finally, I examine the relationship between agents' health status and their spouses' education levels among U.S. households within the framework for multidimensional matching markets. Preliminary analysis reveals a weak positive association between agents' health status and their spouses' education levels. This weak positive association is estimated to be a product of three factors: (a) an attraction between better-educated individuals, (b) an attraction between healthier individuals, and (c) a weak positive association between agents' health status and their education levels. The attraction channel suggests that the insurance risk associated with a two-person family plan is higher than the aggregate risk associated with two individual policies."
"We introduce the *decision-conflict logit*, a simple and disciplined extension of the logit with an outside option that assigns a menu-dependent utility to that option. The relative value of this utility at a menu could be interpreted as proxying decision difficulty and determines the probability of avoiding/delaying choice at that menu. We focus on two intuitively structured special cases of the model that offer complementary insights, and argue that they explain a variety of observed choice-deferral effects that are caused by hard decisions. We conclude by illustrating the usability of the proposed modelling framework in applications."
"The paper studies the problem of auction design in a setting where the auctioneer accesses the knowledge of the valuation distribution only through statistical samples. A new framework is established that combines the statistical decision theory with mechanism design. Two optimality criteria, maxmin, and equivariance, are studied along with their implications on the form of auctions. The simplest form of the equivariant auction is the average bid auction, which set individual reservation prices proportional to the average of other bids and historical samples. This form of auction can be motivated by the Gamma distribution, and it sheds new light on the estimation of the optimal price, an irregular parameter. Theoretical results show that it is often possible to use the regular parameter population mean to approximate the optimal price. An adaptive average bid estimator is developed under this idea, and it has the same asymptotic properties as the empirical Myerson estimator. The new proposed estimator has a significantly better performance in terms of value at risk and expected shortfall when the sample size is small."
"We propose a model of labor market sector self-selection that combines comparative advantage, as in the Roy model, and sector composition preference. Two groups choose between two sectors based on heterogeneous potential incomes and group compositions in each sector. Potential incomes incorporate group specific human capital accumulation and wage discrimination. Composition preferences are interpreted as reflecting group specific amenity preferences as well as homophily and aversion to minority status. We show that occupational segregation is amplified by the composition preferences and we highlight a resulting tension between redistribution and diversity. The model also exhibits tipping from extreme compositions to more balanced ones. Tipping occurs when a small nudge, associated with affirmative action, pushes the system to a very different equilibrium, and when the set of equilibria changes abruptly when a parameter governing the relative importance of pecuniary and composition preferences crosses a threshold."
"Economists often estimate economic models on data and use the point estimates as a stand-in for the truth when studying the model's implications for optimal decision-making. This practice ignores model ambiguity, exposes the decision problem to misspecification, and ultimately leads to post-decision disappointment. Using statistical decision theory, we develop a framework to explore, evaluate, and optimize robust decision rules that explicitly account for estimation uncertainty. We show how to operationalize our analysis by studying robust decisions in a stochastic dynamic investment model in which a decision-maker directly accounts for uncertainty in the model's transition dynamics."
We provide a sharp identification region for discrete choice models where consumers' preferences are not necessarily complete and only aggregate choice data is available. Behavior is modeled using an upper and a lower utility for each alternative so that non-comparability can arise. The identification region places intuitive bounds on the probability distribution of upper and lower utilities. We show that the existence of an instrumental variable can be used to reject the hypothesis that the preferences of all consumers are complete. We apply our methods to data from the 2018 mid-term elections in Ohio.
"Gambits are central to human decision-making. Our goal is to provide a theory of Gambits. A Gambit is a combination of psychological and technical factors designed to disrupt predictable play. Chess provides an environment to study gambits and behavioral game theory. Our theory is based on the Bellman optimality path for sequential decision-making. This allows us to calculate the $Q$-values of a Gambit where material (usually a pawn) is sacrificed for dynamic play. On the empirical side, we study the effectiveness of a number of popular chess Gambits. This is a natural setting as chess Gambits require a sequential assessment of a set of moves (a.k.a. policy) after the Gambit has been accepted. Our analysis uses Stockfish 14.1 to calculate the optimal Bellman $Q$ values, which fundamentally measures if a position is winning or losing. To test whether Bellman's equation holds in play, we estimate the transition probabilities to the next board state via a database of expert human play. This then allows us to test whether the \emph{Gambiteer} is following the optimal path in his decision-making. Our methodology is applied to the popular Stafford and reverse Stafford (a.k.a. Boden-Kieretsky-Morphy) Gambit and other common ones including the Smith-Morra, Goring, Danish and Halloween Gambits. We build on research in human decision-making by proving an irrational skewness preference within agents in chess. We conclude with directions for future research."
"We introduce an Attention Overload Model that captures the idea that alternatives compete for the decision maker's attention, and hence the attention that each alternative receives decreases as the choice problem becomes larger. We provide testable implications on the observed choice behavior that can be used to (point or partially) identify the decision maker's preference and attention frequency. We then enhance our attention overload model to accommodate heterogeneous preferences based on the idea of List-based Attention Overload, where alternatives are presented to the decision makers as a list that correlates with both heterogeneous preferences and random attention. We show that preference and attention frequencies are (point or partially) identifiable under nonparametric assumptions on the list and attention formation mechanisms, even when the true underlying list is unknown to the researcher. Building on our identification results, we develop econometric methods for estimation and inference."
"Humans exhibit irrational decision-making patterns in response to environmental triggers, such as experiencing an economic loss or gain. In this paper we investigate whether algorithms exhibit the same behavior by examining the observed decisions and latent risk and rationality parameters estimated by a random utility model with constant relative risk-aversion utility function. We use a dataset consisting of 10,000 hands of poker played by Pluribus, the first algorithm in the world to beat professional human players and find (1) Pluribus does shift its playing style in response to economic losses and gains, ceteris paribus; (2) Pluribus becomes more risk-averse and rational following a trigger but the humans become more risk-seeking and irrational; (3) the difference in playing styles between Pluribus and the humans on the dimensions of risk-aversion and rationality are particularly differentiable when both have experienced a trigger. This provides support that decision-making patterns could be used as ""behavioral signatures"" to identify human versus algorithmic decision-makers in unlabeled contexts."
"We provide the first behavioral characterization of nested logit, a foundational and widely applied discrete choice model, through the introduction of a non-parametric version of nested logit that we call Nested Stochastic Choice (NSC). NSC is characterized by a single axiom that weakens Independence of Irrelevant Alternatives based on revealed similarity to allow for the similarity effect. Nested logit is characterized by an additional menu-independence axiom. Our axiomatic characterization leads to a practical, data-driven algorithm that identifies the true nest structure from choice data. We also discuss limitations of generalizing nested logit by studying the testable implications of cross-nested logit."
"Time series that display periodicity can be described with a Fourier expansion. In a similar vein, a recently developed formalism enables description of growth patterns with the optimal number of parameters (Elitzur et al, 2020). The method has been applied to the growth of national GDP, population and the COVID-19 pandemic; in all cases the deviations of long-term growth patterns from pure exponential required no more than two additional parameters, mostly only one. Here I utilize the new framework to develop a unified formulation for all functions that describe growth deceleration, wherein the growth rate decreases with time. The result offers the prospects for a new general tool for trend removal in time-series analysis."
"Economists often estimate models using data from a particular domain, e.g. estimating risk preferences in a particular subject pool or for a specific class of lotteries. Whether a model's predictions extrapolate well across domains depends on whether the estimated model has captured generalizable structure. We provide a tractable formulation for this ""out-of-domain"" prediction problem and define the transfer error of a model based on how well it performs on data from a new domain. We derive finite-sample forecast intervals that are guaranteed to cover realized transfer errors with a user-selected probability when domains are iid, and use these intervals to compare the transferability of economic models and black box algorithms for predicting certainty equivalents. We find that in this application, the black box algorithms we consider outperform standard economic models when estimated and tested on data from the same domain, but the economic models generalize across domains better than the black-box algorithms do."
"In many first-price auctions, bidders face considerable strategic uncertainty: They cannot perfectly anticipate the other bidders' bidding behavior. We propose a model in which bidders do not know the entire distribution of opponent bids but only the expected (winning) bid and lower and upper bounds on the opponent bids. We characterize the optimal bidding strategies and prove the existence of equilibrium beliefs. Finally, we apply the model to estimate the cost distribution in highway procurement auctions and find good performance out-of-sample."
"We obtain a necessary and sufficient condition under which random-coefficient discrete choice models, such as mixed-logit models, are rich enough to approximate any nonparametric random utility models arbitrarily well across choice sets. The condition turns out to be the affine-independence of the set of characteristic vectors. When the condition fails, resulting in some random utility models that cannot be closely approximated, we identify preferences and substitution patterns that are challenging to approximate accurately. We also propose algorithms to quantify the magnitude of approximation errors."
"When sample data are governed by an unknown sequence of independent but possibly non-identical distributions, the data-generating process (DGP) in general cannot be perfectly identified from the data. For making decisions facing such uncertainty, this paper presents a novel approach by studying how the data can best be used to robustly improve decisions. That is, no matter which DGP governs the uncertainty, one can make a better decision than without using the data. I show that common inference methods, e.g., maximum likelihood and Bayesian updating cannot achieve this goal. To address, I develop new updating rules that lead to robustly better decisions either asymptotically almost surely or in finite sample with a pre-specified probability. Especially, they are easy to implement as are given by simple extensions of the standard statistical procedures in the case where the possible DGPs are all independent and identically distributed. Finally, I show that the new updating rules also lead to more intuitive conclusions in existing economic models such as asset pricing under ambiguity."
"We conduct an incentivized experiment on a nationally representative US sample \\ (N=708) to test whether people prefer to avoid ambiguity even when it means choosing dominated options. In contrast to the literature, we find that 55\% of subjects prefer a risky act to an ambiguous act that always provides a larger probability of winning. Our experimental design shows that such a preference is not mainly due to a lack of understanding. We conclude that subjects avoid ambiguity \textit{per se} rather than avoiding ambiguity because it may yield a worse outcome. Such behavior cannot be reconciled with existing models of ambiguity aversion in a straightforward manner."
"Motivated by growing evidence of agents' mistakes in strategically simple environments, we propose a solution concept -- robust equilibrium -- that requires only an asymptotically optimal behavior. We use it to study large random matching markets operated by the applicant-proposing Deferred Acceptance (DA). Although truth-telling is a dominant strategy, almost all applicants may be non-truthful in robust equilibrium; however, the outcome must be arbitrarily close to the stable matching. Our results imply that one can assume truthful agents to study DA outcomes, theoretically or counterfactually. However, to estimate the preferences of mistaken agents, one should assume stable matching but not truth-telling."
"Historically, testing if decision-makers obey certain choice axioms using choice data takes two distinct approaches. The 'functional' approach observes and tests the entire 'demand' or 'choice' function, whereas the 'revealed preference(RP)' approach constructs inequalities to test finite choices. I demonstrate that a statistical recasting of the revealed enables uniting both approaches. Specifically, I construct a computationally efficient algorithm to output one-sided statistical tests of choice data from functional characterizations of axiomatic behavior, thus linking statistical and RP testing. An application to weakly separable preferences, where RP characterizations are provably NP-Hard, demonstrates the approach's merit. I also show that without assuming monotonicity, all restrictions disappear. Hence, any ability to resolve axiomatic behavior relies on the monotonicity assumption."
"The objective of this paper is to identify and analyze the response actions of a set of players embedded in sub-networks in the context of interaction and learning. We characterize strategic network formation as a static game of interactions where players maximize their utility depending on the connections they establish and multiple interdependent actions that permit group-specific parameters of players. It is challenging to apply this type of model to real-life scenarios for two reasons: The computation of the Bayesian Nash Equilibrium is highly demanding and the identification of social influence requires the use of excluded variables that are oftentimes unavailable. Based on the theoretical proposal, we propose a set of simulant equations and discuss the identification of the social interaction effect employing multi-modal network autoregressive."
"Welfare effects of price changes are often estimated with cross-sections; these do not identify demand with heterogeneous consumers. We develop a theoretical method addressing this, utilizing uncompensated demand moments to construct local approximations for compensated demand moments, robust to unobserved preference heterogeneity. Our methodological contribution offers robust approximations for average and distributional welfare estimates, extending to price indices, taxable income elasticities, and general equilibrium welfare. Our methods apply to any cross-section; we demonstrate them via UK household budget survey data. We uncover an insight: simple non-parametric representative agent models might be less biased than complex parametric models accounting for heterogeneity."
"An analyst observes the frequency with which an agent takes actions, but not the frequency with which she takes actions conditional on a payoff relevant state. In this setting, we ask when the analyst can rationalize the agent's choices as the outcome of the agent learning something about the state before taking action. Our characterization marries the obedience approach in information design (Bergemann and Morris, 2016) and the belief approach in Bayesian persuasion (Kamenica and Gentzkow, 2011) relying on a theorem by Strassen (1965) and Hall's marriage theorem. We apply our results to ring-network games and to identify conditions under which a data set is consistent with a public information structure in first-order Bayesian persuasion games."
"Systematically biased forecasts are typically interpreted as evidence of forecasters' irrationality and/or asymmetric loss. In this paper we propose an alternative explanation: when forecasts inform economic policy decisions, and the resulting actions affect the realization of the forecast target itself, forecasts may be optimally biased even under quadratic loss. The result arises in environments in which the forecaster is uncertain about the decision maker's reaction to the forecast, which is presumably the case in most applications. We illustrate the empirical relevance of our theory by reviewing some stylized properties of Green Book inflation forecasts and relating them to the predictions from our model. Our results point out that the presence of policy feedback poses a challenge to traditional tests of forecast rationality."
"A decision-maker (DM) faces uncertainty governed by a data-generating process (DGP), which is only known to belong to a set of sequences of independent but possibly non-identical distributions. A robust decision maximizes the DM's expected payoff against the worst possible DGP in this set. This paper studies how such robust decisions can be improved with data, where improvement is measured by expected payoff under the true DGP. In this paper, I fully characterize when and how such an improvement can be guaranteed under all possible DGPs and develop inference methods to achieve it. These inference methods are needed because, as this paper shows, common inference methods (e.g., maximum likelihood or Bayesian) often fail to deliver such an improvement. Importantly, the developed inference methods are given by simple augmentations to standard inference procedures, and are thus easy to implement in practice."
"We use house prices (HP) and house price indices (HPI) as a proxy to income distribution. Specifically, we analyze sale prices in the 1970-2010 window of over 116,000 single-family homes in Hamilton County, Ohio, including Cincinnati metro area of about 2.2 million people. We also analyze HPI, published by Federal Housing Finance Agency (FHFA), for nearly 18,000 US ZIP codes that cover a period of over 40 years starting in 1980's. If HP can be viewed as a first derivative of income, HPI can be viewed as its second derivative. We use generalized beta (GB) family of functions to fit distributions of HP and HPI since GB naturally arises from the models of economic exchange described by stochastic differential equations. Our main finding is that HP and multi-year HPI exhibit a negative Dragon King (nDK) behavior, wherein power-law distribution tail gives way to an abrupt decay to a finite upper limit value, which is similar to our recent findings for realized volatility of S\&P500 index in the US stock market. This type of tail behavior is best fitted by a modified GB (mGB) distribution. Tails of single-year HPI appear to show more consistency with power-law behavior, which is better described by a GB Prime (GB2) distribution. We supplement full distribution fits by mGB and GB2 with direct linear fits (LF) of the tails. Our numerical procedure relies on evaluation of confidence intervals (CI) of the fits, as well as of p-values that give the likelihood that data come from the fitted distributions."
"We add the assumption that players know their opponents' payoff functions and rationality to a model of non-equilibrium learning in signaling games. Agents are born into player roles and play against random opponents every period. Inexperienced agents are uncertain about the prevailing distribution of opponents' play, but believe that opponents never choose conditionally dominated strategies. Agents engage in active learning and update beliefs based on personal observations. Payoff information can refine or expand learning predictions, since patient young senders' experimentation incentives depend on which receiver responses they deem plausible. We show that with payoff knowledge, the limiting set of long-run learning outcomes is bounded above by rationality-compatible equilibria (RCE), and bounded below by uniform RCE. RCE refine the Intuitive Criterion (Cho and Kreps, 1987) and include all divine equilibria (Banks and Sobel, 1987). Uniform RCE sometimes but not always exists, and implies universally divine equilibrium."
"Player-Compatible Equilibrium (PCE) imposes cross-player restrictions on the magnitudes of the players' ""trembles"" onto different strategies. These restrictions capture the idea that trembles correspond to deliberate experiments by agents who are unsure of the prevailing distribution of play. PCE selects intuitive equilibria in a number of examples where trembling-hand perfect equilibrium (Selten, 1975) and proper equilibrium (Myerson, 1978) have no bite. We show that rational learning and weighted fictitious play imply our compatibility restrictions in a steady-state setting."
"In this paper, we build a computational model for the analysis of international wheat spot price formation, its dynamics and the dynamics of internationally exchanged quantities. The model has been calibrated using FAOSTAT data to evaluate its in-sample predictive power. The model is able to generate wheat prices in twelve international markets and wheat used quantities in twenty-four world regions. The time span considered goes from 1992 to 2013. In our study, a particular attention was paid to the impact of Russian Federation's 2010 grain export ban on wheat price and internationally traded quantities. Among other results, we find that wheat average weighted world price in 2013 would have been 3.55\% lower than the observed one if the Russian Federation would not have imposed the export ban in 2010."
"The principle that rational agents should maximize expected utility or choiceworthiness is intuitively plausible in many ordinary cases of decision-making under uncertainty. But it is less plausible in cases of extreme, low-probability risk (like Pascal's Mugging), and intolerably paradoxical in cases like the St. Petersburg and Pasadena games. In this paper I show that, under certain conditions, stochastic dominance reasoning can capture most of the plausible implications of expectational reasoning while avoiding most of its pitfalls. Specifically, given sufficient background uncertainty about the choiceworthiness of one's options, many expectation-maximizing gambles that do not stochastically dominate their alternatives ""in a vacuum"" become stochastically dominant in virtue of that background uncertainty. But, even under these conditions, stochastic dominance will not require agents to accept options whose expectational superiority depends on sufficiently small probabilities of extreme payoffs. The sort of background uncertainty on which these results depend looks unavoidable for any agent who measures the choiceworthiness of her options in part by the total amount of value in the resulting world. At least for such agents, then, stochastic dominance offers a plausible general principle of choice under uncertainty that can explain more of the apparent rational constraints on such choices than has previously been recognized."
"The purpose is to compare the perfect Stochastic Return (SR) model like Islamic banks to the Fixed Return (FR) model as in conventional banks by measuring up their impacts at the macroeconomic level. We prove that if the optimal choice of investor share in SR model {\alpha}* realizes the indifference of the financial institution toward SR and FR models, there exists {\alpha} less than {\alpha}* such that the banks strictly prefers the SR model. Also, there exists {\alpha}, {\gamma} and {\lambda} verifying the conditions of {\alpha}-sharing such that each party in economy can be better under the SR model and the economic welfare could be improved in a Pareto-efficient way."
"An experimenter seeks to learn a subject's preference relation. The experimenter produces pairs of alternatives. For each pair, the subject is asked to choose. We argue that, in general, large but finite data do not give close approximations of the subject's preference, even when the limiting (countably infinite) data are enough to infer the preference perfectly. We provide sufficient conditions on the set of alternatives, preferences, and sequences of pairs so that the observation of finitely many choices allows the experimenter to learn the subject's preference with arbitrary precision. While preferences can be identified under our sufficient conditions, we show that it is harder to identify utility functions. We illustrate our results with several examples, including consumer choice, expected utility, and preferences in the Anscombe-Aumann model."
"A strictly strategy-proof mechanism is one that asks agents to use strictly dominant strategies. In the canonical one-dimensional mechanism design setting with private values, we show that strict strategy-proofness is equivalent to strict monotonicity plus the envelope formula, echoing a well-known characterisation of (weak) strategy-proofness. A consequence is that strategy-proofness can be made strict by an arbitrarily small modification, so that strictness is 'essentially for free'."
"Dynamic Random Subjective Expected Utility (DR-SEU) allows to model choice data observed from an agent or a population of agents whose beliefs about objective payoff-relevant states and tastes can both evolve stochastically. Our observable, the augmented Stochastic Choice Function (aSCF) allows, in contrast to previous work in decision theory, for a direct test of whether the agent's beliefs reflect the true data-generating process conditional on their private information as well as identification of the possibly incorrect beliefs. We give an axiomatic characterization of when an agent satisfies the model, both in a static as well as in a dynamic setting. We look at the case when the agent has correct beliefs about the evolution of objective states as well as at the case when her beliefs are incorrect but unforeseen contingencies are impossible.   We also distinguish two subvariants of the dynamic model which coincide in the static setting: Evolving SEU, where a sophisticated agent's utility evolves according to a Bellman equation and Gradual Learning, where the agent is learning about her taste. We prove easy and natural comparative statics results on the degree of belief incorrectness as well as on the speed of learning about taste.   Auxiliary results contained in the online appendix extend previous decision theory work in the menu choice and stochastic choice literature from a technical as well as a conceptual perspective."
"We establish that statistical discrimination is possible if and only if it is impossible to uniquely identify the signal structure observed by an employer from a realized empirical distribution of skills. The impossibility of statistical discrimination is shown to be equivalent to the existence of a fair, skill-dependent, remuneration for workers. Finally, we connect the statistical discrimination literature to Bayesian persuasion, establishing that if discrimination is absent, then the optimal signaling problem results in a linear payoff function (as well as a kind of converse)."
"Under the same assumptions made by Mas-Colell et al. (1995), I develop a short, simple, and complete proof of existence of equilibrium prices based on excess demand functions. The result is obtained by applying the Brouwer fixed point theorem to a trimmed simplex which does not contain prices equal to zero. The mathematical techniques are based on some results obtained in Neuefeind (1980) and Geanakoplos (2003)."
"News utility is the idea that the utility of an agent depends on changes in her beliefs over consumption and money. We introduce news utility into otherwise classical static Bayesian mechanism design models. We show that a key role is played by the timeline of the mechanism, i.e. whether there are delays between the announcement stage, the participation stage, the play stage and the realization stage of a mechanism. Depending on the timing, agents with news utility can experience two additional news utility effects: a surprise effect derived from comparing to pre-mechanism beliefs, as well as a realization effect derived from comparing post-play beliefs with the actual outcome of the mechanism.   We look at two distinct mechanism design settings reflecting the two main strands of the classical literature. In the first model, a monopolist screens an agent according to the magnitude of her loss aversion. In the second model, we consider a general multi-agent Bayesian mechanism design setting where the uncertainty of each player stems from not knowing the intrinsic types of the other agents. We give applications to auctions and public good provision which illustrate how news utility changes classical results.   For both models we characterize the optimal design of the timeline. A timeline featuring no delay between participation and play but a delay in realization is never optimal in either model. In the screening model the optimal timeline is one without delays. In auction settings, under fairly natural assumptions the optimal timeline has delays between all three stages of the mechanism."
"This paper establishes an interesting link between $k$th price auctions and Catalan numbers by showing that for distributions that have linear density, the bid function at any symmetric, increasing equilibrium of a $k$th price auction with $k\geq 3$ can be represented as a finite series of $k-2$ terms whose $\ell$th term involves the $\ell$th Catalan number. Using an integral representation of Catalan numbers, together with some classical combinatorial identities, we derive the closed form of the unique symmetric, increasing equilibrium of a $k$th price auction for a non-uniform distribution."
"Several structural results for the set of competitive equilibria in trading networks with frictions are established: The lattice theorem, the rural hospitals theorem, the existence of side-optimal equilibria, and a group-incentive-compatibility result hold with imperfectly transferable utility and in the presence of frictions. While our results are developed in a trading network model, they also imply analogous (and new) results for exchange economies with combinatorial demand and for two-sided matching markets with transfers."
"We study a repeated game with payoff externalities and observable actions where two players receive information over time about an underlying payoff-relevant state, and strategically coordinate their actions. Players learn about the true state from private signals, as well as the actions of others. They commonly learn the true state (Cripps et al., 2008), but do not coordinate in every equilibrium. We show that there exist stable equilibria in which players can overcome unfavorable signal realizations and eventually coordinate on the correct action, for any discount factor. For high discount factors, we show that in addition players can also achieve efficient payoffs."
"We study the indirect cost of information from sequential information cost minimization. A key sub-additivity condition, together with monotonicity equivalently characterizes the class of indirect cost functions generated from any direct information cost. Adding an extra (uniform) posterior separability condition equivalently characterizes the indirect cost generated from any direct cost favoring incremental evidences. We also provide the necessary and sufficient condition when prior independent direct cost generates posterior separable indirect cost."
"This paper considers the core of a competitive market economy with an endogenous social division of labour. The theory is founded on the notion of a ""consumer-producer"", who consumes as well as produces commodities. First, we show that the Core of such an economy with an endogenous social division of labour can be founded on deviations of coalitions of arbitrary size, extending the seminal insights of Vind and Schmeidler for pure exchange economies. Furthermore, we establish the equivalence between the Core and the set of competitive equilibria for continuum economies with an endogenous social division of labour. Our analysis also concludes that self-organisation in a social division of labour can be incorporated into the Edgeworthian barter process directly. This is formulated as a Core equivalence result stated for a Structured Core concept based on renegotiations among fully specialised economic agents, i.e., coalitions that use only fully developed internal divisions of labour. Our approach bridges the gap between standard economies with social production and coalition production economies. Therefore, a more straightforward and natural interpretation of coalitional improvement and the Core can be developed than for coalition production economies."
"We consider a symmetric two-player contest, in which the choice set of effort is constrained. We apply a fundamental property of the payoff function to show that, under standard assumptions, there exists a unique Nash equilibrium in pure strategies. It is shown that all equilibria are near the unconstrained equilibrium. Perhaps surprisingly, this is not the case when players have different prize evaluations."
"I consider the sequential implementation of a target information structure. I characterize the set of decision time distributions induced by all signal processes that satisfy a per-period learning capacity constraint. I find that all decision time distributions have the same expectation, and the maximal and minimal elements by mean-preserving spread order are deterministic distribution and exponential distribution. The result implies that when time preference is risk loving (e.g. standard or hyperbolic discounting), Poisson signal is optimal since it induces the most risky exponential decision time distribution. When time preference is risk neutral (e.g. constant delay cost), all signal processes are equally optimal."
"Many online shops offer functionality that help their customers navigate the available alternatives. For instance, options to filter and to sort goods are wide-spread. In this paper we show that sorting and filtering can be used by rational consumers to find their most preferred choice -- quickly. We characterize the preferences which can be expressed through filtering and sorting and show that these preferences exhibit a simple and intuitive logical structure."
"I consider the monopolistic pricing of informational good. A buyer's willingness to pay for information is from inferring the unknown payoffs of actions in decision making. A monopolistic seller and the buyer each observes a private signal about the payoffs. The seller's signal is binary and she can commit to sell any statistical experiment of her signal to the buyer. Assuming that buyer's decision problem involves rich actions, I characterize the profit maximizing menu. It contains a continuum of experiments, each containing different amount of information. I also find a complementarity between buyer's private information and information provision: when buyer's private signal is more informative, the optimal menu contains more informative experiments."
"We study dynamic matching in exchange markets with easy- and hard-to-match agents. A greedy policy, which attempts to match agents upon arrival, ignores the positive externality that waiting agents generate by facilitating future matchings. We prove that this trade-off between a ``thicker'' market and faster matching vanishes in large markets; A greedy policy leads to shorter waiting times, and more agents matched than any other policy. We empirically confirm these findings in data from the National Kidney Registry. Greedy matching achieves as many transplants as commonly-used policies (1.6\% more than monthly-batching), and shorter patient waiting times."
"In this paper, we show that the presence of the Archimedean and the mixture-continuity properties of a binary relation, both empirically non-falsifiable in principle, foreclose the possibility of consistency (transitivity) without decisiveness (completeness), or decisiveness without consistency, or in the presence of a weak consistency condition, neither. The basic result can be sharpened when specialized from the context of a generalized mixture set to that of a mixture set in the sense of Herstein-Milnor (1953). We relate the results to the antecedent literature, and view them as part of an investigation into the interplay of the structure of the choice space and the behavioral assumptions on the binary relation defined on it; the ES research program due to Eilenberg (1941) and Sonnenschein (1965), and one to which Schmeidler (1971) is an especially influential contribution."
"A ""statistician"" takes an action on behalf of an agent, based on the agent's self-reported personal data and a sample involving other people. The action that he takes is an estimated function of the agent's report. The estimation procedure involves model selection. We ask the following question: Is truth-telling optimal for the agent given the statistician's procedure? We analyze this question in the context of a simple example that highlights the role of model selection. We suggest that our simple exercise may have implications for the broader issue of human interaction with ""machine learning"" algorithms."
"Healthy nutrition promotions and regulations have long been regarded as a tool for increasing social welfare. One of the avenues taken in the past decade is sugar consumption regulation by introducing a sugar tax. Such a tax increases the price of extensive sugar containment in products such as soft drinks. In this article we consider a typical problem of optimal regulatory policy design, where the task is to determine the sugar tax rate maximizing the social welfare. We model the problem as a sequential game represented by the three-level mathematical program. On the upper level, the government decides upon the tax rate. On the middle level, producers decide on the product pricing. On the lower level, consumers decide upon their preferences towards the products. While the general problem is computationally intractable, the problem with a few product types is polynomially solvable, even for an arbitrary number of heterogeneous consumers. This paper presents a simple, intuitive and easily implementable framework for computing optimal sugar tax in a market with a few products. This resembles the reality as the soft drinks, for instance, are typically categorized in either regular or no-sugar drinks, e.g. Coca-Cola and Coca-Cola Zero. We illustrate the algorithm using an example based on the real data and draw conclusions for a specific local market."
"Although the integration of two-sided matching markets using stable mechanisms generates expected gains from integration, I show that there are worst-case scenarios in which these are negative. The losses from integration can be large enough that the average rank of an agent's spouse decreases by 37.5% of the length of their preference list in any stable matching mechanism."
"McFadden and Richter (1991) and later McFadden (2005) show that the Axiom of Revealed Stochastic Preference characterizes rationalizability of choice probabilities through random utility models on finite universal choice spaces. This note proves the result in one short, elementary paragraph and extends it to set valued choice. The latter requires a different axiom than is reported in McFadden (2005)."
"This paper studies a robust version of the classic surplus extraction problem, in which the designer knows only that the beliefs of each type belong to some set, and designs mechanisms that are suitable for all possible beliefs in that set. We derive necessary and sufficient conditions for full extraction in this setting, and show that these are natural set-valued analogues of the classic convex independence condition identified by Cremer and McLean (1985, 1988). We show that full extraction is neither generically possible nor generically impossible, in contrast to the standard setting in which full extraction is generic. When full extraction fails, we show that natural additional conditions can restrict both the nature of the contracts a designer can offer and the surplus the designer can obtain."
"We characterize three interrelated concepts in epistemic game theory: permissibility, proper rationalizability, and iterated admissibility. We define the lexicographic epistemic model for a game with incomplete information. Based on it, we give two groups of characterizations. The first group characterizes permissibility and proper rationalizability. The second group characterizes permissibility in an alternative way and iterated admissibility. In each group, the conditions for the latter are stronger than those for the former, which corresponds to the fact that proper rationalizability and iterated admissibility are two (compatible) refinements of permissibility within the complete information framework. The intrinsic difference between the two groups are the role of rationality: the first group does not need it, while the second group does."
"We develop a tool akin to the revelation principle for dynamic mechanism-selection games in which the designer can only commit to short-term mechanisms. We identify a canonical class of mechanisms rich enough to replicate the outcomes of any equilibrium in a mechanism-selection game between an uninformed designer and a privately informed agent. A cornerstone of our methodology is the idea that a mechanism should encode not only the rules that determine the allocation, but also the information the designer obtains from the interaction with the agent. Therefore, how much the designer learns, which is the key tension in design with limited commitment, becomes an explicit part of the design. Our result simplifies the search for the designer-optimal outcome by reducing the agent's behavior to a series of participation, truthtelling, and Bayes' plausibility constraints the mechanisms must satisfy."
"We provide tools to analyze information design problems subject to constraints. We do so by extending the insight in Le Treust and Tomala (2019) to the case of multiple inequality and equality constraints. Namely, that an information design problem subject to constraints can be represented as an unconstrained information design problem with a additional states, one for each constraint. Thus, without loss of generality, optimal solutions induce as many posteriors as the number of states and constraints. We provide results that refine this upper bound. Furthermore, we provide conditions under which there is no duality gap in constrained information design, thus validating a Lagrangian approach. We illustrate our results with applications to mechanism design with limited commitment (Doval and Skreta, 2022a) and persuasion of a privately informed receiver (Kolotilin et al., 2017)."
"We formalize the argument that political disagreements can be traced to a ""clash of narratives"". Drawing on the ""Bayesian Networks"" literature, we model a narrative as a causal model that maps actions into consequences, weaving a selection of other random variables into the story. An equilibrium is defined as a probability distribution over narrative-policy pairs that maximizes a representative agent's anticipatory utility, capturing the idea that public opinion favors hopeful narratives. Our equilibrium analysis sheds light on the structure of prevailing narratives, the variables they involve, the policies they sustain and their contribution to political polarization."
"People employ their knowledge to recognize things. This paper is concerned with how to measure people's knowledge for recognition and how it changes. The discussion is based on three assumptions. Firstly, we construct two evolution process equations, of which one is for uncertainty and knowledge, and the other for uncertainty and ignorance. Secondly, by solving the equations, formulas for measuring the levels of knowledge and the levels of ignorance are obtained in two particular cases. Thirdly, a new concept of knowledge entropy is introduced. Its similarity with Boltzmann's entropy and its difference with Shannon's Entropy are examined. Finally, it is pointed out that the obtained formulas of knowledge and knowledge entropy reflect two fundamental principles: (1) The knowledge level of a group is not necessarily a simple sum of the individuals' knowledge levels; and (2) An individual's knowledge entropy never increases if the individual's thirst for knowledge never decreases."
"We investigate whether fairness is compatible with efficiency in economies with multi-self agents, who may not be able to integrate their multiple objectives into a single complete and transitive ranking. We adapt envy-freeness, egalitarian-equivalence and the fair-share guarantee in two different ways. An allocation is unambiguously-fair if it satisfies the chosen criterion of fairness according to every objective of any agent; it is aggregate-fair if it satisfies the criterion for some aggregation of each agent's objectives.   While efficiency is always compatible with the unambiguous fair-share guarantee, it is incompatible with unambiguous envy-freeness in economics with at least three agents. Two agents are enough for efficiency and unambiguous egalitarian-equivalence to clash. Efficiency and the unambiguous fair-share guarantee can be attained together with aggregate envy-freeness, or aggregate egalitarian-equivalence."
"The observed proportionality between nominal prices and average embodied energies cannot be interpreted with conventional economic theory. A model is presented that places energy transfers as the focal point of scarcity based on the idea that (1) goods are material rearrangements, and (2) humans can only rearrange matter with energy transfers. Modified consumer and producer problems for an autarkic agent show that the opportunity cost of goods are given by their marginal energy transfers, which depend on subjective and objective factors (e.g. consumer preferences and direct energy transfers). Allowing for exchange and under perfect competition, nominal prices arise as social manifestations of goods' marginal energy transfers. The proportionality between nominal prices and average embodied energy follows given the relation between the latter and marginal energy transfers."
"We suggest that one individual holds multiple degrees of belief about an outcome, given the evidence. We then investigate the implications of such noisy probabilities for a buyer and a seller of binary options and find the odds agreed upon to ensure zero-expectation betting, differ from those consistent with the relative frequency of outcomes. More precisely, the buyer and the seller agree to odds that are higher (lower) than the reciprocal of their averaged unbiased probabilities when this average indicates the outcome is more (less) likely to occur than chance. The favorite-longshot bias thereby emerges to establish the foundation of an equitable market. As corollaries, our work suggests the old-established way of revealing someone's degree of belief through wagers may be more problematic than previously thought, and implies that betting markets cannot generally promise to support rational decisions."
"We define and investigate a property of mechanisms that we call ""strategic simplicity,"" and that is meant to capture the idea that, in strategically simple mechanisms, strategic choices require limited strategic sophistication. We define a mechanism to be strategically simple if choices can be based on first-order beliefs about the other agents' preferences and first-order certainty about the other agents' rationality alone, and there is no need for agents to form higher-order beliefs, because such beliefs are irrelevant to the optimal strategies. All dominant strategy mechanisms are strategically simple. But many more mechanisms are strategically simple. In particular, strategically simple mechanisms may be more flexible than dominant strategy mechanisms in the bilateral trade problem and the voting problem."
"This paper studies the income fluctuation problem with capital income risk (i.e., dispersion in the rate of return to wealth). Wealth returns and labor earnings are allowed to be serially correlated and mutually dependent. Rewards can be bounded or unbounded. Under rather general conditions, we develop a set of new results on the existence and uniqueness of solutions, stochastic stability of the model economy, as well as efficient computation of the ergodic wealth distribution. A variety of applications are discussed. Quantitative analysis shows that both stochastic volatility and mean persistence in wealth returns have nontrivial impact on wealth inequality."
"In group decision making, the preference map and Cook-Seiford vector are two concepts as ways of describing ties-permitted ordinal rankings. This paper shows that they are equivalent for representing ties-permitted ordinal rankings. Transformation formulas from one to the other are given and the inherent consistency of the mutual conversion is discussed. The proposed methods are illustrated by some examples. Some possible future applications of the proposed formulas are also pointed out."
"We develop an axiomatic theory of information acquisition that captures the idea of constant marginal costs in information production: the cost of generating two independent signals is the sum of their costs, and generating a signal with probability half costs half its original cost. Together with Blackwell monotonicity and a continuity condition, these axioms determine the cost of a signal up to a vector of parameters. These parameters have a clear economic interpretation and determine the difficulty of distinguishing states."
"Growth theory has rarely considered energy despite its invisible hand in all physical systems. We develop a theoretical framework that places energy transfers at centerstage of growth theory based on two principles: (1) goods are material rearrangements and (2) such rearrangements are done by energy transferred by prime movers (e.g. workers, engines). We derive the implications of these principles for an autarkic agent that maximizes utility subject to an energy budget constraint and maximizes energy surplus to relax such constraint. The solution to these problems shows that growth is driven by positive marginal energy surplus of energy goods (e.g. rice, oil), yet materializes through prime mover accumulation. This perspective brings under one framework several results from previous attempts to insert energy within growth theory, reconciles economics with natural sciences, and provides a basis for a general reinterpretation of economics and growth as the interplay between human desires and thermodynamic processes."
"We propose a decision-theoretic model akin to Savage (1972) that is useful for defining causal effects. Within this framework, we define what it means for a decision maker (DM) to act as if the relation between the two variables is causal. Next, we provide axioms on preferences and show that these axioms are equivalent to the existence of a (unique) Directed Acyclic Graph (DAG) that represents the DM's preference. The notion of representation has two components: the graph factorizes the conditional independence properties of the DM's subjective beliefs, and arrows point from cause to effect. Finally, we explore the connection between our representation and models used in the statistical causality literature (for example, Pearl (1995))."
"This paper investigates the consumption and risk taking decision of an economic agent with partial irreversibility of consumption decision by formalizing the theory proposed by Duesenberry (1949). The optimal policies exhibit a type of the (s, S) policy: there are two wealth thresholds within which consumption stays constant. Consumption increases or decreases at the thresholds and after the adjustment new thresholds are set. The share of risky investment in the agent's total investment is inversely U-shaped within the (s, S) band, which generates time-varying risk aversion that can fluctuate widely over time. This property can explain puzzles and questions on asset pricing and households' portfolio choices, e.g., why aggregate consumption is so smooth whereas the high equity premium is high and the equity return has high volatility, why the risky share is so low whereas the estimated risk aversion by the micro-level data is small, and whether and when an increase in wealth has an impact on the risky share. Also, the partial irreversibility model can explain both the excess sensitivity and the excess smoothness of consumption."
"This note considers cartel stability when the cartelized products are vertically differentiated. If market shares are maintained at pre-collusive levels, then the firm with the lowest competitive price-cost margin has the strongest incentive to deviate from the collusive agreement. The lowest-quality supplier has the tightest incentive constraint when the difference in unit production costs is sufficiently small."
"We study conditions for the existence of stable and group-strategy-proof mechanisms in a many-to-one matching model with contracts if students' preferences are monotone in contract terms. We show that ""equivalence"", properly defined, to a choice profile under which contracts are substitutes and the law of aggregate holds is a necessary and sufficient condition for the existence of a stable and group-strategy-proof mechanism.   Our result can be interpreted as a (weak) embedding result for choice functions under which contracts are observable substitutes and the observable law of aggregate demand holds."
"Interdistrict school choice programs-where a student can be assigned to a school outside of her district-are widespread in the US, yet the market-design literature has not considered such programs. We introduce a model of interdistrict school choice and present two mechanisms that produce stable or efficient assignments. We consider three categories of policy goals on assignments and identify when the mechanisms can achieve them. By introducing a novel framework of interdistrict school choice, we provide a new avenue of research in market design."
"We study a finite horizon optimal contracting problem of a risk-neutral principal and a risk-averse agent who receives a stochastic income stream when the agent is unable to make commitments. The problem involves an infinite number of constraints at each time and each state of the world. Miao and Zhang (2015) have developed a dual approach to the problem by considering a Lagrangian and derived a Hamilton-Jacobi-Bellman equation in an infinite horizon. We consider a similar Lagrangian in a finite horizon, but transform the dual problem into an infinite series of optimal stopping problems. For each optimal stopping problem we provide an analytic solution by providing an integral equation representation for the free boundary. We provide a verification theorem that the value function of the original principal's problem is the Legender-Fenchel transform of the integral of the value functions of the optimal stopping problems. We also provide some numerical simulation results of optimal contracting strategies"
"We present a limits-to-arbitrage model to study the impact of securitization, leverage and credit risk protection on the cyclicity of bank credit. In a stable bank credit situation, no cycles of credit expansion or contraction appear. Unlevered securitization together with mis-pricing of securitized assets increases lending cyclicality, favoring credit booms and busts. Leverage changes the state of affairs with respect to the simple securitization. First, the volume of real activity and banking profits increases. Second, banks sell securities when markets decline. This selling puts further pressure on falling prices. The mis-pricing of credit risk protection or securitized assets influences the real economy. Trading in these contracts reduces the amount of funding available to entrepreneurs, particularly to high-credit-risk borrowers. This trading decreases the liquidity of the securitized assets, and especially those based on investments with high credit risk."
"We are studying the Gately point, an established solution concept for cooperative games. We point out that there are superadditive games for which the Gately point is not unique, i.e. in general the concept is rather set-valued than an actual point. We derive conditions under which the Gately point is guaranteed to be a unique imputation and provide a geometric interpretation. The Gately point can be understood as the intersection of a line defined by two points with the set of imputations. Our uniqueness conditions guarantee that these two points do not coincide. We provide demonstrative interpretations for negative propensities to disrupt. We briefly show that our uniqueness conditions for the Gately point include quasibalanced games and discuss the relation of the Gately point to the $\tau$-value in this context. Finally, we point out relations to cost games and the ACA method and end upon a few remarks on the implementation of the Gately point and an upcoming software package for cooperative game theory."
"We consider a model for decision making based on an adaptive, k-period, learning process where the priors are selected according to Von Neumann-Morgenstern expected utility principle. A preference relation between two prospects is introduced, defined by the condition which prospect is selected more often. We show that the new preferences have similarities with the preferences obtained by Kahneman and Tversky (1979) in the context of the prospect theory. Additionally, we establish that in the limit of large learning period, the new preferences coincide with the expected utility principle."
"We study a communication game between an informed sender and an uninformed receiver with repeated interactions and voluntary transfers. Transfers motivate the receiver's decision-making and signal the sender's information. Although full separation can always be supported in equilibrium, partial or complete pooling is optimal if the receiver's decision-making is highly responsive to information. In this case, the receiver's decision-making is disciplined by pooling extreme states where she is most tempted to defect."
"In this paper we study a model of weighted network formation. The bilateral interaction is modeled as a Tullock contest game with the possibility of a draw. We describe stable networks under different concepts of stability. We show that a Nash stable network is either the empty network or the complete network. The complete network is not immune to bilateral deviations. When we allow for limited farsightedness, stable networks immune to bilateral deviations must be complete $M$-partite networks, with partitions of different sizes. The empty network is the efficient network. We provide several comparative statics results illustrating the importance of network structure in mediating the effects of shocks and interventions. In particular, we show that an increase in the likelihood of a draw has a non-monotonic effect on the level of wasteful contest spending in the society. To the best of our knowledge, this paper is the first attempt to model weighted network formation when the actions of individuals are neither strategic complements nor strategic substitutes."
"Nowadays, we are surrounded by a large number of complex phenomena ranging from rumor spreading, social norms formation to rise of new economic trends and disruption of traditional businesses. To deal with such phenomena,Complex Adaptive System (CAS) framework has been found very influential among social scientists,especially economists. As the most powerful methodology of CAS modeling, Agent-based modeling (ABM) has gained a growing application among academicians and practitioners. ABMs show how simple behavioral rules of agents and local interactions among them at micro-scale can generate surprisingly complex patterns at macro-scale. Despite a growing number of ABM publications, those researchers unfamiliar with this methodology have to study a number of works to understand (1) the why and what of ABMs and (2) the ways they are rigorously developed. Therefore, the major focus of this paper is to help social sciences researchers,especially economists get a big picture of ABMs and know how to develop them both systematically and rigorously."
"Existing cooperative game theoretic studies of bargaining power in gas pipeline systems are based on the so called characteristic function form (CFF). This approach is potentially misleading if some pipelines fall under regulated third party access (TPA). TPA, which is by now the norm in the EU, obliges the owner of a pipeline to transport gas for others, provided they pay a regulated transport fee. From a game theoretic perspective, this institutional setting creates so called ""externalities,"" the description of which requires partition function form (PFF) games. In this paper we propose a method to compute payoffs, reflecting the power structure, for a pipeline system with regulated TPA. The method is based on an iterative flow mechanism to determine gas flows and transport fees for individual players and uses the recursive core and the minimal claim function to convert the PPF game back into a CFF game, which can be solved by standard methods. We illustrate the approach with a simple stylized numerical example of the gas network in Central Eastern Europe with a focus on Ukraine's power index as a major transit country."
"How can a receiver design an information structure in order to elicit information from a sender? We study how a decision-maker can acquire more information from an agent by reducing her own ability to observe what the agent transmits. Intuitively, when the two parties' preferences are not perfectly aligned, this garbling relaxes the sender's concern that the receiver will use her information to the sender's disadvantage. We characterize the optimal information structure for the receiver. The main result is that under broad conditions, the receiver can do just as well as if she could commit to a rule mapping the sender's message to actions: information design is just as good as full commitment. Similarly, we show that these conditions guarantee that ex ante information acquisition always benefits the receiver, even though this learning might actually lower the receiver's expected payoff in the absence of garbling. We illustrate these effects in a range of economically relevant examples."
"A principal can restrict an agent's information (the persuasion problem) or restrict an agent's discretion (the delegation problem). We show that these problems are generally equivalent - solving one solves the other. We use tools from the persuasion literature to generalize and extend many results in the delegation literature, as well as to address novel delegation problems, such as monopoly regulation with a participation constraint."
"Most comparisons of preferences are instances of single-crossing dominance. We examine the lattice structure of single-crossing dominance, proving characterisation, existence and uniqueness results for minimum upper bounds of arbitrary sets of preferences. We apply these theorems to derive new comparative statics theorems for collective choice and under analyst uncertainty, to characterise a general 'maxmin' class of uncertainty-averse preferences over Savage acts, and to revisit the tension between liberalism and Pareto-efficiency in social choice."
"I propose a cheap-talk model in which the sender can use private messages and only cares about persuading a subset of her audience. For example, a candidate only needs to persuade a majority of the electorate in order to win an election. I find that senders can gain credibility by speaking truthfully to some receivers while lying to others. In general settings, the model admits information transmission in equilibrium for some prior beliefs. The sender can approximate her preferred outcome when the fraction of the audience she needs to persuade is sufficiently small. I characterize the sender-optimal equilibrium and the benefit of not having to persuade your whole audience in separable environments. I also analyze different applications and verify that the results are robust to some perturbations of the model, including non-transparent motives as in Crawford and Sobel (1982), and full commitment as in Kamenica and Gentzkow (2011)."
The potential benefits of portfolio diversification have been known to investors for a long time. Markowitz (1952) suggested the seminal approach for optimizing the portfolio problem based on finding the weights as budget shares that minimize the variance of the underlying portfolio. Hatemi-J and El-Khatib (2015) suggested finding the weights that will result in maximizing the risk adjusted return of the portfolio. This approach seems to be preferred by the rational investors since it combines risk and return when the optimal budget shares are sought for. The current paper provides a general solution for this risk adjusted return problem that can be utilized for any potential number of assets that are included in the portfolio.
"Consumers in many markets are uncertain about firms' qualities and costs, so buy based on both the price and the quality inferred from it. Optimal pricing depends on consumer heterogeneity only when firms with higher quality have higher costs, regardless of whether costs and qualities are private or public. If better quality firms have lower costs, then good quality is sold cheaper than bad under private costs and qualities, but not under public. However, if higher quality is costlier, then price weakly increases in quality under both informational environments."
"The broad concept of an individual's welfare is actually a cluster of related specific concepts that bear a ""family resemblance"" to one another. One might care about how a policy will affect people both in terms of their subjective preferences and also in terms of some notion of their objective interests. This paper provides a framework for evaluation of policies in terms of welfare criteria that combine these two considerations. Sufficient conditions are provided for such a criterion to imply the same ranking of social states as does Pareto's unanimity criterion. Sufficiency is proved via study of a community of agents with interdependent ordinal preferences."
"What are the value and form of optimal persuasion when information can be generated only slowly? We study this question in a dynamic model in which a 'sender' provides public information over time subject to a graduality constraint, and a decision-maker takes an action in each period. Using a novel 'viscosity' dynamic programming principle, we characterise the sender's equilibrium value function and information provision. We show that the graduality constraint inhibits information provision relative to unconstrained persuasion. The gap can be substantial, but closes as the constraint slackens. Contrary to unconstrained persuasion, less-than-full information may be provided even if players have aligned preferences but different prior beliefs."
"We study the core of normal form games with a continuum of players and without side payments. We consider the weak-core concept, which is an approximation of the core, introduced by Weber, Shapley and Shubik. For payoffs depending on the players' strategy profile, we prove that the weak-core is nonempty. The existence result establishes a weak-core element as a limit of elements in weak-cores of appropriate finite games. We establish by examples that our regularity hypotheses are relevant in the continuum case and the weak-core can be strictly larger than the Aumann's $\alpha$-core. For games where payoffs depend on the distribution of players' strategy profile, we prove that analogous regularity conditions ensuring the existence of pure strategy Nash equilibria are irrelevant for the non-vacuity of the weak-core."
"We consider the interim core of normal form cooperative games and exchange economies with incomplete information based on the partition model. We develop a solution concept that we can situate roughly between Wilson's coarse core and Yannelis's private core. We investigate the interim negotiation of contracts and address the two situations of contract delivery: interim and ex post. Our solution differs from Wilson's concept because the measurability of strategies in our solution is postponed until the consumption date (assumed with respect to the information that will be known by the players at the consumption date). For interim consumption, our concept differs from Yannelis's private core because players can negotiate conditional on proper common knowledge events in our solution, which strengthens the interim aspect of the game, as we will illustrate with examples."
"Observational learning often involves congestion: an agent gets lower payoff from an action when more predecessors have taken that action. This preference to act differently from previous agents may paradoxically increase all but one agent's probability of matching the actions of the predecessors. The reason is that when previous agents conform to their predecessors despite the preference to differ, their actions become more informative. The desire to match predecessors' actions may reduce herding by a similar reasoning."
"A buyer wishes to purchase a durable good from a seller who in each period chooses a mechanism under limited commitment. The buyer's valuation is binary and fully persistent. We show that posted prices implement all equilibrium outcomes of an infinite-horizon, mechanism selection game. Despite being able to choose mechanisms, the seller can do no better and no worse than if he chose prices in each period, so that he is subject to Coase's conjecture. Our analysis marries insights from information and mechanism design with those from the literature on durable goods. We do so by relying on the revelation principle in Doval and Skreta (2020)."
"Central to the official ""green growth"" discourse is the conjecture that absolute decoupling can be achieved with certain market instruments. This paper evaluates this claim focusing on the role of technology, while changes in GDP composition are treated elsewhere. Some fundamental difficulties for absolute decoupling, referring specifically to thermodynamic costs, are identified through a stylized model based on empirical knowledge on innovation and learning. Normally, monetary costs decrease more slowly than production grows, and this is unlikely to change should monetary costs align with thermodynamic costs, except, potentially, in the transition after the price reform. Furthermore, thermodynamic efficiency must eventually saturate for physical reasons. While this model, as usual, introduces technological innovation just as a source of efficiency, innovation also creates challenges: therefore, attempts to sustain growth by ever-accelerating innovation collide also with the limited reaction capacity of people and institutions. Information technology could disrupt innovation dynamics in the future, permitting quicker gains in eco-efficiency, but only up to saturation and exacerbating the downsides of innovation. These observations suggest that long-term sustainability requires much deeper transformations than the green growth discourse presumes, exposing the need to rethink scales, tempos and institutions, in line with ecological economics and the degrowth literature."
"We demonstrate how a static optimal income taxation problem can be analyzed using dynamical methods. Specifically, we show that the taxation problem is intimately connected to the heat equation. Our first result is a new property of the optimal tax which we call the fairness principle. The optimal tax at any income is invariant under a family of properly adjusted Gaussian averages (the heat kernel) of the optimal taxes at other incomes. That is, the optimal tax at a given income is equal to the weighted by the heat kernels average of optimal taxes at other incomes and income densities. Moreover, this averaging happens at every scale tightly linked to each other providing a unified weighting scheme at all income ranges. The fairness principle arises not due to equality considerations but rather it represents an efficient way to smooth the burden of taxes and generated revenues across incomes. Just as nature wants to distribute heat evenly, the optimal way for a government to raise revenues is to distribute the tax burden and raised revenues evenly among individuals. We then construct a gradient flow of taxes -- a dynamic process changing the existing tax system in the direction of the increase in tax revenues -- and show that it takes the form of a heat equation. The fairness principle holds also for the short-term asymptotics of the gradient flow, where the averaging is done over the current taxes. The gradient flow we consider can be viewed as a continuous process of a reform of the nonlinear income tax schedule and thus unifies the variational approach to taxation and optimal taxation. We present several other characteristics of the gradient flow focusing on its smoothing properties."
"We study the relationship between invariant transformations on extensive game structures and backward dominance procedure (BD), a generalization of the classical backward induction introduced in Perea (2014). We show that behavioral equivalence with unambiguous orderings of information sets, a critical property that guarantees BD's applicability, can be characterized by the classical Coalescing and a modified Interchange/Simultanizing in Battigalli et al. (2020). We also give conditions on transformations that improve BD's efficiency. In addition, we discuss the relationship between transformations and Bonanno (2014)'s generalized backward induction."
"This paper considers incentives to provide goods that are partially shareable along social links. We introduce a model in which each individual in a social network not only decides how much of a shareable good to provide, but also decides which subset of neighbours to nominate as co-beneficiaries. An outcome of the model specifies an endogenously generated subnetwork of the original network and a public goods game occurring over the realised subnetwork. We prove the existence of specialised pure strategy Nash equilibria: those in which some individuals contribute while the remaining individuals free ride. We then consider how the set of efficient specialised equilibria vary as the constraints on sharing are relaxed and we show that, paradoxically, an increase in shareability may decrease efficiency."
"In the pool of people seeking partners, a uniformly greater preference for abstinence increases the prevalence of infection and worsens everyone's welfare. In contrast, prevention and treatment reduce prevalence and improve payoffs. The results are driven by adverse selection: people who prefer more partners are likelier disease carriers. A given decrease in the number of matches is a smaller proportional reduction for people with many partners, thus increases the fraction of infected in the pool. The greater disease risk further decreases partner-seeking and payoffs."
"We introduce and study the property of orthogonal independence, a restricted additivity axiom applying when alternatives are orthogonal. The axiom requires that the preference for one marginal change over another should be maintained after each marginal change has been shifted in a direction that is orthogonal to both.   We show that continuous preferences satisfy orthogonal independence if and only if they are spherical: their indifference curves are spheres with the same center, with preference being ""monotone"" either away or towards the center. Spherical preferences include linear preferences as a special (limiting) case. We discuss different applications to economic and political environments. Our result delivers Euclidean preferences in models of spatial voting, quadratic welfare aggregation in social choice, and expected utility in models of choice under uncertainty."
"We advance empirical equilibrium analysis (Velez and Brown, 2020, arXiv:1907.12408) of the winner-bid and loser-bid auctions for the dissolution of a partnership. We show, in a complete information environment, that even though these auctions are essentially equivalent for the Nash equilibrium prediction, they can be expected to differ in fundamental ways when they are operated. Besides the direct policy implications, two general consequences follow. First, a mechanism designer who accounts for the empirical plausibility of equilibria may not be constrained by Maskin invariance. Second, a mechanism designer who does not account for the empirical plausibility of equilibria may inadvertently design biased mechanisms."
"In this paper, the credit scoring problem is studied by incorporating networked information, where the advantages of such incorporation are investigated theoretically in two scenarios. Firstly, a Bayesian optimal filter is proposed to provide risk prediction for lenders assuming that published credit scores are estimated merely from structured financial data. Such prediction can then be used as a monitoring indicator for the risk management in lenders' future decisions. Secondly, a recursive Bayes estimator is further proposed to improve the precision of credit scoring by incorporating the dynamic interaction topology of clients. It is shown that under the proposed evolution framework, the designed estimator has a higher precision than any efficient estimator, and the mean square errors are strictly smaller than the Cram\'er-Rao lower bound for clients within a certain range of scores. Finally, simulation results for a special case illustrate the feasibility and effectiveness of the proposed algorithms."
"For a many-to-one matching market where firms have strict and $\boldsymbol{q}$-responsive preferences, we give a characterization of the set of strongly stable fractional matchings as the union of the convex hull of all connected sets of stable matchings. Also, we prove that a strongly stable fractional matching is represented as a convex combination of stable matchings that are ordered in the common preferences of all firms."
"We study surplus extraction in the general environment of McAfee and Reny (1992), and provide two alternative proofs of their main theorem. The first is an analogue of the classic argument of Cremer and McLean (1985, 1988), using geometric features of the set of agents' beliefs to construct a menu of contracts extracting the desired surplus. This argument, which requires a finite state space, also leads to a counterexample showing that full extraction is not possible without further significant conditions on agents' beliefs or surplus, even if the designer offers an infinite menu of contracts. The second argument uses duality and applies for an infinite state space, thus yielding the general result of McAfee and Reny (1992). Both arguments suggest methods for studying surplus extraction in settings beyond the standard model, in which the designer or agents might have objectives other than risk neutral expected value maximization."
"In this paper, a relation between shadow price and the Lagrangian multiplier for nonsmooth problem is explored. It is shown that the Lagrangian Multiplier is the upper bound of shadow price for convex optimization and a class of Lipschtzian optimizations. This work can be used in shadow pricing for nonsmooth situation. The several nonsmooth functions involved in this class of Lipschtzian optimizations is listed. Finally, an application to electricity pricing is discussed."
"We develop a theory of repeated interaction for coalitional behavior. We consider stage games where both individuals and coalitions may deviate. However, coalition members cannot commit to long-run behavior, and anticipate that today's actions influence tomorrow's behavior. We evaluate the degree to which history-dependence can deter coalitional deviations. If monitoring is perfect, every feasible and strictly individually rational payoff can be supported by history-dependent conventions. By contrast, if players can make secret side-payments to each other, every coalition achieves a coalitional minmax value, potentially reducing the set of supportable payoffs to the core of the stage game."
"We study the migrants' assimilation, which we conceptualize as forming human capital productive on the labor market of a developed host country, and we link the observed frequent lack of assimilation with the relative deprivation that the migrants start to feel when they move in social space towards the natives. In turn, we presume that the native population is heterogenous and consists of high-skill and low-skill workers. The presence of assimilated migrants might shape the comparison group of the natives, influencing the relative deprivation of the low-skill workers and, in consequence, the choice to form human capital and become highly skilled. To analyse this interrelation between assimilation choices of migrants and skill formation of natives, we construct a coevolutionary model of the open-to-migration economy. Showing that the economy might end up in a non-assimilation equilibrium, we discuss welfare consequences of an assimilation policy funded from tax levied on the native population. We identify conditions under which such costly policy can bring the migrants to assimilation and at the same time increase the welfare of the natives, even though the incomes of the former take a beating."
"Auction theories are believed to provide a better selling opportunity for the resources to be allocated. Various organizations have taken measures to increase trust among participants towards their auction system, but trust alone cannot ensure a high level of participation. We propose a new type of auction system which takes advantage of lucky draw and gambling addictions to increase the engagement level of candidates in an auction. Our system makes use of security features present in existing auction systems for ensuring fairness and maintaining trust among participants."
"We revisit the linear Cournot model with uncertain demand that is studied in Lagerl\""of (2006)* and provide sufficient conditions for equilibrium uniqueness that complement the existing results. We show that if the distribution of the demand intercept has the decreasing mean residual demand (DMRD) or the increasing generalized failure rate (IGFR) property, then uniqueness of equilibrium is guaranteed. The DMRD condition implies log-concavity of the expected profits per unit of output without additional assumptions on the existence or the shape of the density of the demand intercept and, hence, answers in the affirmative the conjecture of Lagerl\""of (2006)* that such conditions may not be necessary.   *Johan Lagerl\""of, Equilibrium uniqueness in a Cournot model with demand uncertainty. The B.E. Journal in Theoretical Economics, Vol. 6: Iss 1. (Topics), Article 19:1--6, 2006."
"We study a pure-exchange incomplete-market economy with heterogeneous agents. In each period, the agents choose how much to save (i.e., invest in a risk-free bond), how much to consume, and which bundle of goods to consume while their endowments are fluctuating. We focus on a competitive stationary equilibrium (CSE) in which the wealth distribution is invariant, the agents maximize their expected discounted utility, and both the prices of consumption goods and the interest rate are market-clearing. Our main contribution is to extend some general equilibrium results to an incomplete-market Bewley-type economy with many consumption goods. Under mild conditions on the agents' preferences, we show that the aggregate demand for goods depends only on their relative prices and that the aggregate demand for savings is homogeneous of degree in prices, and we prove the existence of a CSE. When the agents' preferences can be represented by a CES (constant elasticity of substitution) utility function with an elasticity of substitution that is higher than or equal to one, we prove that the CSE is unique. Under the same preferences, we show that a higher inequality of endowments does not change the equilibrium prices of goods, and decreases the equilibrium interest rate. Our results shed light on the impact of market incompleteness on the properties of general equilibrium models."
"We study bilateral trade with interdependent values as an informed-principal problem. The mechanism-selection game has multiple equilibria that differ with respect to principal's payoff and trading surplus. We characterize the equilibrium that is worst for every type of principal, and characterize the conditions under which there are no equilibria with different payoffs for the principal. We also show that this is the unique equilibrium that survives the intuitive criterion."
"I introduce a stability notion, dynamic stability, for two-sided dynamic matching markets where (i) matching opportunities arrive over time, (ii) matching is one-to-one, and (iii) matching is irreversible. The definition addresses two conceptual issues. First, since not all agents are available to match at the same time, one must establish which agents are allowed to form blocking pairs. Second, dynamic matching markets exhibit a form of externality that is not present in static markets: an agent's payoff from remaining unmatched cannot be defined independently of what other contemporaneous agents' outcomes are. Dynamically stable matchings always exist. Dynamic stability is a necessary condition to ensure timely participation in the economy by ensuring that agents do not strategically delay the time at which they are available to match."
"This paper introduces an evolutionary dynamics based on imitate the better realization (IBR) rule. Under this rule, agents in a population game imitate the strategy of a randomly chosen opponent whenever the opponent`s realized payoff is higher than their own. Such behavior generates an ordinal mean dynamics which is polynomial in strategy utilization frequencies. We demonstrate that while the dynamics does not possess Nash stationarity or payoff monotonicity, under it pure strategies iteratively strictly dominated by pure strategies are eliminated and strict equilibria are locally stable. We investigate the relationship between the dynamics based on the IBR rule and the replicator dynamics. In trivial cases, the two dynamics are topologically equivalent. In Rock-Paper-Scissors games we conjecture that both dynamics exhibit the same types of behavior, but the partitions of the game set do not coincide. In other cases, the IBR dynamics exhibits behaviors that are impossible under the replicator dynamics."
"In this paper we develop a general framework to analyze stochastic dynamic problems with unbounded utility functions and correlated and unbounded shocks. We obtain new results of the existence and uniqueness of solutions to the Bellman equation through a general fixed point theorem that generalizes known results for Banach contractions and local contractions. We study an endogenous growth model as well as the Lucas asset pricing model in an exchange economy, significantly expanding their range of applicability."
"Firms strategically disclose product information in order to attract consumers, but recipients often find it costly to process all of it, especially when products have complex features. We study a model of competitive information disclosure by two senders, in which the receiver may garble each sender's experiment, subject to a cost increasing in the informativeness of the garbling. For a large class of parameters, it is an equilibrium for the senders to provide the receiver's first best level of information - i.e. as much as she would learn if she herself controlled information provision. Information on one sender substitutes for information on the other, which nullifies the profitability of a unilateral provision of less information. Thus, we provide a novel channel through which competition with attention costs encourages information disclosure."
"We study a method for proportional representation that was proposed at the turn from the nineteenth to the twentieth century by Gustav Enestr\""om and Edvard Phragm\'en. Like Phragm\'en's better-known iterative minimax method, it is assumed that the voters express themselves by means of approval voting. In contrast to the iterative minimax method, however, here one starts by fixing a quota, i.e. the number of votes that give the right to a seat. As a matter of fact, the method of Enestr\""om and Phragm\'en can be seen as an extension of the method of largest remainders from closed lists to open lists, or also as an adaptation of the single transferable vote to approval rather than preferential voting. The properties of this method are studied and compared with those of other methods of the same kind."
The main aim of this paper is to prove the existence of a new production function with variable elasticity of factor substitution. This production function is a more general form which includes the Cobb-Douglas production function and the CES production function as particular cases. The econometric estimates presented in the paper confirm some other results and reinforces the conclusion that the sigma is well-below the Cobb-Douglas value of one.
"In a recent paper, Naz and Chaudry provided two solutions for the model of Lucas-Uzawa, via the Partial Hamiltonian Approach. The first one of these solutions coincides exactly with that determined by Chilarescu. For the second one, they claim that this is a new solution, fundamentally different than that obtained by Chilarescu. We will prove in this paper, using the existence and uniqueness theorem of nonlinear differential equations, that this is not at all true."
"A Bayesian agent experiences gain-loss utility each period over changes in belief about future consumption (""news utility""), with diminishing sensitivity over the magnitude of news. Diminishing sensitivity induces a preference over news skewness: gradual bad news, one-shot good news is worse than one-shot resolution, which is in turn worse than gradual good news, one-shot bad news. So, the agent's preference between gradual information and one-shot resolution can depend on his consumption ranking of different states. In a dynamic cheap-talk framework where a benevolent sender communicates the state over multiple periods, the babbling equilibrium is essentially unique without loss aversion. More loss-averse agents may enjoy higher news utility in equilibrium, contrary to the commitment case. We characterize the family of gradual good news equilibria that exist with high enough loss aversion, and find the sender conveys progressively larger pieces of good news. We discuss applications to media competition and game shows."
In the paper there is studied an optimal saving model in which the interest-rate risk for saving is a fuzzy number. The total utility of consumption is defined by using a concept of possibilistic expected utility. A notion of possibilistic precautionary saving is introduced as a measure of the variation of optimal saving level when moving from a sure saving model to a possibilistic risk model. A first result establishes a necessary and sufficient condition that the presence of a possibilistic interest-rate risk generates an extra-saving. This result can be seen as a possibilistic version of a Rothschilld and Stiglitz theorem on a probabilistic model of saving. A second result of the paper studies the variation of the optimal saving level when moving from a probabilistic model (the interest-rate risk is a random variable) to a possibilistic model (the interest-rate risk is a fuzzy number).
"In this paper, we extend and improve the production chain model introduced by Kikuchi et al. (2018). Utilizing the theory of monotone concave operators, we prove the existence, uniqueness, and global stability of equilibrium price, hence improving their results on production networks with multiple upstream partners. We propose an algorithm for computing the equilibrium price function that is more than ten times faster than successive evaluations of the operator. The model is then generalized to a stochastic setting that offers richer implications for the distribution of firms in a production network."
"Data-based decisionmaking must account for the manipulation of data by agents who are aware of how decisions are being made and want to affect their allocations. We study a framework in which, due to such manipulation, data becomes less informative when decisions depend more strongly on data. We formalize why and how a decisionmaker should commit to underutilizing data. Doing so attenuates information loss and thereby improves allocation accuracy."
"In various situations, decision makers face experts that may provide conflicting advice. This advice may be in the form of probabilistic forecasts over critical future events. We consider a setting where the two forecasters provide their advice repeatedly and ask whether the decision maker can learn to compare and rank the two forecasters based on past performance. We take an axiomatic approach and propose three natural axioms that a comparison test should comply with. We propose a test that complies with our axioms. Perhaps, not surprisingly, this test is closely related to the likelihood ratio of the two forecasts over the realized sequence of events. More surprisingly, this test is essentially unique. Furthermore, using results on the rate of convergence of supermartingales, we show that whenever the two experts\textquoteright{} advice are sufficiently distinct, the proposed test will detect the informed expert in any desired degree of precision in some fixed finite time."
This paper uses an axiomatic foundation to create a new measure for the cost of learning that allows for multiple perceptual distances in a single choice environment so that some events can be harder to differentiate between than others. The new measure maintains the tractability of Shannon's classic measure but produces richer choice predictions and identifies a new form of informational bias significant for welfare and counterfactual analysis.
"I introduce a model of predictive scoring. A receiver wants to predict a sender's quality. An intermediary observes multiple features of the sender and aggregates them into a score. Based on the score, the receiver makes a decision. The sender wants the most favorable decision, and she can distort each feature at a privately known cost. I characterize the most accurate scoring rule. This rule underweights some features to deter sender distortion, and overweights other features so that the score is correct on average. The receiver prefers this scoring rule to full disclosure because information aggregation mitigates his commitment problem."
"This paper analyzes the role of money in asset markets characterized by search frictions. We develop a dynamic framework that brings together a model for illiquid financial assets `a la Duffie, Garleanu, and Pedersen, and a search-theoretic model of monetary exchange `a la Lagos and Wright. The presence of decentralized financial markets generates an essential role for money, which helps investors re-balance their portfolios. We provide conditions that guarantee the existence of a monetary equilibrium. In this case, asset prices are always above their fundamental value, and this differential represents a liquidity premium. We are able to derive an asset pricing theory that delivers an explicit connection between monetary policy, asset prices, and welfare. We obtain a negative relationship between inflation and equilibrium asset prices. This key result stems from the complementarity between money and assets in our framework."
"We propose a pseudo-market solution to resource allocation problems subject to constraints. Our treatment of constraints is general: including bihierarchical constraints due to considerations of diversity in school choice, or scheduling in course allocation; and other forms of constraints needed to model, for example, the market for roommates, and combinatorial assignment problems. Constraints give rise to pecuniary externalities, which are internalized via prices. Agents pay to the extent that their purchases affect the value of relevant constraints at equilibrium prices. The result is a constrained efficient market equilibrium outcome. The outcome is fair whenever the constraints do not single out individual agents. Our result can be extended to economies with endowments, and address participation constraints."
"We explore conclusions a person draws from observing society when he allows for the possibility that individuals' outcomes are affected by group-level discrimination. Injecting a single non-classical assumption, that the agent is overconfident about himself, we explain key observed patterns in social beliefs, and make a number of additional predictions. First, the agent believes in discrimination against any group he is in more than an outsider does, capturing widely observed self-centered views of discrimination. Second, the more group memberships the agent shares with an individual, the more positively he evaluates the individual. This explains one of the most basic facts about social judgments, in-group bias, as well as ""legitimizing myths"" that justify an arbitrary social hierarchy through the perceived superiority of the privileged group. Third, biases are sensitive to how the agent divides society into groups when evaluating outcomes. This provides a reason why some ethnically charged questions should not be asked, as well as a potential channel for why nation-building policies might be effective. Fourth, giving the agent more accurate information about himself increases all his biases. Fifth, the agent is prone to substitute biases, implying that the introduction of a new outsider group to focus on creates biases against the new group but lowers biases vis a vis other groups. Sixth, there is a tendency for the agent to agree more with those in the same groups. As a microfoundation for our model, we provide an explanation for why an overconfident agent might allow for potential discrimination in evaluating outcomes, even when he initially did not conceive of this possibility."
"Under some initial conditions, it is shown that time consistency requirements prevent rational expectation equilibrium (REE) existence for dynamic stochastic general equilibrium models induced by consumer heterogeneity, in contrast to static models. However, one can consider REE-prohibiting initial conditions as limits of other initial conditions. The REE existence issue then is overcome by using a limit of economies. This shows that significant care must be taken of when dealing with rational expectation equilibria."
"I prove an envelope theorem with a converse: the envelope formula is equivalent to a first-order condition. Like Milgrom and Segal's (2002) envelope theorem, my result requires no structure on the choice set. I use the converse envelope theorem to extend to general outcomes and preferences the canonical result in mechanism design that any increasing allocation is implementable, and apply this to selling information."
"We analyze undiscounted continuous-time games of strategic experimentation with two-armed bandits. The risky arm generates payoffs according to a L\'{e}vy process with an unknown average payoff per unit of time which nature draws from an arbitrary finite set. Observing all actions and realized payoffs, plus a free background signal, players use Markov strategies with the common posterior belief about the unknown parameter as the state variable. We show that the unique symmetric Markov perfect equilibrium can be computed in a simple closed form involving only the payoff of the safe arm, the expected current payoff of the risky arm, and the expected full-information payoff, given the current belief. In particular, the equilibrium does not depend on the precise specification of the payoff-generating processes."
"This note strengthens the main result of Lagziel and Lehrer (2019) (LL) ""A bias in screening"" using Chambers Healy (2011) (CH) ""Reversals of signal-posterior monotonicity for any bounded prior"". LL show that the conditional expectation of an unobserved variable of interest, given that a noisy signal of it exceeds a cutoff, may decrease in the cutoff. CH prove that the distribution of a variable conditional on a lower signal may first order stochastically dominate the distribution conditional on a higher signal.   The nonmonotonicity result is also extended to the empirically relevant exponential and Pareto distributions, and to a wide range of signals."
"We analyze the implications of transboundary pollution externalities on environmental policymaking in a spatial and finite time horizon setting. We focus on a simple regional optimal pollution control problem in order to compare the global and local solutions in which, respectively, the transboundary externality is and is not taken into account in the determination of the optimal policy by individual local policymakers. We show that the local solution is suboptimal and as such a global approach to environmental problems is effectively needed. Our conclusions hold true in different frameworks, including situations in which the spatial domain is either bounded or unbounded, and situations in which macroeconomic-environmental feedback effects are taken into account. We also show that if every local economy implements an environmental policy stringent enough, then the global average level of pollution will fall. If this is the case, over the long run the entire global economy will be able to achieve a completely pollution-free status."
"We study a model of vendors competing to sell a homogeneous product to customers spread evenly along a circular city. This model is based on Hotelling's celebrated paper in 1929. Our aim in this paper is to present a necessary and sufficient condition for the equilibrium. This yields a representation for the equilibrium. To achieve this, we first formulate the model mathematically. Next, we prove that the condition holds if and only if vendors are equilibrium."
"We present a unified duality approach to Bayesian persuasion. The optimal dual variable, interpreted as a price function on the state space, is shown to be a supergradient of the concave closure of the objective function at the prior belief. Strong duality holds when the objective function is Lipschitz continuous.   When the objective depends on the posterior belief through a set of moments, the price function induces prices for posterior moments that solve the corresponding dual problem. Thus, our general approach unifies known results for one-dimensional moment persuasion, while yielding new results for the multi-dimensional case. In particular, we provide a necessary and sufficient condition for the optimality of convex-partitional signals, derive structural properties of solutions, and characterize the optimal persuasion scheme in the case when the state is two-dimensional and the objective is quadratic."
"In several matching markets, in order to achieve diversity, agents' priorities are allowed to vary across an institution's available seats, and the institution is let to choose agents in a lexicographic fashion based on a predetermined ordering of the seats, called a (capacity-constrained) lexicographic choice rule. We provide a characterization of lexicographic choice rules and a characterization of deferred acceptance mechanisms that operate based on a lexicographic choice structure under variable capacity constraints. We discuss some implications for the Boston school choice system and show that our analysis can be helpful in applications to select among plausible choice rules."
"How does an expert's ability persuade change with the availability of messages? We study games of Bayesian persuasion the sender is unable to fully describe every state of the world or recommend all possible actions. We characterize the set of attainable payoffs. Sender always does worse with coarse communication and values additional signals. We show that there exists an upper bound on the marginal value of a signal for the sender. In a special class of games, the marginal value of a signal is increasing when the receiver is difficult to persuade. We show that an additional signal does not directly translate into more information and the receiver might prefer coarse communication. Finally, we study the geometric properties of optimal information structures. Using these properties, we show that the sender's optimization problem can be solved by searching within a finite set."
"We study a disclosure game with a large evidence space. There is an unknown binary state. A sender observes a sequence of binary signals about the state and discloses a left truncation of the sequence to a receiver in order to convince him that the state is good. We focus on truth-leaning equilibria (cf. Hart et al. (2017)), where the sender discloses truthfully when doing so is optimal, and the receiver takes off-path disclosure at face value. In equilibrium, seemingly sub-optimal truncations are disclosed, and the disclosure contains the longest truncation that yields the maximal difference between the number of good and bad signals. We also study a general framework of disclosure games which is compatible with large evidence spaces, a wide range of disclosure technologies, and finitely many states. We characterize the unique equilibrium value function of the sender and propose a method to construct equilibria for a broad class of games."
"We study how a principal should optimally choose between implementing a new policy and maintaining the status quo when information relevant for the decision is privately held by agents. Agents are strategic in revealing their information; the principal cannot use monetary transfers to elicit this information, but can verify an agent's claim at a cost. We characterize the mechanism that maximizes the expected utility of the principal. This mechanism can be implemented as a cardinal voting rule, in which agents can either cast a baseline vote, indicating only whether they are in favor of the new policy, or they make specific claims about their type. The principal gives more weight to specific claims and verifies a claim whenever it is decisive."
"We study intertemporal decision making under uncertainty. We fully characterize discounted expected utility in a framework \`a la Savage. Despite the popularity of this model, no characterization is available in this setting. The concept of stationarity, introduced by Koopmans for deterministic discounted utility, plays a central role for both attitudes towards time and towards uncertainty. We show that a strong stationarity axiom characterizes discounted expected utility. When hedging considerations are taken into account, a weaker stationarity axiom generalizes discounted expected utility to Choquet discounted expected utility, allowing for non-neutral attitudes towards uncertainty."
"We present an abstract social aggregation theorem. Society, and each individual, has a preorder that may be interpreted as expressing values or beliefs. The preorders are allowed to violate both completeness and continuity, and the population is allowed to be infinite. The preorders are only assumed to be represented by functions with values in partially ordered vector spaces, and whose product has convex range. This includes all preorders that satisfy strong independence. Any Pareto indifferent social preorder is then shown to be represented by a linear transformation of the representations of the individual preorders. Further Pareto conditions on the social preorder correspond to positivity conditions on the transformation. When all the Pareto conditions hold and the population is finite, the social preorder is represented by a sum of individual preorder representations. We provide two applications. The first yields an extremely general version of Harsanyi's social aggregation theorem. The second generalizes a classic result about linear opinion pooling."
"This paper proposes and axiomatizes a new updating rule: Relative Maximum Likelihood (RML) for ambiguous beliefs represented by a set of priors (C). This rule takes the form of applying Bayes' rule to a subset of C. This subset is a linear contraction of C towards its subset ascribing a maximal probability to the observed event. The degree of contraction captures the extent of willingness to discard priors based on likelihood when updating. Two well-known updating rules of multiple priors, Full Bayesian (FB) and Maximum Likelihood (ML), are included as special cases of RML. An axiomatic characterization of conditional preferences generated by RML updating is provided when the preferences admit Maxmin Expected Utility representations. The axiomatization relies on weakening the axioms characterizing FB and ML. The axiom characterizing ML is identified for the first time in this paper, addressing a long-standing open question in the literature."
"Two extensive game structures with imperfect information are said to be behaviorally equivalent if they share the same map (up to relabelings) from profiles of structurally reduced strategies to induced terminal paths. We show that this is the case if and only if one can be transformed into the other through a composition of two elementary transformations, commonly known as \textquotedblleft Interchanging of Simultaneous Moves\textquotedblright\ and \textquotedblleft Coalescing Moves/Sequential Agent Splitting.\textquotedblright"
"We study a seller who sells a single good to multiple bidders with uncertainty over the joint distribution of bidders' valuations, as well as bidders' higher-order beliefs about their opponents. The seller only knows the (possibly asymmetric) means of the marginal distributions of each bidder's valuation and the range. An adversarial nature chooses the worst-case distribution within this ambiguity set along with the worst-case information structure. We find that a second-price auction with a symmetric, random reserve price obtains the optimal revenue guarantee within a broad class of mechanisms we refer to as competitive mechanisms, which include standard auction formats, including the first-price auction, with or without reserve prices. The optimal mechanism possesses two notable characteristics. First, the mechanism treats all bidders identically even in the presence of ex-ante asymmetries. Second, when bidders are identical and the number of bidders $n$ grows large, the seller's optimal reserve price converges in probability to a non-binding reserve price and the revenue guarantee converges to the best possible revenue guarantee at rate $O(1/n)$."
"We consider a platform facilitating trade between sellers and buyers with the objective of maximizing consumer surplus. Even though in many such marketplaces prices are set by revenue-maximizing sellers, platforms can influence prices through (i) price-dependent promotion policies that can increase demand for a product by featuring it in a prominent position on the webpage and (ii) the information revealed to sellers about the value of being promoted. Identifying effective joint information design and promotion policies is a challenging dynamic problem as sellers can sequentially learn the promotion value from sales observations and update prices accordingly. We introduce the notion of confounding promotion policies, which are designed to prevent a Bayesian seller from learning the promotion value (at the expense of the short-run loss of diverting some consumers from the best product offering). Leveraging these policies, we characterize the maximum long-run average consumer surplus that is achievable through joint information design and promotion policies when the seller sets prices myopically. We then construct a Bayesian Nash equilibrium in which the seller's best response to the platform's optimal policy is to price myopically in every period. Moreover, the equilibrium we identify is platform-optimal within the class of horizon-maximin equilibria, in which strategies are not predicated on precise knowledge of the horizon length, and are designed to maximize payoff over the worst-case horizon. Our analysis allows one to identify practical long-run average optimal platform policies in a broad range of demand models."
"To divide a ""manna"" {\Omega} of private items (commodities, workloads, land, time intervals) between n agents, the worst case measure of fairness is the welfare guaranteed to each agent, irrespective of others' preferences. If the manna is non atomic and utilities are continuous (not necessarily monotone or convex), we can guarantee the minMax utility: that of our agent's best share in her worst partition of the manna; and implement it by Kuhn's generalisation of Divide and Choose. The larger Maxmin utility -- of her worst share in her best partition -- cannot be guaranteed, even for two agents. If for all agents more manna is better than less (or less is better than more), our Bid & Choose rules implement guarantees between minMax and Maxmin by letting agents bid for the smallest (or largest) size of a share they find acceptable."
"Under very general conditions, we construct a micro-macro model for closed economy with a large number of heterogeneous agents. By introducing both financial capital (i.e. valued capital---- equities of firms) and physical capital (i.e. capital goods), our framework gives a logically consistent, complete factor income distribution theory with micro-foundation. The model shows factor incomes obey different distribution rules at the micro and macro levels, while marginal distribution theory and no-arbitrage princi-ple are unified into a common framework. Our efforts solve the main problems of Cambridge capital controversy, and reasonably explain the equity premium puzzle. Strong empirical evidences support our results."
"Since the 1990s, artificial intelligence (AI) systems have achieved 'superhuman performance' in major zero-sum games, where winning has an unambiguous definition. However, most economic and social interactions are non-zero-sum, where measuring 'performance' is a non-trivial task. In this paper, I introduce a novel benchmark, super-Nash performance, and a solution concept, optimin, whereby every player maximizes their minimal payoff under unilateral profitable deviations of the others. Optimin achieves super-Nash performance in that, for every Nash equilibrium, there exists an optimin where each player not only receives but also guarantees super-Nash payoffs, even if other players deviate unilaterally and profitably from the optimin. Further, optimin generalizes and unifies several key results across domains: it coincides with (i) the maximin strategies in zero-sum games, and (ii) the core in cooperative games when the core is nonempty, though it exists even if the core is empty; additionally, optimin generalizes (iii) Nash equilibrium in $n$-person constant-sum games. Finally, optimin is consistent with the direction of non-Nash deviations in games in which cooperation has been extensively studied, including the finitely repeated prisoner's dilemma, the centipede game, the traveler's dilemma, and the finitely repeated public goods game."
"Paul A. Samuelson's (1966) capitulation during the so-called Cambridge controversy on the re-switching of techniques in capital theory had implications not only in pointing at supposed internal contradiction of the marginal theory of production and distribution, but also in preserving vested interests in the academic and political world. Based on a new non-switching theorem, the present paper demonstrates that Samuelson's capitulation was logically groundless from the point of view of the economic theory of production."
"We study collusion in a second-price auction with two bidders in a dynamic environment. One bidder can make a take-it-or-leave-it collusion proposal, which consists of both an offer and a request of bribes, to the opponent. We show that there always exists a robust equilibrium in which the collusion success probability is one. In the equilibrium, for each type of initiator the expected payoff is generally higher than the counterpart in any robust equilibria of the single-option model (Es\""{o} and Schummer (2004)) and any other separating equilibria in our model."
"Central to privacy concerns is that firms may use consumer data to price discriminate. A common policy response is that consumers should be given control over which firms access their data and how. Since firms learn about a consumer's preferences based on the data seen and the consumer's disclosure choices, the equilibrium implications of consumer control are unclear. We study whether such measures improve consumer welfare in monopolistic and competitive markets. We find that consumer control can improve consumer welfare relative to both perfect price discrimination and no personalized pricing. First, consumers can use disclosure to amplify competitive forces. Second, consumers can disclose information to induce even a monopolist to lower prices. Whether consumer control improves welfare depends on the disclosure technology and market competitiveness. Simple disclosure technologies suffice in competitive markets. When facing a monopolist, a consumer needs partial disclosure possibilities to obtain any welfare gains."
"We present a fuzzy version of the Group Identification Problem (""Who is a J?"") introduced by Kasher and Rubinstein (1997). We consider a class $N = \{1,2,\ldots,n\}$ of agents, each one with an opinion about the membership to a group J of the members of the society, consisting in a function $\pi : N \to [0; 1]$, indicating for each agent, including herself, the degree of membership to J. We consider the problem of aggregating those functions, satisfying different sets of axioms and characterizing different aggregators. While some results are analogous to those of the originally crisp model, the fuzzy version is able to overcome some of the main impossibility results of Kasher and Rubinstein."
"This article is addressing the problem of river sharing between two agents along a river in the presence of negative externalities. Where, each agent claims river water based on the hydrological characteristics of the territories. The claims can be characterized by some international framework (principles) of entitlement. These international principles are appears to be inequitable by the other agents in the presence of negative externalities. The negotiated treaties address sharing water along with the issue of negative externalities imposed by the upstream agent on the downstream agents. The market based bargaining mechanism is used for modeling and for characterization of agreement points."
"Kasher and Rubinstein (1997) introduced the problem of classifying the members of a group in terms of the opinions of their potential members. This involves a finite set of agents $N = \{1,2,\ldots,n\}$, each one having an opinion about which agents should be classified as belonging to a specific subgroup J. A Collective Identity Function (CIF) aggregates those opinions yielding the class of members deemed $J$. Kasher and Rubinstein postulate axioms, intended to ensure fair and socially desirable outcomes, characterizing different CIFs. We follow their lead by replacing their liberal axiom by other axioms, constraining the spheres of influence of the agents. We show that some of them lead to different CIFs while in another instance we find an impossibility result."
"This paper proposes a simple descriptive model of discrete-time double auction markets for divisible assets. As in the classical models of exchange economies, we consider a finite set of agents described by their initial endowments and preferences. Instead of the classical Walrasian-type market models, however, we assume that all trades take place in a centralized double auction where the agents communicate through sealed limit orders for buying and selling. We find that, under nonstrategic bidding, the double auction clears with zero trades precisely when the agents' current holdings are on the Pareto frontier. More interestingly, the double auctions implement Adam Smith's ""invisible hand"" in the sense that, when starting from disequilibrium, repeated double auctions lead to a sequence of allocations that converges to individually rational Pareto allocations."
"This paper explores the gain maximization problem of two nations engaging in non-cooperative bilateral trade. Probabilistic model of an exchange of commodities under different price systems is considered. Volume of commodities exchanged determines the demand each nation has over the counter party's currency. However, each nation can manipulate this quantity by imposing a tariff on imported commodities. As long as the gain from trade is determined by the balance between imported and exported commodities, such a scenario results in a two party game where Nash equilibrium tariffs are determined for various foreign currency demand functions and ultimately, the exchange rate based on optimal tariffs is obtained."
"This paper studies whether a planner who only has information about the network topology can discriminate among agents according to their network position. The planner proposes a simple menu of contracts, one for each location, in order to maximize total welfare, and agents choose among the menu. This mechanism is immune to deviations by single agents, and to deviations by groups of agents of sizes 2, 3 and 4 if side-payments are ruled out. However, if compensations are allowed, groups of agents may have an incentive to jointly deviate from the optimal contract in order to exploit other agents. We identify network topologies for which the optimal contract is group incentive compatible with transfers: undirected networks and regular oriented trees, and network topologies for which the planner must assign uniform quantities: single root and nested neighborhoods directed networks."
"Always, if the number of states is equal to two; or if the number of receiver actions is equal to two and i. The number of states is three or fewer, or ii. The game is cheap talk, or ii. There are just two available messages for the sender. A counterexample is provided for each failure of these conditions."
"We study the susceptibility of committee governance (e.g. by boards of directors), modelled as the collective determination of a ranking of a set of alternatives, to manipulation of the order in which pairs of alternatives are voted on -- agenda-manipulation. We exhibit an agenda strategy called insertion sort that allows a self-interested committee chair with no knowledge of how votes will be cast to do as well as if she had complete knowledge. Strategies with this 'regret-freeness' property are characterised by their efficiency, and by their avoidance of two intuitive errors. What distinguishes regret-free strategies from each other is how they prioritise among alternatives; insertion sort prioritises lexicographically."
"In an auction each party bids a certain amount and the one which bids the highest is the winner. Interestingly, auctions can also be used as models for other real-world systems. In an all pay auction all parties must pay a forfeit for bidding. In the most commonly studied all pay auction, parties forfeit their entire bid, and this has been considered as a model for expenditure on political campaigns. Here we consider a number of alternative forfeits which might be used as models for different real-world competitions, such as preparing bids for defense or infrastructure contracts."
"We explore an application of all-pay auctions to model trade wars and territorial annexation. Specifically, in the model we consider the expected resource, production, and aggressive (military/tariff) power are public information, but actual resource levels are private knowledge. We consider the resource transfer at the end of such a competition which deprives the weaker country of some fraction of its original resources. In particular, we derive the quasi-equilibria strategies for two country conflicts under different scenarios. This work is relevant for the ongoing US-China trade war, and the recent Russian capture of Crimea, as well as historical and future conflicts."
"We give a structure theorem for all coalitionally strategy-proof social choice functions whose range is a subset of cardinality two of a given larger set of alternatives.   We provide this in the case where the voters/agents are allowed to express indifference and the domain consists of profiles of preferences over a society of arbitrary cardinality. The theorem, that takes the form of a representation formula, can be used to construct all functions under consideration."
"In this paper we propose a mechanism for the allocation of pipeline capacities, assuming that the participants bidding for capacities do have subjective evaluation of various network routes. The proposed mechanism is based on the concept of bidding for route-quantity pairs. Each participant defines a limited number of routes and places multiple bids, corresponding to various quantities, on each of these routes. The proposed mechanism assigns a convex combination of the submitted bids to each participant, thus its called convex combinatorial auction. The capacity payments in the proposed model are determined according to the Vickrey-Clarke-Groves principle. We compare the efficiency of the proposed algorithm with a simplified model of the method currently used for pipeline capacity allocation in the EU (simultaneous ascending clock auction of pipeline capacities) via simulation, according to various measures, such as resulting utility of players, utilization of network capacities, total income of the auctioneer and fairness."
"In this work I clarify VAT evasion incentives through a game theoretical approach. Traditionally, evasion has been linked to the decreasing risk aversion in higher revenues (Allingham and Sandmo (1972), Cowell (1985) (1990)). I claim tax evasion to be a rational choice when compliance is stochastically more expensive than evading, even in absence of controls and sanctions. I create a framework able to measure the incentives for taxpayers to comply. The incentives here are deductions of specific VAT documented expenses from the income tax. The issue is very well known and deduction policies at work in many countries. The aim is to compute the right parameters for each precise class of taxpayers. VAT evasion is a collusive conduct between the two counterparts of the transaction. I therefore first explore the convenience for the two private counterparts to agree on the joint evasion and to form a coalition. Crucial is that compliance incentives break the agreement among the transaction participants' coalition about evading. The game solution leads to boundaries for marginal tax rates or deduction percentages, depending on parameters, able to create incentives to comply The stylized example presented here for VAT policies, already in use in many countries, is an attempt to establish a more general method for tax design, able to make compliance the ""dominant strategy"", satisfying the ""outside option"" constraint represented by evasion, even in absence of audit and sanctions. The theoretical results derived here can be easily applied to real data for precise tax design engineering."
"For a many-to-many matching market, we study the lattice structure of the set of random stable matchings. We define a partial order on the random stable set and present two intuitive binary operations to compute the least upper bound and the greatest lower bound for each side of the matching market. Then, we prove that with these binary operations the set of random stable matchings forms two dual lattices."
"We introduce a solution concept for extensive-form games of incomplete information in which players need not assign likelihoods to what they do not know about the game. This is embedded in a model in which players can hold multiple priors. Players make choices by looking for compromises that yield a good performance under each of their updated priors. Our solution concept is called perfect compromise equilibrium. It generalizes perfect Bayesian equilibrium. We show how it deals with ambiguity in Cournot and Bertrand markets, public good provision, Spence's job market signaling, bilateral trade with common value, and forecasting."
"I consider decision-making constrained by considerations of morality, rationality, or other virtues. The decision maker (DM) has a true preference over outcomes, but feels compelled to choose among outcomes that are top-ranked by some preference that he considers ""justifiable."" This model unites a broad class of empirical work on distributional preferences, charitable donations, prejudice/discrimination, and corruption/bribery. I provide a behavioral characterization of the model. I also show that the set of justifications can be identified from choice behavior when the true preference is known, and that choice behavior substantially restricts both the true preference and justifications when neither is known. I argue that the justifiability model represents an advancement over existing models of rationalization because the structure it places on possible ""rationales"" improves tractability, interpretation and identification."
"The resident matching algorithm, Gale-Shapley, currently used by SF Match and the National Residency Match Program (NRMP), has been in use for over 50 years without fundamental alteration. The algorithm is a 'stable-marriage' method that favors applicant outcomes. However, in these 50 years, there has been a big shift in the supply and demand of applicants and programs. These changes along with the way the Match is implemented have induced a costly race among applicants to apply and interview at as many programs as possible. Meanwhile programs also incur high costs as they maximize their probability of matching by interviewing as many candidates as possible."
"We consider a dynamic model of Bayesian persuasion in which information takes time and is costly for the sender to generate and for the receiver to process, and neither player can commit to their future actions. Persuasion may totally collapse in a Markov perfect equilibrium (MPE) of this game. However, for persuasion costs sufficiently small, a version of a folk theorem holds: outcomes that approximate Kamenica and Gentzkow (2011)'s sender-optimal persuasion as well as full revelation and everything in between are obtained in MPE, as the cost vanishes."
"The simple pairwise comparison is a method to provide different criteria with weights. We show that the values of those weights (in particular the maximum) depend just on the number of criteria. Additionally, it is shown that the distance between the weights is always the same.   -----   Der einfache paarweise Vergleich ist ein Verfahren verschiedene Kriterien mit einer Gewichtung zu versehen. Wir zeigen, dass die Werte dieser Gewichte (insbesondere auch der maximale Wert) ausschlie{\ss}lich von der Anzahl der Kriterien abh\""angt. Dar\""uber hinaus wird gezeigt, dass der Abstand der Gewichtungen stets gleich ist."
"We say a model is continuous in utilities (resp., preferences) if small perturbations of utility functions (resp., preferences) generate small changes in the model's outputs. While similar, these two concepts are equivalent only when the topology satisfies the following universal property: for each continuous mapping from preferences to model's outputs there is a unique mapping from utilities to model's outputs that is faithful to the preference map and is continuous. The topologies that satisfy such a universal property are called final topologies. In this paper we analyze the properties of the final topology for preference sets. This is of practical importance since most of the analysis on continuity is done via utility functions and not the primitive preference space. Our results allow the researcher to extrapolate continuity in utility to continuity in the underlying preferences."
"We offer a search-theoretic model of statistical discrimination, in which firms treat identical groups unequally based on their occupational choices. The model admits symmetric equilibria in which the group characteristic is ignored, but also asymmetric equilibria in which a group is statistically discriminated against, even when symmetric equilibria are unique. Moreover, a robust possibility is that symmetric equilibria become unstable when the group characteristic is introduced. Unlike most previous literature, our model can justify affirmative action since it eliminates asymmetric equilibria without distorting incentives."
"The basic concepts of the differential geometry are shortly reviewed and applied to the study of VES production function in the spirit of the works of V\^ilcu and collaborators. A similar characterization is given for a more general production function, namely the Kadiyala production function, in the case of developable surfaces."
"A group of experts, for instance climate scientists, is to choose among two policies $f$ and $g$. Consider the following decision rule. If all experts agree that the expected utility of $f$ is higher than the expected utility of $g$, the unanimity rule applies, and $f$ is chosen. Otherwise the precautionary principle is implemented and the policy yielding the highest minimal expected utility is chosen.   This decision rule may lead to time inconsistencies when an intermediate period of partial resolution of uncertainty is added. We provide axioms that enlarge the initial group of experts with veto power, which leads to a set of probabilistic beliefs that is ""rectangular"" in a minimal sense. This makes this decision rule dynamically consistent and provides, as a byproduct, a novel behavioral characterization of rectangularity."
"Motivated by the need for real-world matching problems, this paper formulates a large class of practical choice rules, Generalized Lexicographic Choice Rules (GLCR), for institutions that consist of multiple divisions. Institutions fill their divisions sequentially, and each division is endowed with a sub-choice rule that satisfies classical substitutability and size monotonicity in conjunction with a new property that we introduce, quota monotonicity. We allow rich interactions between divisions in the form of capacity transfers. The overall choice rule of an institution is defined as the union of the sub-choices of its divisions. The cumulative offer mechanism (COM) with respect to GLCR is the unique stable and strategy-proof mechanism. We define a choice-based improvement notion and show that the COM respects improvements. We employ the theory developed in this paper in our companion paper, Ayg\""un and Turhan (2020), to design satisfactory matching mechanisms for India with comprehensive affirmative action constraints."
"Since 1950, India has been implementing the most comprehensive affirmative action program in the world. Vertical reservations are provided to members of historically discriminated Scheduled Castes (SC), Scheduled Tribes (ST), and Other Backward Classes (OBC). Horizontal reservations are provided for other disadvantaged groups, such as women and disabled people, within each vertical category. There is no well-defined procedure to implement horizontal reservations jointly with vertical reservation and OBC de-reservations. Sequential processes currently in use for OBC de-reservations and meritorious reserve candidates lead to severe shortcomings. Most importantly, indirect mechanisms currently used in practice do not allow reserve category applicants to fully express their preferences. To overcome these and other related issues, we design several different choice rules for institutions that take meritocracy, vertical and horizontal reservations, and OBC de-reservations into account. We propose a centralized mechanism to satisfactorily clear matching markets in India."
"In many real-world matching applications, there are restrictions for institutions either on priorities of their slots or on the transferability of unfilled slots over others (or both). Motivated by the need in such real-life matching problems, this paper formulates a family of practical choice rules, slot-specific priorities with capacity transfers (SSPwCT). These practical rules invoke both slot-specific priorities structure and transferability of vacant slots. We show that the cumulative offer mechanism (COM) is stable, strategy-proof and respects improvements with regards to SSPwCT choice rules. Transferring the capacity of one more unfilled slot, while all else is constant, leads to strategy-proof Pareto improvement of the COM. Following Kominer's (2020) formulation, we also provide comparative static results for expansion of branch capacity and addition of new contracts in the SSPwCT framework. Our results have implications for resource allocation problems with diversity considerations."
"Behavioural economics provides labels for patterns in human economic behaviour. Probability weighting is one such label. It expresses a mismatch between probabilities used in a formal model of a decision (i.e. model parameters) and probabilities inferred from real people's decisions (the same parameters estimated empirically). The inferred probabilities are called ""decision weights."" It is considered a robust experimental finding that decision weights are higher than probabilities for rare events, and (necessarily, through normalisation) lower than probabilities for common events. Typically this is presented as a cognitive bias, i.e. an error of judgement by the person. Here we point out that the same observation can be described differently: broadly speaking, probability weighting means that a decision maker has greater uncertainty about the world than the observer. We offer a plausible mechanism whereby such differences in uncertainty arise naturally: when a decision maker must estimate probabilities as frequencies in a time series while the observer knows them a priori. This suggests an alternative presentation of probability weighting as a principled response by a decision maker to uncertainties unaccounted for in an observer's model."
"This paper identifies the mathematical equivalence between economic networks of Cobb-Douglas agents and Artificial Neural Networks. It explores two implications of this equivalence under general conditions. First, a burgeoning literature has established that network propagation can transform microeconomic perturbations into large aggregate shocks. Neural network equivalence amplifies the magnitude and complexity of this phenomenon. Second, if economic agents adjust their production and utility functions in optimal response to local conditions, market pricing is a sufficient and robust channel for information feedback leading to macro learning."
"We study a school choice problem under affirmative action policies where authorities reserve a certain fraction of the slots at each school for specific student groups, and where students have preferences not only over the schools they are matched to but also the type of slots they receive. Such reservation policies might cause waste in instances of low demand from some student groups. To propose a solution to this issue, we construct a family of choice functions, dynamic reserves choice functions, for schools that respect within-group fairness and allow the transfer of otherwise vacant slots from low-demand groups to high-demand groups. We propose the cumulative offer mechanism (COM) as an allocation rule where each school uses a dynamic reserves choice function and show that it is stable with respect to schools' choice functions, is strategy-proof, and respects improvements. Furthermore, we show that transferring more of the otherwise vacant slots leads to strategy-proof Pareto improvement under the COM."
"Empirical evidence suggests that the rich have higher propensity to save than do the poor. While this observation may appear to contradict the homotheticity of preferences, we theoretically show that that is not the case. Specifically, we consider an income fluctuation problem with homothetic preferences and general shocks and prove that consumption functions are asymptotically linear, with an exact analytical characterization of asymptotic marginal propensities to consume (MPC). We provide necessary and sufficient conditions for the asymptotic MPCs to be zero. We calibrate a model with standard constant relative risk aversion utility and show that zero asymptotic MPCs are empirically plausible, implying that our mechanism has the potential to accommodate a large saving rate of the rich and high wealth inequality (small Pareto exponent) as observed in the data."
"We propose and axiomatize the categorical thinking model (CTM) in which the framing of the decision problem affects how agents categorize alternatives, that in turn affects their evaluation of it. Prominent models of salience, status quo bias, loss-aversion, inequality aversion, and present bias all fit under the umbrella of CTM. This suggests categorization is an underlying mechanism of key departures from the neoclassical model of choice. We specialize CTM to provide a behavioral foundation for the salient thinking model of Bordalo et al. (2013) that highlights its strong predictions and distinctions from other models."
"An equilibrium is communication-proof if it is unaffected by new opportunities to communicate and renegotiate. We characterize the set of equilibria of coordination games with pre-play communication in which players have private preferences over the coordinated outcomes. The set of communication-proof equilibria is a small and relatively homogeneous subset of the set of qualitatively diverse Bayesian Nash equilibria. Under a communication-proof equilibrium, players never miscoordinate, play their jointly preferred outcome whenever there is one, and communicate only the ordinal part of their preferences. Moreover, such equilibria are robust to changes in players' beliefs and interim Pareto efficient"
"We develop a result on expected posteriors for Bayesians with heterogenous priors, dubbed information validates the prior (IVP). Under familiar ordering requirements, Anne expects a (Blackwell) more informative experiment to bring Bob's posterior mean closer to Anne's prior mean. We apply the result in two contexts of games of asymmetric information: voluntary testing or certification, and costly signaling or falsification. IVP can be used to determine how an agent's behavior responds to additional exogenous or endogenous information. We discuss economic implications."
"We study population dynamics under which each revising agent tests each strategy k times, with each trial being against a newly drawn opponent, and chooses the strategy whose mean payoff was highest. When k = 1, defection is globally stable in the prisoner`s dilemma. By contrast, when k > 1 we show that there exists a globally stable state in which agents cooperate with probability between 28% and 50%. Next, we characterize stability of strict equilibria in general games. Our results demonstrate that the empirically plausible case of k > 1 can yield qualitatively different predictions than the case of k = 1 that is commonly studied in the literature."
"We propose a new method to define trading algorithms in market design environments. Dropping the traditional idea of clearing cycles in generated graphs, we use parameterized linear equations to define trading algorithms. Our method has two advantages. First, our method avoids discussing the details of who trades with whom and how, which can be a difficult question in complex environments. Second, by controlling parameter values in our equations, our method is flexible and transparent to satisfy various fairness criteria. We apply our method to several models and obtain new trading algorithms that are efficient and fair."
"Voting is the aggregation of individual preferences in order to select a winning alternative. Selection of a winner is accomplished via a voting rule, e.g., rank-order voting, majority rule, plurality rule, approval voting. Which voting rule should be used? In social choice theory, desirable properties of voting rules are expressed as axioms to be satisfied. This thesis focuses on axioms concerning strategic manipulation by voters. Sometimes, voters may intentionally misstate their true preferences in order to alter the outcome for their own advantage. For example, in plurality rule, if a voter knows that their top-choice candidate will lose, then they might instead vote for their second-choice candidate just to avoid an even less desirable result. When no coalition of voters can strategically manipulate, then the voting rule is said to satisfy the axiom of Strategy-Proofness. A less restrictive axiom is Weak Strategy-Proofness (as defined by Dasgupta and Maskin (2019)), which allows for strategic manipulation by all but the smallest coalitions. Under certain intuitive conditions, Dasgupta and Maskin (2019) proved that the only voting rules satisfying Strategy-Proofness are rank-order voting and majority rule. In my thesis, I generalize their result, by proving that rank-order voting and majority rule are surprisingly still the only voting rules satisfying Weak Strategy-Proofness."
"This paper considers a simple model where a social planner can influence the spread-intensity of an infection wave, and, consequently, also the economic activity and population health, through a single parameter. Population health is assumed to only be negatively affected when the number of simultaneously infected exceeds health care capacity. The main finding is that if (i) the planner attaches a positive weight on economic activity and (ii) it is more harmful for the economy to be locked down for longer than shorter time periods, then the optimal policy is to (weakly) exceed health care capacity at some time."
"Efficiency and fairness are two desiderata in market design. Fairness requires randomization in many environments. Observing the inadequacy of Top Trading Cycle (TTC) to incorporate randomization, Yu and Zhang (2020) propose the class of Fractional TTC mechanisms to solve random allocation problems efficiently and fairly. The assumption of strict preferences in the paper restricts the application scope. This paper extends Fractional TTC to the full preference domain in which agents can be indifferent between objects. Efficiency and fairness of Fractional TTC are preserved. As a corollary, we obtain an extension of the probabilistic serial mechanism in the house allocation model to the full preference domain. Our extension does not require any knowledge beyond elementary computation."
"Many markets rely on traders truthfully communicating who has cheated in the past and ostracizing those traders from future trade. This paper investigates when truthful communication is incentive compatible. We find that if each side has a myopic incentive to deviate, then communication incentives are satisfied only when the volume of trade is low. By contrast, if only one side has a myopic incentive to deviate, then communication incentives do not constrain the volume of supportable trade. Accordingly, there are strong gains from structuring trade so that one side either moves first or has its cooperation guaranteed by external enforcement."
"Following the ideas laid out in Myerson (1996), Hofbauer (2000) defined a Nash equilibrium of a finite game as sustainable if it can be made the unique Nash equilibrium of a game obtained by deleting/adding a subset of the strategies that are inferior replies to it. This paper proves two results about sustainable equilibria. The first concerns the Hofbauer-Myerson conjecture about the relationship between the sustainability of an equilibrium and its index: for a generic class of games, an equilibrium is sustainable iff its index is $+1$. Von Schemde and von Stengel (2008) proved this conjecture for bimatrix games; we show that the conjecture is true for all finite games. More precisely, we prove that an isolated equilibrium has index +1 if and only if it can be made unique in a larger game obtained by adding finitely many strategies that are inferior replies to that equilibrium. Our second result gives an axiomatic extension of sustainability to all games and shows that only the Nash components with positive index can be sustainable."
"Plurality and approval voting are two well-known voting systems with different strengths and weaknesses. In this paper we consider a new voting system we call beta(k) which allows voters to select a single first-choice candidate and approve of any other number of candidates, where k denotes the relative weight given to a first choice; this system is essentially a hybrid of plurality and approval. Our primary goal is to characterize the behavior of beta(k) for any value of k. Under certain reasonable assumptions, beta(k) can be made to mimic plurality or approval voting in the event of a single winner while potentially breaking ties otherwise. Under the assumption that voters are honest, we show that it is possible to find the values of k for which a given candidate will win the election if the respective approval and plurality votes are known. Finally, we show how some of the commonly used voting system criteria are satisfied by beta(k)."
"We consider contest success functions (CSFs) that extract contestants' prize values. In the common-value case, there exists a CSF extractive in any equilibrium. In the observable-private-value case, there exists a CSF extractive in some equilibrium; there exists a CSF extractive in any equilibrium if and only if the number of contestants is greater than or equal to three or the values are homogeneous. In the unobservable-private-value case, there exists no CSF extractive in some equilibrium. When extractive CSFs exist, we explicitly present one of them."
"While auction theory views bids and valuations as continuous variables, real-world auctions are necessarily discrete. In this paper, we use a combination of analytical and computational methods to investigate whether incorporating discreteness substantially changes the predictions of auction theory, focusing on the case of uniformly distributed valuations so that our results bear on the majority of auction experiments. In some cases, we find that introducing discreteness changes little. For example, the first-price auction with two bidders and an even number of values has a symmetric equilibrium that closely resembles its continuous counterpart and converges to its continuous counterpart as the discretisation goes to zero. In others, however, we uncover discontinuity results. For instance, introducing an arbitrarily small amount of discreteness into the all-pay auction makes its symmetric, pure-strategy equilibrium disappear; and appears (based on computational experiments) to rob the game of pure-strategy equilibria altogether. These results raise questions about the continuity approximations on which auction theory is based and prompt a re-evaluation of the experimental literature."
"We study several models of growth driven by innovation and imitation by a continuum of firms, focusing on the interaction between the two. We first investigate a model on a technology ladder where innovation and imitation combine to generate a balanced growth path (BGP) with compact support, and with productivity distributions for firms that are truncated power-laws. We start with a simple model where firms can adopt technologies of other firms with higher productivities according to exogenous probabilities. We then study the case where the adoption probabilities depend on the probability distribution of productivities at each time. We finally consider models with a finite number of firms, which by construction have firm productivity distributions with bounded support. Stochastic imitation and innovation can make the distance of the productivity frontier to the lowest productivity level fluctuate, and this distance can occasionally become large. Alternatively, if we fix the length of the support of the productivity distribution because firms too far from the frontier cannot survive, the number of firms can fluctuate randomly."
"A proposer requires the approval of a veto player to change a status quo. Preferences are single peaked. Proposer is uncertain about Vetoer's ideal point. We study Proposer's optimal mechanism without transfers. Vetoer is given a menu, or a delegation set, to choose from. The optimal delegation set balances the extent of Proposer's compromise with the risk of a veto. Under reasonable conditions, ""full delegation"" is optimal: Vetoer can choose any action between the status quo and Proposer's ideal action. This outcome largely nullifies Proposer's bargaining power; Vetoer frequently obtains her ideal point, and there is Pareto efficiency despite asymmetric information. More generally, we identify when ""interval delegation"" is optimal. Optimal interval delegation can be a Pareto improvement over cheap talk. We derive comparative statics. Vetoer receives less discretion when preferences are more likely to be aligned, by contrast to expertise-based delegation. Methodologically, our analysis handles stochastic mechanisms."
"We study private-good allocation under general constraints. Several prominent examples are special cases, including house allocation, roommate matching, social choice, and multiple assignment. Every individually strategy-proof and Pareto efficient two-agent mechanism is an ""adapted local dictatorship."" Every group strategy-proof N-agent mechanism has two-agent marginal mechanisms that are adapted local dictatorships. These results yield new characterizations and unifying insights for known characterizations. We find all group strategy-proof and Pareto efficient mechanisms for the roommates problem. We give a related result for multiple assignment. We prove the Gibbard--Satterthwaite Theorem and give a partial converse."
"A well-intentioned principal provides information to a rationally inattentive agent without internalizing the agent's cost of processing information. Whatever information the principal makes available, the agent may choose to ignore some. We study optimal information provision in a tractable model with quadratic payoffs where full disclosure is not optimal. We characterize incentive-compatible information policies, that is, those to which the agent willingly pays full attention. In a leading example with three states, optimal disclosure involves information distortion at intermediate costs of attention. As the cost increases, optimal information abruptly changes from downplaying the state to exaggerating the state."
"I study a social learning model in which the object to learn is a strategic player's endogenous actions rather than an exogenous state. A patient seller faces a sequence of buyers and decides whether to build a reputation for supplying high quality products. Each buyer does not have access to the seller's complete records, but can observe all previous buyers' actions, and some informative private signal about the seller's actions. I examine how the buyers' private signals affect the speed of social learning and the seller's incentives to establish reputations. When each buyer privately observes a bounded subset of the seller's past actions, the speed of learning is strictly positive but can vanish to zero as the seller becomes patient. As a result, reputation building can lead to low payoff for the patient seller and low social welfare. When each buyer observes an unboundedly informative private signal about the seller's current-period action, the speed of learning is uniformly bounded from below and a patient seller can secure high returns from building reputations. My results shed light on the effectiveness of various policies in accelerating social learning and encouraging sellers to establish good reputations."
"I study repeated communication games between a patient sender and a sequence of receivers. The sender has persistent private information about his psychological cost of lying, and in every period, can privately observe the realization of an i.i.d. state before communication takes place. I characterize every type of sender's highest equilibrium payoff. When the highest lying cost in the support of the receivers' prior belief approaches the sender's benefit from lying, every type's highest equilibrium payoff in the repeated communication game converges to his equilibrium payoff in a one-shot Bayesian persuasion game. I also show that in every sender-optimal equilibrium, no type of sender mixes between telling the truth and lying at every history. When there exist ethical types whose lying costs outweigh their benefits, I provide necessary and sufficient conditions for all non-ethical type senders to attain their optimal commitment payoffs. I identify an outside option effect through which the possibility of being ethical decreases every non-ethical type's payoff."
"I study a repeated game in which a patient player (e.g., a seller) wants to win the trust of some myopic opponents (e.g., buyers) but can strictly benefit from betraying them. Her benefit from betrayal is strictly positive and is her persistent private information. I characterize every type of patient player's highest equilibrium payoff. Her persistent private information affects this payoff only through the lowest benefit in the support of her opponents' prior belief. I also show that in every equilibrium which is optimal for the patient player, her on-path behavior is nonstationary, and her long-run action frequencies are pinned down for all except two types. Conceptually, my payoff-type approach incorporates a realistic concern that no type of reputation-building player is immune to reneging temptations. Compared to commitment-type models, the incentive constraints for all types of patient player lead to a sharp characterization of her highest attainable payoff and novel predictions on her behaviors."
"This paper develops a Nash-equilibrium extension of the classic SIR model of infectious-disease epidemiology (""Nash SIR""), endogenizing people's decisions whether to engage in economic activity during a viral epidemic and allowing for complementarity in social-economic activity. An equilibrium epidemic is one in which Nash equilibrium behavior during the epidemic generates the epidemic. There may be multiple equilibrium epidemics, in which case the epidemic trajectory can be shaped through the coordination of expectations, in addition to other sorts of interventions such as stay-at-home orders and accelerated vaccine development. An algorithm is provided to compute all equilibrium epidemics."
"Economic theory has provided an estimable intuition in understanding the perplexing ideologies in law, in the areas of economic law, tort law, contract law, procedural law and many others. Most legal systems require the parties involved in a legal dispute to exchange information through a process called discovery. The purpose is to reduce the relative optimisms developed by asymmetric information between the parties. Like a head or tail phenomenon in stochastic processes, uncertainty in the adjudication affects the decisions of the parties in a legal negotiation. This paper therefore applies the principles of aleatory analysis to determine how negotiations fail in the legal process, introduce the axiological concept of optimal transaction cost and formulates a numerical methodology based on backwards induction and stochastic options pricing economics in estimating the reasonable and fair bargain in order to induce settlements thereby increasing efficiency and reducing social costs."
"This paper examines necessary and sufficient conditions for the uniqueness of dynamic Groves mechanisms when the domain of valuations is restricted. Our approach is to appropriately define the total valuation function, which is the expected discounted sum of each period's valuation function from the allocation and thus a dynamic counterpart of the static valuation function, and then to port the results for static Groves mechanisms to the dynamic setting."
"McCall (1970) examines the search behaviour of an infinitely-lived and risk-neutral job seeker maximizing her lifetime earnings by accepting or rejecting real-valued scalar wage offers. In practice, job offers have multiple attributes, and job seekers solve a multicriteria search problem. This paper presents a multicriteria search model and new comparative statics results."
"We consider games in which players search for a hidden prize, and they have asymmetric information about the prize location. We study the social payoff in equilibria of these games. We present sufficient conditions for the existence of an equilibrium that yields the first-best payoff (i.e., the highest social payoff under any strategy profile), and we characterize the first-best payoff. The results have interesting implications for innovation contests and R&D races."
"In a decision problem comprised of multiple choices, a person may fail to take into account the interdependencies between their choices. To understand how people make decisions in such problems, we design a novel experiment and revealed preference tests that determine how each subject brackets their choices. In separate portfolio allocation under risk, social allocation, and induced-value function shopping experiments, we find that 40-43% of our subjects are consistent with narrow bracketing while 0-15% are consistent with broad bracketing. Adjusting for each model's predictive precision, 74% of subjects are best described by narrow bracketing, 13% by broad bracketing, and 6% by intermediate cases."
"We investigate how distorted, yet structured, beliefs can persist in strategic situations. Specifically, we study two-player games in which each player is endowed with a biased-belief function that represents the discrepancy between a player's beliefs about the opponent's strategy and the actual strategy. Our equilibrium condition requires that (i) each player choose a best-response strategy to his distorted belief about the opponent's strategy, and (ii) the distortion functions form best responses to one another. We obtain sharp predictions and novel insights into the set of stable outcomes and their supporting stable biases in various classes of games."
"We develop a framework in which individuals' preferences coevolve with their abilities to deceive others about their preferences and intentions. Specifically, individuals are characterised by (i) a level of cognitive sophistication and (ii) a subjective utility function. Increased cognition is costly, but higher-level individuals have the advantage of being able to deceive lower-level opponents about their preferences and intentions in some of the matches. In the remaining matches, the individuals observe each other's preferences. Our main result shows that, essentially, only efficient outcomes can be stable. Moreover, under additional mild assumptions, we show that an efficient outcome is stable if and only if the gain from unilateral deviation is smaller than the effective cost of deception in the environment."
"We study environments in which agents are randomly matched to play a Prisoner's Dilemma, and each player observes a few of the partner's past actions against previous opponents. We depart from the existing related literature by allowing a small fraction of the population to be commitment types. The presence of committed agents destabilizes previously proposed mechanisms for sustaining cooperation. We present a novel intuitive combination of strategies that sustains cooperation in various environments. Moreover, we show that under an additional assumption of stationarity, this combination of strategies is essentially the unique mechanism to support full cooperation, and it is robust to various perturbations. Finally, we extend the results to a setup in which agents also observe actions played by past opponents against the current partner, and we characterize which observation structure is optimal for sustaining cooperation."
"A patient player privately observes a persistent state that directly affects his myopic opponents' payoffs, and can be one of the several commitment types that plays the same mixed action in every period. I characterize the set of environments under which the patient player obtains at least his commitment payoff in all equilibria regardless of his stage-game payoff function. Due to interdependent values, the patient player cannot guarantee his mixed commitment payoff by imitating the mixed-strategy commitment type, and small perturbations to a pure commitment action can significantly reduce the patient player's guaranteed equilibrium payoff."
"We consider a principal agent project selection problem with asymmetric information. There are $N$ projects and the principal must select exactly one of them. Each project provides some profit to the principal and some payoff to the agent and these profits and payoffs are the agent's private information. We consider the principal's problem of finding an optimal mechanism for two different objectives: maximizing expected profit and maximizing the probability of choosing the most profitable project. Importantly, we assume partial verifiability so that the agent cannot report a project to be more profitable to the principal than it actually is. Under this no-overselling constraint, we characterize the set of implementable mechanisms. Using this characterization, we find that in the case of two projects, the optimal mechanism under both objectives takes the form of a simple cutoff mechanism. The simple structure of the optimal mechanism also allows us to find evidence in support of the well-known ally-principle which says that principal delegates more authority to an agent who shares their preferences."
"Let V be a finite society whose members express weak orderings (hence also indifference, possibly) about two alternatives. We show a simple representation formula that is valid for all, and only, anonymous, non-manipulable, binary social choice functions on V . The number of such functions is $2^{n+1}$ if V contains $n$ agents."
"We consider a dynamic moral hazard problem between a principal and an agent, where the sole instrument the principal has to incentivize the agent is the disclosure of information. The principal aims at maximizing the (discounted) number of times the agent chooses a particular action, e.g., to work hard. We show that there exists an optimal contract, where the principal stops disclosing information as soon as its most preferred action is a static best reply for the agent or else continues disclosing information until the agent perfectly learns the principal's private information. If the agent perfectly learns the state, he learns it in finite time with probability one; the more patient the agent, the later he learns it."
"Evidence games study situations where a sender persuades a receiver by selectively disclosing hard evidence about an unknown state of the world. Evidence games often have multiple equilibria. Hart et al. (2017) propose to focus on truth-leaning equilibria, i.e., perfect Bayesian equilibria where the sender discloses truthfully when indifferent, and the receiver takes off-path disclosure at face value. They show that a truth-leaning equilibrium is an equilibrium of a perturbed game where the sender has an infinitesimal reward for truth-telling. We show that, when the receiver's action space is finite, truth-leaning equilibrium may fail to exist, and it is not equivalent to equilibrium of the perturbed game. To restore existence, we introduce a disturbed game with a small uncertainty about the receiver's payoff. A purifiable truthful equilibrium is the limit of a sequence of truth-leaning equilibria in the disturbed games as the disturbances converge to zero. It exists and features a simple characterization. A truth-leaning equilibrium that is also purifiable truthful is an equilibrium of the perturbed game. Moreover, purifiable truthful equilibria are receiver optimal and give the receiver the same payoff as the optimal deterministic mechanism."
"Dynamic tolls present an opportunity for municipalities to eliminate congestion and fund infrastructure. Imposing tolls that regulate travel along a public highway through monetary fees raise worries of inequity. In this article, we introduce the concept of time poverty, emphasize its value in policy-making in the same ways income poverty is already considered, and argue the potential equity concern posed by time-varying tolls that produce time poverty. We also compare the cost burdens of a no-toll, system optimal toll, and a proposed ``time-equitable"" toll on heterogeneous traveler groups using an analytical Vickrey bottleneck model where travelers make departure time decisions to arrive at their destination at a fixed time. We show that the time-equitable toll is able to eliminate congestion while creating equitable travel patterns amongst traveler groups."
"We study a dynamic stopping game between a principal and an agent. The agent is privately informed about his type. The principal learns about the agent's type from a noisy performance measure, which can be manipulated by the agent via a costly and hidden action. We fully characterize the unique Markov equilibrium of this game. We find that terminations/market crashes are often preceded by a spike in (expected) performance. Our model also predicts that, due to endogenous signal manipulation, too much transparency can inhibit learning. As the players get arbitrarily patient, the principal elicits no useful information from the observed signal."
"This paper characterizes informational outcomes in a model of dynamic signaling with vanishing commitment power. It shows that contrary to popular belief, informative equilibria with payoff-relevant signaling can exist without requiring unreasonable off-path beliefs. The paper provides a sharp characterization of possible separating equilibria: all signaling must take place through attrition, when the weakest type mixes between revealing own type and pooling with the stronger types. The framework explored in the paper is general, imposing only minimal assumptions on payoff monotonicity and single-crossing. Applications to bargaining, monopoly price signaling, and labor market signaling are developed to demonstrate the results in specific contexts."
"We prove that a random choice rule satisfies Luce's Choice Axiom if and only if its support is a choice correspondence that satisfies the Weak Axiom of Revealed Preference, thus it consists of alternatives that are optimal according to some preference, and random choice then occurs according to a tie breaking among such alternatives that satisfies Renyi's Conditioning Axiom. Our result shows that the Choice Axiom is, in a precise formal sense, a probabilistic version of the Weak Axiom. It thus supports Luce's view of his own axiom as a ""canon of probabilistic rationality."""
"Data analytics allows an analyst to gain insight into underlying populations through the use of various computational approaches, including Monte Carlo methods. This paper discusses an approach to apply Monte Carlo methods to hedonic games. Hedonic games have gain popularity over the last two decades leading to several research articles that are concerned with the necessary, sufficient, or both conditions of the existence of a core partition. Researchers have used analytical methods for this work. We propose that using a numerical approach will give insights that might not be available through current analytical methods. In this paper, we describe an approach to representing hedonic games, with strict preferences, in a matrix form that can easily be generated; that is, a hedonic game with randomly generated preferences for each player. Using this generative approach, we were able to create and solve, i.e., find any core partitions, of millions of hedonic games. Our Monte Carlo experiment generated games with up to thirteen players. The results discuss the distribution form of the core size of the games of a given number of players. We also discuss computational considerations. Our numerical study of hedonic games gives insight into the underlying properties of hedonic games."
"We examine a patient player's behavior when he can build reputations in front of a sequence of myopic opponents. With positive probability, the patient player is a commitment type who plays his Stackelberg action in every period. We characterize the patient player's action frequencies in equilibrium. Our results clarify the extent to which reputations can refine the patient player's behavior and provide new insights to entry deterrence, business transactions, and capital taxation. Our proof makes a methodological contribution by establishing a new concentration inequality."
"We study dynamic signaling when the informed party does not observe the signals generated by her actions. A long-run player signals her type continuously over time to a myopic second player who privately monitors her behavior; in turn, the myopic player transmits his private inferences back through an imperfect public signal of his actions. Preferences are linear-quadratic and the information structure is Gaussian. We construct linear Markov equilibria using belief states up to the long-run player's $\textit{second-order belief}$. Because of the private monitoring, this state is an explicit function of the long-run player's past play. A novel separation effect then emerges through this second-order belief channel, altering the traditional signaling that arises when beliefs are public. Applications to models of leadership, reputation, and trading are examined."
"In this paper I discuss truthful equilibria in common agency models. Specifically, I provide general conditions under which truthful equilibria are plausible, easy to calculate and efficient. These conditions generalize similar results in the literature and allow the use of truthful equilibria in novel economic applications. Moreover, I provide two such applications. The first application is a market game in which multiple sellers sell a uniform good to a single buyer. The second application is a lobbying model in which there are externalities in contributions between lobbies. This last example indicates that externalities between principals do not necessarily prevent efficient equilibria. In this regard, this paper provides a set of conditions, under which, truthful equilibria in common agency models with externalities are efficient."
"A priority system has traditionally been the protocol of choice for the allocation of scarce life-saving resources during public health emergencies. Covid-19 revealed the limitations of this allocation rule. Many argue that priority systems abandon ethical values such as equity by discriminating against disadvantaged communities. We show that a restrictive feature of the traditional priority system largely drives these limitations. Following minimalist market design, an institution design paradigm that integrates research and policy efforts, we formulate pandemic allocation of scarce life-saving resources as a new application of market design. Interfering only with the restrictive feature of the priority system to address its shortcomings, we formulate a reserve system as an alternative allocation rule. Our theoretical analysis develops a general theory of reserve design. We relate our analysis to debates during Covid-19 and describe the impact of our paper on policy and practice."
"We study sequential search without priors. Our interest lies in decision rules that are close to being optimal under each prior and after each history. We call these rules dynamically robust. The search literature employs optimal rules based on cutoff strategies that are not dynamically robust. We derive dynamically robust rules and show that their performance exceeds 1/2 of the optimum against binary environments and 1/4 of the optimum against all environments. This performance improves substantially with the outside option value, for instance, it exceeds 2/3 of the optimum if the outside option exceeds 1/6 of the highest possible alternative."
"This paper derives primitive, easily verifiable sufficient conditions for existence and uniqueness of (stochastic) recursive utilities for several important classes of preferences. In order to accommodate models commonly used in practice, we allow both the state-space and per-period utilities to be unbounded. For many of the models we study, existence and uniqueness is established under a single, primitive ""thin tail"" condition on the distribution of growth in per-period utilities. We present several applications to robust preferences, models of ambiguity aversion and learning about hidden states, and Epstein-Zin preferences."
"The standard model of choice in economics is the maximization of a complete and transitive preference relation over a fixed set of alternatives. While completeness of preferences is usually regarded as a strong assumption, weakening it requires care to ensure that the resulting model still has enough structure to yield interesting results. This paper takes a step in this direction by studying the class of ""connected preferences"", that is, preferences that may fail to be complete but have connected maximal domains of comparability. We offer four new results. Theorem 1 identifies a basic necessary condition for a continuous preference to be connected in the sense above, while Theorem 2 provides sufficient conditions. Building on the latter, Theorem 3 characterizes the maximal domains of comparability. Finally, Theorem 4 presents conditions that ensure that maximal domains are arc-connected."
"This is a general competitive analysis paper. A model is presented that describes how an individual with a physical disability, or mobility impairment, would go about utility maximization. These results are then generalized. Subsequently, a selection of disability policies from Canada and the United States are compared to the insights of the model, and it is shown that there are sources of inefficiency in many North American disability support systems."
"We study Bayesian Persuasion with multiple senders who have access to conditionally independent experiments (and possibly others). Senders have zero-sum preferences over information revealed. We characterize when any set of states can be pooled in equilibrium and when all equilibria are fully revealing. The state is fully revealed in every equilibrium if and only if sender utility functions are `globally nonlinear'. With two states, this is equivalent to some sender having nontrivial preferences. The upshot is that `most' zero-sum sender preferences result in full revelation. We explore what conditions are important for competition to result in such stark information revelation."
"Electre Tri is a set of methods designed to sort alternatives evaluated on several criteria into ordered categories. In these methods, alternatives are assigned to categories by comparing them with reference profiles that represent either the boundary or central elements of the category. The original Electre Tri-B method uses one limiting profile for separating a category from the category below. A more recent method, Electre Tri-nB, allows one to use several limiting profiles for the same purpose. We investigate the properties of Electre Tri-nB using a conjoint measurement framework. When the number of limiting profiles used to define each category is not restricted, Electre Tri-nB is easy to characterize axiomatically and is found to be equivalent to several other methods proposed in the literature. We extend this result in various directions."
"We examine the design of optimal rating systems in the presence of moral hazard. First, an intermediary commits to a rating scheme. Then, a decision-maker chooses an action that generates value for the buyer. The intermediary then observes a noisy signal of the decision-maker's choice and sends the buyer a signal consistent with the rating scheme. Here we fully characterize the set of allocations that can arise in equilibrium under any arbitrary rating system. We use this characterization to study various design aspects of optimal rating systems. Specifically, we study the properties of optimal ratings when the decision-maker's effort is productive and when the decision-maker can manipulate the intermediary's signal with a noise. With manipulation, rating uncertainty is a fairly robust feature of optimal rating systems."
"A collective choice problem is a finite set of social alternatives and a finite set of economic agents with vNM utility functions. We associate a public goods economy with each collective choice problem and establish the existence and efficiency of (equal income) Lindahl equilibrium allocations. We interpret collective choice problems as cooperative bargaining problems and define a set-valued solution concept, {\it the equitable solution} (ES). We provide axioms that characterize ES and show that ES contains the Nash bargaining solution. Our main result shows that the set of ES payoffs is the same a the set of Lindahl equilibrium payoffs. We consider two applications: in the first, we show that in a large class of matching problems without transfers the set of Lindahl equilibrium payoffs is the same as the set of (equal income) Walrasian equilibrium payoffs. In our second application, we show that in any discrete exchange economy without transfers every Walrasian equilibrium payoff is a Lindahl equilibrium payoff of the corresponding collective choice market. Moreover, for any cooperative bargaining problem, it is possible to define a set of commodities so that the resulting economy's utility possibility set is that bargaining problem {\it and} the resulting economy's set of Walrasian equilibrium payoffs is the same as the set of Lindahl equilibrium payoffs of the corresponding collective choice market."
"The aim of this article is to propose a core game theory model of transaction costs wherein it is indicated how direct costs determine the probability of loss and subsequent transaction costs. The existence of optimum is proven, and the way in which exposure influences the location of the optimum is demonstrated. The decisions are described as a two-player game and it is discussed how the transaction cost sharing rule determines whether the optimum point of transaction costs is the same as the equilibrium of the game. A game modelling dispute between actors regarding changing the share of transaction costs to be paid by each party is also presented. Requirements of efficient transaction cost sharing rules are defined, and it is posited that a solution exists which is not unique. Policy conclusions are also devised based on principles of design of institutions to influence the nature of transaction costs."
"We characterize Pareto optimality via ""near"" weighted utilitarian welfare maximization. One characterization sequentially maximizes utilitarian welfare functions using a finite sequence of nonnegative and eventually positive welfare weights. The other maximizes a utilitarian welfare function with a certain class of positive hyperreal weights. The social welfare ordering represented by these ""near"" weighted utilitarian welfare criteria is characterized by the standard axioms for weighted utilitarianism under a suitable weakening of the continuity axiom."
"We study a model of electoral accountability and selection whereby heterogeneous voters aggregate incumbent politician's performance data into personalized signals through paying limited attention. Extreme voters' signals exhibit an own-party bias, which hampers their ability to discern the good and bad performances of the incumbent. While this effect alone would undermine electoral accountability and selection, there is a countervailing effect stemming from partisan disagreement, which makes the centrist voter more likely to be pivotal. In case the latter's unbiased signal is very informative about the incumbent's performance, the combined effect on electoral accountability and selection can actually be a positive one. For this reason, factors that carry a negative connotation in every political discourse -- such as increasing mass polarization and shrinking attention span -- have ambiguous accountability and selection effects in general. Correlating voters' signals, if done appropriately, unambiguously improves electoral accountability and selection and, hence, voter welfare."
"In random expected utility (Gul and Pesendorfer, 2006), the distribution of preferences is uniquely recoverable from random choice. This paper shows through two examples that such uniqueness fails in general if risk preferences are random but do not conform to expected utility theory. In the first, non-uniqueness obtains even if all preferences are confined to the betweenness class (Dekel, 1986) and are suitably monotone. The second example illustrates random choice behavior consistent with random expected utility that is also consistent with random non-expected utility. On the other hand, we find that if risk preferences conform to weighted utility theory (Chew, 1983) and are monotone in first-order stochastic dominance, random choice again uniquely identifies the distribution of preferences. Finally, we argue that, depending on the domain of risk preferences, uniqueness may be restored if joint distributions of choice across a limited number of feasible sets are available."
"Recently, many matching systems around the world have been reformed. These reforms responded to objections that the matching mechanisms in use were unfair and manipulable. Surprisingly, the mechanisms remained unfair even after the reforms: the new mechanisms may induce an outcome with a blocking student who desires and deserves a school which she did not receive. However, as we show in this paper, the reforms introduced matching mechanisms which are more fair compared to the counterfactuals. First, most of the reforms introduced mechanisms that are more fair by stability: whenever the old mechanism does not have a blocking student, the new mechanism does not have a blocking student either. Second, some reforms introduced mechanisms that are more fair by counting: the old mechanism always has at least as many blocking students as the new mechanism. These findings give a novel rationale to the reforms and complement the recent literature showing that the same reforms have introduced less manipulable matching mechanisms. We further show that the fairness and manipulability of the mechanisms are strongly logically related."
"Strategy-proof mechanisms are widely used in market design. In an abstract allocation framework where outside options are available to agents, we obtain two results for strategy-proof mechanisms. They provide a unified foundation for several existing results in distinct models and imply new results in some models. The first result proves that, for individually rational and strategy-proof mechanisms, pinning down every agent's probability of choosing his outside option is equivalent to pinning down a mechanism. The second result provides a sufficient condition for two strategy-proof mechanisms to be equivalent when the number of possible allocations is finite."
"We introduce and study a model of long-run convention formation for rare interactions. Players in this model form beliefs by observing a recency-weighted sample of past interactions, to which they noisily best respond. We propose a continuous state Markov model, well-suited for our setting, and develop a methodology that is relevant for a larger class of similar learning models. We show that the model admits a unique asymptotic distribution which concentrates its mass on some minimal CURB block configuration. In contrast to existing literature of long-run convention formation, we focus on behavior inside minimal CURB blocks and provide conditions for convergence to (approximate) mixed equilibria conventions inside minimal CURB blocks."
"Carroll and Kimball (1996) have shown that, in the class of utility functions that are strictly increasing, strictly concave, and have nonnegative third derivatives, hyperbolic absolute risk aversion (HARA) is sufficient for the concavity of consumption functions in general consumption-saving problems. This paper shows that HARA is necessary, implying the concavity of consumption is not a robust prediction outside the HARA class."
"When learning from others, people tend to focus their attention on those with similar views. This is often attributed to flawed reasoning, and thought to slow learning and polarize beliefs. However, we show that echo chambers are a rational response to uncertainty about the accuracy of information sources, and can improve learning and reduce disagreement. Furthermore, overextending the range of views someone is exposed to can backfire, slowing their learning by making them less responsive to information from others. We model a Bayesian decision maker who chooses a set of information sources and then observes a signal from one. With uncertainty about which sources are accurate, focusing attention on signals close to one's own expectation can be beneficial, as their expected accuracy is higher. The optimal echo chamber balances the credibility of views similar to one's own against the usefulness of those further away."
"The problem of finding a (continuous) utility function for a semiorder has been studied since in 1956 R.D. Luce introduced in \emph{Econometrica} the notion. There was almost no results on the continuity of the representation. A similar result to Debreu's Lemma, but for semiorders, was never achieved. Recently, some necessary conditions for the existence of a continuous representation as well as some conjectures were presented by A. Estevan. In the present paper we prove these conjectures, achieving the desired version of Debreu's Open Gap Lemma for bounded semiorders. This result allows to remove the open-closed and closed-open gaps of a subset $S\subseteq \mathbb{R}$, but now keeping the constant threshold, so that $x+1<y$ if and only if $g(x)+1<g(y) \, (x,y\in S)$. Therefore, the continuous representation (in the sense of Scott-Suppes) of bounded semiorders is characterized. These results are achieved thanks to the key notion of $\epsilon$-continuity, which generalizes the idea of continuity for semiorders."
"A variety of social, economic, and political interactions have long been modelled after Blotto games. In this paper, we introduce a general model of dynamic $n$-player Blotto contests. The players have asymmetric resources, and the battlefield prizes are not necessarily homogeneous. Each player's probability of winning the prize in a battlefield is governed by a contest success function and players' resource allocation on that battlefield. We show that there exists a subgame perfect equilibrium in which players allocate their resources proportional to the battlefield prizes for every history. This result is robust to exogenous resource shocks throughout the game."
"Consider a persuasion game where both the sender and receiver are ambiguity averse with maxmin expected utility (MEU) preferences and the sender can choose to design an ambiguous information structure. This paper studies the game with an ex-ante formulation: The sender first commits to a (possibly ambiguous) information structure and then the receiver best responds by choosing an ex-ante message-contingent action plan. Under this formulation, I show it is never strictly beneficial for the sender to use an ambiguous information structure as opposed to a standard (unambiguous) information structure. This result is shown to be robust to the receiver having non-MEU Uncertainty Averse preferences but not to the sender having non-MEU preferences."
"Is the overall value of a world just the sum of values contributed by each value-bearing entity in that world? Additively separable axiologies (like total utilitarianism, prioritarianism, and critical level views) say 'yes', but non-additive axiologies (like average utilitarianism, rank-discounted utilitarianism, and variable value views) say 'no'. This distinction is practically important: additive axiologies support 'arguments from astronomical scale' which suggest (among other things) that it is overwhelmingly important for humanity to avoid premature extinction and ensure the existence of a large future population, while non-additive axiologies need not. We show, however, that when there is a large enough 'background population' unaffected by our choices, a wide range of non-additive axiologies converge in their implications with some additive axiology -- for instance, average utilitarianism converges to critical-level utilitarianism and various egalitarian theories converge to prioritiarianism. We further argue that real-world background populations may be large enough to make these limit results practically significant. This means that arguments from astronomical scale, and other arguments in practical ethics that seem to presuppose additive separability, may be truth-preserving in practice whether or not we accept additive separability as a basic axiological principle."
"We show that under plausible levels of background risk, no theory of choice under risk -- such as expected utility theory, prospect theory, or rank dependent utility -- can simultaneously satisfy the following three economic postulates: (i) Decision makers are risk-averse over small gambles, (ii) they respect stochastic dominance, and (iii) they account for background risk."
"We study stable allocations in college admissions markets where students can attend the same college under different financial terms. The deferred acceptance algorithm identifies a stable allocation where funding is allocated based on merit. While merit-based stable allocations assign the same students to college, non-merit-based stable allocations may differ in the number of students assigned to college. In large markets, this possibility requires heterogeneity in applicants' sensitivity to financial terms. In Hungary, where such heterogeneity is present, a non-merit-based stable allocation would increase the number of assigned applicants by 1.9%, and affect 8.3% of the applicants relative to any merit-based stable allocation. These findings contrast sharply with findings from the matching (without contracts) literature."
"I formulate and characterize the following two-stage choice behavior. The decision maker is endowed with two preferences. She shortlists all maximal alternatives according to the first preference. If the first preference is decisive, in the sense that it shortlists a unique alternative, then that alternative is the choice. If multiple alternatives are shortlisted, then, in a second stage, the second preference vetoes its minimal alternative in the shortlist, and the remaining members of the shortlist form the choice set. Only the final choice set is observable. I assume that the first preference is a weak order and the second is a linear order. Hence the shortlist is fully rationalizable but one of its members can drop out in the second stage, leading to bounded rational behavior. Given the asymmetric roles played by the underlying binary relations, the consequent behavior exhibits a minimal compromise between two preferences. To our knowledge it is the first Choice function that satisfies Sen's $\beta$ axiom of choice,but not $\alpha$."
"We study the information design problem in a single-unit auction setting. The information designer controls independent private signals according to which the buyers infer their binary private values. Assuming that the seller adopts the optimal auction due to Myerson (1981) in response, we characterize both the buyer-optimal information structure, which maximizes the buyers' surplus, and the sellerworst information structure, which minimizes the seller's revenue. We translate both information design problems into finite-dimensional, constrained optimization problems in which one can explicitly solve for the optimal information structures. In contrast to the case with one buyer (Roesler and Szentes, 2017), we show that with two or more buyers, the symmetric buyer-optimal information structure is different from the symmetric seller-worst information structure. The good is always sold under the seller-worst information structure but not under the buyer-optimal information structure. Nevertheless, as the number of buyers goes to infinity, both symmetric information structures converge to no disclosure. We also show that in an ex ante symmetric setting, an asymmetric information structure is never seller-worst but can generate a strictly higher surplus for the buyers than the symmetric buyer-optimal information structure."
"By analyzing the duopoly market of computer graphics cards, we categorized the effects of enterprise's technological progress into two types, namely, cost reduction and product diversification. Our model proved that technological progress is the most effective means for enterprises in this industry to increase profits. Due to the technology-intensive nature of this industry, monopolistic enterprises face more intense competition compared with traditional manufacturing. Therefore, they have more motivation for technological innovation. Enterprises aiming at maximizing profits have incentives to reduce costs and achieve a higher degree of product differentiation through technological innovation."
"We study interactions with uncertainty about demand sensitivity. In our solution concept (1) firms choose seemingly-optimal strategies given the level of sophistication of their data analytics, and (2) the levels of sophistication form best responses to one another. Under the ensuing equilibrium firms underestimate price elasticities and overestimate advertising effectiveness, as observed empirically. The misestimates cause firms to set prices too high and to over-advertise. In games with strategic complements (substitutes), profits Pareto dominate (are dominated by) those of the Nash equilibrium. Applying the model to team production games explains the prevalence of overconfidence among entrepreneurs and salespeople."
"We consider the problem of allocating indivisible objects to agents when agents have strict preferences over objects. There are inherent trade-offs between competing notions of efficiency, fairness and incentives in assignment mechanisms. It is, therefore, natural to consider mechanisms that satisfy two of these three properties in their strongest notions, while trying to improve on the third dimension. In this paper, we are motivated by the following question: Is there a strategy-proof and envy-free random assignment mechanism more efficient than equal division?   Our contributions in this paper are twofold. First, we further explore the incompatibility between efficiency and envy-freeness in the class of strategy-proof mechanisms. We define a new notion of efficiency that is weaker than ex-post efficiency and prove that any strategy-proof and envy-free mechanism must sacrifice efficiency even in this very weak sense. Next, we introduce a new family of mechanisms called Pairwise Exchange mechanisms and make the surprising observation that strategy-proofness is equivalent to envy-freeness within this class. We characterize the set of all neutral and strategy-proof (and hence, also envy-free) mechanisms in this family and show that they admit a very simple linear representation."
"Generating payoff matrices of normal-form games at random, we calculate the frequency of games with a unique pure strategy Nash equilibrium in the ensemble of $n$-player, $m$-strategy games. These are perfectly predictable as they must converge to the Nash equilibrium. We then consider a wider class of games that converge under a best-response dynamic, in which each player chooses their optimal pure strategy successively. We show that the frequency of convergent games goes to zero as the number of players or the number of strategies goes to infinity. In the $2$-player case, we show that for large games with at least $10$ strategies, convergent games with multiple pure strategy Nash equilibria are more likely than games with a unique Nash equilibrium. Our novel approach uses an $n$-partite graph to describe games."
We study the problem of assigning objects to agents in the presence of arbitrary linear constraints when agents are allowed to be indifferent between objects. Our main contribution is the generalization of the (Extended) Probabilistic Serial mechanism via a new mechanism called the Constrained Serial Rule. This mechanism is computationally efficient and maintains desirable efficiency and fairness properties namely constrained ordinal efficiency and envy-freeness among agents of the same type. Our mechanism is based on a linear programming approach that accounts for all constraints and provides a re-interpretation of the bottleneck set of agents that form a crucial part of the Extended Probabilistic Serial mechanism.
"This paper analyses how risk-taking behaviour and preferences over consumption rank can emerge as a neutrally stable equilibrium when individuals face an anti-coordination task. If in an otherwise homogeneous society information about relative consumption becomes available, this cannot be ignored. Despite concavity in the objective function, stable types must be willing to accept risky gambles to differentiate themselves, and thus allow for coordination. Relative consumption acts as a form of costly communication. This suggests status preferences to be salient in settings where miscoordination is particularly costly."
"Cross-group externalities and network effects in two-sided platform markets shape market structure and competition policy, and are the subject of extensive study. Less understood are the within-group externalities that arise when the platform designs many-to-many matchings: the value to agent $i$ of matching with agent $j$ may depend on the set of agents with which $j$ is matched. These effects are present in a wide range of settings in which firms compete for individuals' custom or attention. I characterize platform-optimal matchings in a general model of many-to-many matching with within-group externalities. I prove a set of comparative statics results for optimal matchings, and show how these can be used to analyze the welfare effects various changes, including vertical integration by the platform, horizontal mergers between firms on one side of the market, and changes in the platform's information structure. I then explore market structure and regulation in two in-depth applications. The first is monopolistic competition between firms on a retail platform such as Amazon. The second is a multi-channel video program distributor (MVPD) negotiating transfer fees with television channels and bundling these to sell to individuals."
"In many settings, multiple uninformed agents bargain simultaneously with a single informed agent in each of multiple periods. For example, workers and firms negotiate each year over salaries, and the firm has private information about the value of workers' output. I study the effects of transparency in these settings; uninformed agents may observe others' past bargaining outcomes, e.g. wages. I show that in equilibrium, each uninformed agent will choose in each period whether to try to separate the informed agent's types (screen) or receive the same outcome regardless of type (pool). In other words, the agents engage in a form of experimentation via their bargaining strategies. There are two main theoretical insights. First, there is a complementary screening effect: the more agents screen in equilibrium, the lower the information rents that each will have to pay. Second, the payoff of the informed agent will have a certain supermodularity property, which implies that equilibria with screening are ""fragile"" to deviations by uninformed agents. I apply the results to study pay-secrecy regulations and anti-discrimination policy. I show that, surprisingly, penalties for pay discrimination have no impact on bargaining outcomes. I discuss how this result depends on the legal framework for discrimination cases, and suggest changes to enhance the efficacy of anti-discrimination regulations. In particular, anti-discrimination law should preclude the so-called ""salary negotiation defense""."
"We study the problem of allocating $n$ indivisible objects to $n$ agents when the latter can express strict and purely ordinal preferences and preference intensities. We suggest a rank-based criterion to make ordinal interpersonal comparisons of preference intensities in such an environment without assuming interpersonally comparable utilities. We then define an allocation to be ""intensity-efficient"" if it is Pareto efficient and also such that, whenever another allocation assigns the same pairs of objects to the same pairs of agents but in a ""flipped"" way, then the former assigns the commonly preferred alternative within every such pair to the agent who prefers it more. We show that an intensity-efficient allocation exists for all 1,728 profiles when $n=3$."
"A seller is selling a pair of divisible complementary goods to an agent. The agent consumes the goods only in a specific ratio and freely disposes of excess in either goods. The value of the bundle and the ratio are private information of the agent. In this two-dimensional type space model, we characterize the incentive constraints and show that the optimal (expected revenue-maximizing) mechanism is a ratio-dependent posted price or a posted price mechanism for a class of distributions. We also show that the optimal mechanism is a posted price mechanism when the value and the ratio are independently distributed."
"We analyze situations in which players build reputations for honesty rather than for playing particular actions. A patient player facing a sequence of short-run opponents makes an announcement about their intended action after observing an idiosyncratic shock, and before players act. The patient player is either an honest type whose action coincides with their announcement, or an opportunistic type who can freely choose their actions. We show that the patient player can secure a high payoff by building a reputation for being honest when the short-run players face uncertainty about which of the patient player's actions are currently feasible, but may receive a low payoff when there is no such uncertainty."
"Despite the tremendous successes of science in providing knowledge and technologies, the Replication Crisis has highlighted that scientific institutions have much room for improvement. Peer-review is one target of criticism and suggested reforms. However, despite numerous controversies peer review systems, plus the obvious complexity of the incentives affecting the decisions of authors and reviewers, there is very little systematic and strategic analysis of peer-review systems. In this paper, we begin to address this feature of the peer-review literature by applying the tools of game theory. We use simulations to develop an evolutionary model based around a game played by authors and reviewers, before exploring some of its tendencies. In particular, we examine the relative impact of double-blind peer-review and open review on incentivising reviewer effort under a variety of parameters. We also compare (a) the impact of one review system versus another with (b) other alterations, such as higher costs of reviewing. We find that is no reliable difference between peer-review systems in our model. Furthermore, under some conditions, higher payoffs for good reviewing can lead to less (rather than more) author effort under open review. Finally, compared to the other parameters that we vary, it is the exogenous utility of author effort that makes an important and reliable difference in our model, which raises the possibility that peer-review might not be an important target for institutional reforms."
"We identify a new dynamic agency problem: that of incentivising the prompt disclosure of productive information. To study it, we introduce a general model in which a technological breakthrough occurs at an uncertain time and is privately observed by an agent, and a principal must incentivise disclosure via her control of a payoff-relevant physical allocation. We uncover a deadline structure of optimal mechanisms: they have a simple deadline form in an important special case, and a graduated deadline structure in general. We apply our results to the design of unemployment insurance schemes."
"We study a persuasion problem in which a sender designs an information structure to induce a non-Bayesian receiver to take a particular action. The receiver, who is privately informed about his preferences, is a wishful thinker: he is systematically overoptimistic about the most favorable outcomes. We show that wishful thinking can lead to a qualitative shift in the structure of optimal persuasion compared to the Bayesian case, whenever the sender is uncertain about what the receiver perceives as the best-case outcome in his decision problem."
"We study a game of strategic information design between a sender, who chooses state-dependent information structures, a mediator who can then garble the signals generated from these structures, and a receiver who takes an action after observing the signal generated by the first two players. We characterize sufficient conditions for information revelation, compare outcomes with and without a mediator and provide comparative statics with regard to the preferences of the sender and the mediator. We also provide novel conceptual and computational insights about the set of feasible posterior beliefs that the sender can induce, and use these results to obtain insights about equilibrium outcomes. The sender never benefits from mediation, while the receiver might. Strikingly, the receiver benefits when the mediator's preferences are not perfectly aligned with hers; rather the mediator should prefer more information revelation than the sender, but less than perfect revelation."
"We consider the allocation of indivisible objects when agents have preferences over their own allocations, but share the ownership of the resources to be distributed. Examples might include seats in public schools, faculty offices, and time slots in public tennis courts. Given an allocation, groups of agents who would prefer an alternative allocation might challenge it. An assignment is popular if it is not challenged by another one. By assuming that agents' ability to challenge allocations can be represented by weighted votes, we characterize the conditions under which popular allocations might exist and when these can be implemented via strategy-proof mechanisms. Serial dictatorships that use orderings consistent with the agents' weights are not only strategy-proof and Pareto efficient, but also popular, whenever these assignments exist. We also provide a new characterization for serial dictatorships as the only mechanisms that are popular, strategy-proof, non-wasteful, and satisfy a consistency condition."
"We evaluate the goal of maximizing the number of individuals matched to acceptable outcomes. We show that it implies incentive, fairness, and implementation impossibilities. Despite that, we present two classes of mechanisms that maximize assignments. The first are Pareto efficient, and undominated -- in terms of number of assignments -- in equilibrium. The second are fair for unassigned students and assign weakly more students than stable mechanisms in equilibrium."
"We introduce a new family of mechanisms for one-sided matching markets, denoted pick-an-object (PAO) mechanisms. When implementing an allocation rule via PAO, agents are asked to pick an object from individualized menus. These choices may be rejected later on, and these agents are presented with new menus. When the procedure ends, agents are assigned the last object they picked. We characterize the allocation rules that can be sequentialized by PAO mechanisms, as well as the ones that can be implemented in a robust truthful equilibrium. We justify the use of PAO as opposed to direct mechanisms by showing that its equilibrium behavior is closely related to the one in obviously strategy-proof (OSP) mechanisms, but implements commonly used rules, such as Gale-Shapley DA and top trading cycles, which are not OSP-implementable. We run laboratory experiments comparing truthful behavior when using PAO, OSP, and direct mechanisms to implement different rules. These indicate that agents are more likely to behave in line with the theoretical prediction under PAO and OSP implementations than their direct counterparts."
"During its history, the ultimate goal of economics has been to develop similar frameworks for modeling economic behavior as invented in physics. This has not been successful, however, and current state of the process is the neoclassical framework that bases on static optimization. By using a static framework, however, we cannot model and forecast the time paths of economic quantities because for a growing firm or a firm going into bankruptcy, a positive profit maximizing flow of production does not exist. Due to these problems, we present a dynamic theory for the production of a profit-seeking firm where the adjustment may be stable or unstable. This is important, currently, because we should be able to forecast the possible future bankruptcies of firms due to the Covid-19 pandemic. By using the model, we can solve the time moment of bankruptcy of a firm as a function of several parameters. The proposed model is mathematically identical with Newtonian model of a particle moving in a resisting medium, and so the model explains the reasons that stop the motion too. The frameworks for modeling dynamic events in physics are thus applicable in economics, and we give reasons why physics is more important for the development of economics than pure mathematics. (JEL D21, O12)   Keywords: Limitations of neoclassical framework, Dynamics of production, Economic force, Connections between economics and physics."
"How to guarantee that firms perform due diligence before launching potentially dangerous products? We study the design of liability rules when (i) limited liability prevents firms from internalizing the full damage they may cause, (ii) penalties are paid only if damage occurs, regardless of the product's inherent riskiness, (iii) firms have private information about their products' riskiness before performing due diligence. We show that (i) any liability mechanism can be implemented by a tariff that depends only on the evidence acquired by the firm if a damage occurs, not on any initial report by the firm about its private information, (ii) firms that assign a higher prior to product riskiness always perform more due diligence but less than is socially optimal, and (iii) under a simple and intuitive condition, any type-specific launch thresholds can be implemented by a monotonic tariff."
"In many countries and institutions around the world, the hiring of workers is made through open competitions. In them, candidates take tests and are ranked based on scores in exams and other predetermined criteria. Those who satisfy some eligibility criteria are made available for hiring from a ""pool of workers."" In each of an ex-ante unknown number of rounds, vacancies are announced, and workers are then hired from that pool. When the scores are the only criterion for selection, the procedure satisfies desired fairness and independence properties. We show that when affirmative action policies are introduced, the established methods of reserves and procedures used in Brazil, France, and Australia, fail to satisfy those properties. We then present a new rule, which we show to be the unique rule that extends static notions of fairness to problems with multiple rounds while satisfying aggregation independence, a consistency requirement. Finally, we show that if multiple institutions hire workers from a single pool, even minor consistency requirements are incompatible with variations in the institutions' rules."
"Along with the energy transition, the energy markets change their organization toward more decentralized and self-organized structures, striving for locally optimal profits. These tendencies may endanger the physical grid stability. One realistic option is the exhaustion of reserve energy due to an abuse by arbitrageurs. We map the energy market to different versions of a minority game and determine the expected amount of arbitrage as well as its fluctuations as a function of the model parameters. Of particular interest are the impact of heterogeneous contributions of arbitrageurs, the interplay between external stochastic events and nonlinear price functions of reserve power, and the effect of risk aversion due to suspected penalties. The non-monotonic dependence of arbitrage on the control parameters reveals an underlying phase transition that is the counterpart to replica symmetry breaking in spin glasses. As conclusions from our results we propose economic and statutory measures to counteract a detrimental effect of arbitrage."
"The role of specific cognitive processes in deviations from constant discounting in intertemporal choice is not well understood. We evaluated decreased impatience in intertemporal choice tasks independent of discounting rate and non-linearity in long-scale time representation; nonlinear time representation was expected to explain inconsistencies in discounting rate. Participants performed temporal magnitude estimation and intertemporal choice tasks. Psychophysical functions for time intervals were estimated by fitting linear and power functions, while discounting functions were estimated by fitting exponential and hyperbolic functions. The temporal magnitude estimates of 65% of the participants were better fit with power functions (mostly compression). 63% of the participants had intertemporal choice patterns corresponding best to hyperbolic functions. Even when the perceptual bias in the temporal magnitude estimations was compensated in the discounting rate computation, the data of 8 out of 14 participants continued exhibiting temporal inconsistency. The results suggest that temporal inconsistency in discounting rate can be explained to different degrees by the bias in temporal representations. Non-linearity in temporal representation and discounting rate should be evaluated on an individual basis. Keywords: Intertemporal choice, temporal magnitude, model comparison, impatience, time inconsistency"
"We seek to take a different approach in deriving the optimal search policy for the repeated consumer search model found in Fishman and Rob (1995) with the main motivation of dropping the assumption of prior knowledge of the price distribution $F(p)$ in each period. We will do this by incorporating the famous multi-armed bandit problem (MAB). We start by modifying the MAB framework to fit the setting of the repeated consumer search model and formulate the objective as a dynamic optimization problem. Then, given any sequence of exploration, we assign a value to each store in that sequence using Bellman equations. We then proceed to break down the problem into individual optimal stopping problems for each period which incidentally coincides with the framework of the famous secretary problem where we proceed to derive the optimal stopping policy. We will see that implementing the optimal stopping policy in each period solves the original dynamic optimization by `forward induction' reasoning."
"We introduce a new updating rule, the conditional maximum likelihood rule (CML) for updating ambiguous information. The CML formula replaces the likelihood term in Bayes' rule with the maximal likelihood of the given signal conditional on the state. We show that CML satisfies a new axiom, increased sensitivity after updating, while other updating rules do not. With CML, a decision maker's posterior is unaffected by the order in which independent signals arrive. CML also accommodates recent experimental findings on updating signals of unknown accuracy and has simple predictions on learning with such signals. We show that an information designer can almost achieve her maximal payoff with a suitable ambiguous information structure whenever the agent updates according to CML."
"We define notions of dominance between two actions in a dynamic game. Local dominance considers players who have a blurred view of the future and compare the two actions by first focusing on the outcomes that may realize at the current stage. When considering the possibility that the game may continue, they can only check that the local comparison is not overturned under the assumption of ""continuing in the same way"" after the two actions (in a newly defined sense). Despite the lack of forward planning, local dominance solves dynamic mechanisms that were found easy to play and implements social choice functions that cannot be implemented in obviously-dominant strategies."
"We propose a model of incomplete \textit{twofold multiprior preferences}, in which an act $f$ is ranked above an act $g$ only when $f$ provides higher utility in a worst-case scenario than what $g$ provides in a best-case scenario. The model explains failures of contingent reasoning, captured through a weakening of the state-by-state monotonicity (or dominance) axiom. Our model gives rise to rich comparative statics results, as well as extension exercises, and connections to choice theory. We present an application to second-price auctions."
"We present a directed variant of Salop (1979) model to analyze bus transport dynamics. The players are operators competing in cooperative and non-cooperative games. Utility, like in most bus concession schemes in emerging countries, is proportional to the total fare collection. Competition for picking up passengers leads to well documented and dangerous driving practices that cause road accidents, traffic congestion and pollution. We obtain theoretical results that support the existence and implementation of such practices, and give a qualitative description of how they come to occur. In addition, our results allow to compare the current or base transport system with a more cooperative one."
"This study examines the mechanism design problem for public goods provision in a large economy with $n$ independent agents. We propose a class of dominant-strategy incentive compatible and ex-post individually rational mechanisms, which we call the adjusted mean-thresholding (AMT) mechanisms. We show that when the cost of provision grows slower than the $\sqrt{n}$-rate, the AMT mechanisms are both eventually ex-ante budget balanced and asymptotically efficient. When the cost grows faster than the $\sqrt{n}$-rate, in contrast, we show that any incentive compatible, individually rational, and eventually ex-ante budget balanced mechanism must have provision probability converging to zero and hence cannot be asymptotically efficient. The AMT mechanisms have a simple form and are more informationally robust when compared to, for example, the second-best mechanism. This is because the construction of an AMT mechanism depends only on the first moment of the valuation distribution."
"I study costly information acquisition in a two-sided matching problem, such as matching applicants to schools. An applicant's utility is a sum of common and idiosyncratic components. The idiosyncratic component is unknown to the applicant but can be learned at a cost. When applicants are assigned using an ordinal strategy-proof mechanism, too few acquire information, generating a significant welfare loss. Affirmative action and other realistic policies may lead to a Pareto improvement. As incentives to acquire information differ across mechanisms, ignoring such incentives may lead to incorrect welfare assessments, for example, in comparing a popular Immediate Assignment and an ordinal strategy-proof mechanism."
"The Kolkata Paise Restaurant Problem is a challenging game, in which $n$ agents must decide where to have lunch during their lunch break. The game is very interesting because there are exactly $n$ restaurants and each restaurant can accommodate only one agent. If two or more agents happen to choose the same restaurant, only one gets served and the others have to return back to work hungry. In this paper we tackle this problem from an entirely new angle. We abolish certain implicit assumptions, which allows us to propose a novel strategy that results in greater utilization for the restaurants. We emphasize the spatially distributed nature of our approach, which, for the first time, perceives the locations of the restaurants as uniformly distributed in the entire city area. This critical change in perspective has profound ramifications in the topological layout of the restaurants, which now makes it completely realistic to assume that every agent has a second chance. Every agent now may visit, in case of failure, more than one restaurants, within the predefined time constraints."
"Large-scale institutional changes require strong commitment and involvement of all stakeholders. We use the standard framework of cooperative game theory developed by Ichiishi (1983, pp. 78-149) to: (i) establish analytically the difference between policy maker and political leader; (ii) formally study interactions between a policy maker and his followers; (iii) examine the role of leadership in the implementation of structural reforms. We show that a policy maker can be both partisan and non-partisan, while a political leader can only be non-partisan. Following this distinction, we derive the probability of success of an institutional change, as well as the nature of the gain that such a change would generate on the beneficiary population. Based on the restrictions of this simple mathematical model and using some evidence from the Congolese experience between 2012 and 2016, we show that institutional changes can indeed benefit the majority of the population, when policy makers are truly partisan."
"We study interactions between strategic players and markets whose behavior is guided by an algorithm. Algorithms use data from prior interactions and a limited set of decision rules to prescribe actions. While as-if rational play need not emerge if the algorithm is constrained, it is possible to guide behavior across a rich set of possible environments using limited details. Provided a condition known as weak learnability holds, Adaptive Boosting algorithms can be specified to induce behavior that is (approximately) as-if rational. Our analysis provides a statistical perspective on the study of endogenous model misspecification."
"We study information design settings where the designer controls information about a state, and there are multiple agents interacting in a game who are privately informed about their types. Each agent's utility depends on all agents' types and actions, as well as (linearly) on the state. To optimally screen the agents, the designer first asks agents to report their types and then sends a private action recommendation to each agent whose distribution depends on all reported types and the state. We show that there always exists an optimal mechanism which is laminar partitional. Such a mechanism partitions the state space for each type profile and recommends the same action profile for states that belong to the same partition element. Furthermore, the convex hulls of any two partition elements are such that either one contains the other or they have an empty intersection. In the single-agent case, each state is either perfectly revealed or lies in an interval in which the number of different signal realizations is at most the number of different types of the agent plus two. A similar result is established for the multi-agent case.   We also highlight the value of screening: without screening the best achievable payoff could be as low as one over the number of types fraction of the optimal payoff. Along the way, we shed light on the solutions of optimization problems over distributions subject to a mean-preserving contraction constraint and additional side constraints, which might be of independent interest."
"Two types of interventions are commonly implemented in networks: characteristic intervention, which influences individuals' intrinsic incentives, and structural intervention, which targets the social links among individuals. In this paper we provide a general framework to evaluate the distinct equilibrium effects of both types of interventions. We identify a hidden equivalence between a structural intervention and an endogenously determined characteristic intervention. Compared with existing approaches in the literature, the perspective from such an equivalence provides several advantages in the analysis of interventions that target network structure. We present a wide range of applications of our theory, including identifying the most wanted criminal(s) in delinquent networks and targeting the key connector for isolated communities."
"I study dynamic random utility with finite choice sets and exogenous total menu variation, which I refer to as stochastic utility (SU). First, I characterize SU when each choice set has three elements. Next, I prove several mathematical identities for joint, marginal, and conditional Block--Marschak sums, which I use to obtain two characterizations of SU when each choice set but the last has three elements. As a corollary under the same cardinality restrictions, I sharpen an axiom to obtain a characterization of SU with full support over preference tuples. I conclude by characterizing SU without cardinality restrictions. All of my results hold over an arbitrary finite discrete time horizon."
"This paper provides a behavioral analysis of conservatism in beliefs. I introduce a new axiom, Dynamic Conservatism, that relaxes Dynamic Consistency when information and prior beliefs ""conflict."" When the agent is a subjective expected utility maximizer, Dynamic Conservatism implies that conditional beliefs are a convex combination of the prior and the Bayesian posterior. Conservatism may result in belief dynamics consistent with confirmation bias, representativeness, and the good news-bad news effect, suggesting a deeper behavioral connection between these biases. An index of conservatism and a notion of comparative conservatism are characterized. Finally, I extend conservatism to the case of an agent with incomplete preferences that admit a multiple priors representation."
"We study the robust double auction mechanisms, that is, the double auction mechanisms that satisfy dominant strategy incentive compatibility, ex-post individual rationality and ex-post budget balance. We first establish that the price in any robust mechanism does not depend on the valuations of the trading players. We next establish that, with a non-bossiness assumption, the price in any robust mechanism does not depend on players' valuations at all, whether trading or non-trading. Our main result is the characterization result that, with a non-bossy assumption along with other assumptions on the properties of the mechanism, the generalized posted mechanism in which a constant price is posted for each possible set of traders is the only robust double auction mechanism. We also show that, even without the non-bossiness assumption, it is quite difficult to find a reasonable robust double auction mechanism other than the generalized posted price mechanism."
"We propose a multivariate extension of Yaari's dual theory of choice under risk. We show that a decision maker with a preference relation on multidimensional prospects that preserves first order stochastic dominance and satisfies comonotonic independence behaves as if evaluating prospects using a weighted sum of quantiles. Both the notions of quantiles and of comonotonicity are extended to the multivariate framework using optimal transportation maps. Finally, risk averse decision makers are characterized within this framework and their local utility functions are derived. Applications to the measurement of multi-attribute inequality are also discussed."
"We introduce the refined assortment optimization problem where a firm may decide to make some of its products harder to get instead of making them unavailable as in the traditional assortment optimization problem. Airlines, for example, offer fares with severe restrictions rather than making them unavailable. This is a more subtle way of handling the trade-off between demand induction and demand cannibalization. For the latent class MNL model, a firm that engages in refined assortment optimization can make up to $\min(n,m)$ times more than one that insists on traditional assortment optimization, where $n$ is the number of products and $m$ the number of customer types. Surprisingly, the revenue-ordered assortment heuristic has the same performance guarantees relative to {\em personalized} refined assortment optimization as it does to traditional assortment optimization. Based on this finding, we construct refinements of the revenue-order heuristic and measure their improved performance relative to the revenue-ordered assortment and the optimal traditional assortment optimization problem. We also provide tight bounds on the ratio of the expected revenues for the refined versus the traditional assortment optimization for some well known discrete choice models."
"Sanctioned by its constitution, India is home to the world's most comprehensive affirmative action program, where historically discriminated groups are protected with vertical reservations implemented as ""set asides,"" and other disadvantaged groups are protected with horizontal reservations implemented as ""minimum guarantees."" A mechanism mandated by the Supreme Court in 1995 suffers from important anomalies, triggering countless litigations in India. Foretelling a recent reform correcting the flawed mechanism, we propose the 2SMG mechanism that resolves all anomalies, and characterize it with desiderata reflecting laws of India. Subsequently rediscovered with a high court judgment and enforced in Gujarat, 2SMG is also endorsed by Saurav Yadav v. State of UP (2020), in a Supreme Court ruling that rescinded the flawed mechanism. While not explicitly enforced, 2SMG is indirectly enforced for an important subclass of applications in India, because no other mechanism satisfies the new mandates of the Supreme Court."

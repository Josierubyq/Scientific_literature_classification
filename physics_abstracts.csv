abstract
"In 1926 Albert Einstein gave a clear explanation of the physical processes involved in the meander formation and evolution in open channels (Einstein, 1926). Although this work is far from being recognized as one of his greatest achievements, such as his annus mirabilis papers in 1905, he shows a truly remarkable didactic skills that make it easy to understand even to the non-specialist. In particular, a brilliant explanation of the tea leaf paradox can be found in this paper of 1926, presented as a simple experiment for clarifying the role of Earth rotation and flow curvature in the differential river banks erosion. This work deserves to be considered as a pioneering work that has laid a basic knowledge in currently very active research fields in fluvial geomorphology, estuarine physics, and hydraulic engineering. In response to the curiosity aroused and transmitted to the authors over the years by undergraduates and MSc. students, and also due to its historical and scientific significance, we present here the Spanish translation of Einstein's original work published in German in 1926 in Die Naturwissenschaften (Einstein, 1926). Einstein's drawings have not been interpreted, but just updated preserving their original spirit."
"For several decades a portrait of Johannes Kepler has been widely circulating among professional astronomers, scientific and academic institutions, and the general public. Despite its provenance and identification having been questioned in the early part of the last century, this painting has reached iconic status. We review its history from its first mention in the literature in the 1870s to a published but virtually unknown judgment of competent art experts of the 1920s that the work is in fact an early nineteenth century forgery. We display the painting in context with other more secure portraits and suggest that if it is based on anything, the painting may derive from the well known portrait from life of Michael M\""astlin. This correction takes on certain urgency since 2021 is the 450th anniversary of Kepler's birth."
"Scientific education and divulgation not only amplify people's vocabulary and repertory of scientific concepts but, at the same time, promote the diffusion of certain conceptual and cognitive metaphors. Here we make some hypothesis about this process, proposing a classification in terms of visible, invisible, basic and derived metaphors. We focus our attention in contemporary and classical physics metaphors applied to psychological and socio-economical phenomena, and we study two exemplar cases through an exhaustive exam of the online content of large Brazilian journalistic portals. Finally, we present implications and suggestions from the cognitive metaphor theory for the scientific education and divulgation process."
"Using the Gaussian-03 for ab initio calculations, we have studied interaction of different acid molecules with a single water molecule. The molecular and supermolecular optimized structures were found with the Becke-3-Lee-Yang-Parr (B3LYP-hybrid potential) calculations of density-functional theory (DFT) methods as well as the Moeller-Plesset second-order perturbation theory, using the basis set of Aug-cc-pVDZ quality and the CRENBL ECP effective core potential for molecules containing heavy iodine atom. Possible isomers of studied acids and supermolecules, consisting of acid molecules coupled with a single water molecule, are shown. Energies, zero-point energies (ZPEs), thermal enthalpies and free energies, as well as the corresponding binding energies for the theoretical methods were calculated. It was found that optimized structures of supermolecular isomers with lowest energies corresponding to the global minimum on the potential energy surfaces can be different for both theories. The simplest structure acids H2S and H2Se, forming acid-water supermolecules, can give clear evidence of disagreement of the two theoretical methods concerning optimization of lowest energy structures, because the B3LYP-DFT method gives the lowest-energy structure for the first supermolecular isomer, but the MP2 method for the second possible isomer. A dramatic difference between potential energy surfaces for both theories applying to the optimized structure finding of the H2SO3-H2O supermolecular isomers was found, because MP2 supermolecular geometries cannot exist for the corresponding B3LYP-DFT ones, for which the frequency characteristics of the supermolecular isomers were also calculated. In general, the binding energies and ZPE ones for the MP2 method are 10-15% larger than those for the B3LYP-DFT method."
"We analyze retrospectively/prospectively the transient variations of three different physical parameters of atmosphere during the time of M7.8 and M7.3 events in Nepal: outgoing earth radiation (OLR), GPS/TEC and the thermodynamic proprieties in the lower atmosphere. We found that in mid March 2015 a rapid augment of satellite observed earth radiation in atmosphere and the anomaly located in close vicinity to the future M7.8 epicenter reached the maximum on April 21-22. Our continuous satellite analysis revealed prospectively the new strong anomaly on May 3th, which was the reason to contemplate another large event in the area. On May 12, 2015 a large aftershock of M7.3 occurred. The analysis of air temperature from weather ground station near Katmandu shows analogous patterns with offset 1-2 days earlier to the satellite anomalies. The GPS/TEC data analysis indicates an augment and variation in electron density reaching a maximum value during April 22-24 period. A strong negative TEC anomaly in the crest of EIA (Equatorial Ionospheric Anomaly) has occurred on April 21st and strong positive on April 24th, 2015. Our preliminary results show correlation between the pre-earthquake atmospheric and ionospheric anomalies and the occurrence of 2015 M7.8 and M7.3 events in Nepal."
"Despite efforts to stabilize CO_2 concentrations, it is possible that the climate system could respond abruptly with catastrophic consequences. Intentional intervention in the climate system to avoid or ameliorate such consequences has been proposed as one possible response, should such a scenario arise. In a one-week study, the authors of this report conducted a technical review and evaluation of proposed climate engineering concepts that might serve as a rapid palliative response to such climate emergency scenarios.   Because of their potential to induce a prompt (less than one year) global cooling, this study concentrated on Shortwave Climate Engineering (SWCE) methods for moderately reducing the amount of shortwave solar radiation reaching the Earth. The study's main objective was to outline a decade-long agenda of technical research that would maximally reduce the uncertainty surrounding the benefits and risks associated with SWCE. For rigor of technical analysis, the study focused the research agenda on one particular SWCE concept--stratospheric aerosol injection--and in doing so developed several conceptual frameworks and methods valuable for assessing any SWCE proposal."
"The effects of coherently enhanced radiation reaction on the motion of subwavelength electron bunches in interaction with intense laser pulses are analyzed. The radiation reaction force behaves as a radiation pressure in the laser beam direction, combined with a viscous force in the perpendicular direction. Due to Coulomb expansion of the electron bunch, coherent radiation reaction effects only occur in the initial stage of the laser-bunch interaction while the bunch is still smaller than the wavelength. It is shown that this initial stage can have observable effects on the trajectory of the bunch. By scaling the system to larger bunch charges, these effects may be increased to such an extent that they can suppress the radial instability normally found in ponderomotive acceleration schemes, thereby enabling the full potential of laser-vacuum electron bunch acceleration to GeV energies."
"The analysis of the scattered signal was carried out in the cases of ground scatter and ionospheric scatter. The analysis is based on the data of the decameter coherent EKB ISTP SB RAS radar. In the paper the signals scattered in each sounding run were analyzed before their statistical averaging. Based on the analysis, a model is constructed for ionospheric scatter and ground scatter signal, based on previously studied mechanisms. Within the framework of the Bayesian approach and based on large number of the data, the technique for identifying the two types of signals is constructed based on their different nature. The technique works without using traditional SuperDARN methods for estimating scattered signal parameters - spectral width, Doppler drift velocity or ray-tracing. The statistical analysis of the results was carried out. The total error produced by our IQ algorithm over the selected data was $13.3\%$, that is about two times less than total error produced by traditional algorithms."
"We report on theoretical courses by Fermi and Majorana, giving evidence of the first appearance and further development of Quantum Mechanics teaching in Italy. On the basis of original documents, we make a comparison between Fermi's and Majorana's approaches. A detailed analysis is carried out of Fermi's course on Theoretical Physics attended by Majorana in 1927-28. Three (previously unknown) programs on advanced Physics courses submitted by Majorana to the University of Rome between 1933 and 1936 and the course he held in Naples in 1938 complete our analysis: Fermi's phenomenological approach resounded in Majorana, who however combined it with a deeper theoretical approach, closer to the modern way of presenting Quantum Mechanics."
"Although several optical techniques have been recently developed in order to overcome the resolution limit in microscopy, the imaging of sub-wavelength features is still a real challenge. In practise, super-resolution techniques remain difficult to build or are photo-toxic for the biological samples. However, microsphere-assisted microscopy has recently made super-resolution imaging accessible to scientists (e.g. optical metrologists, engineers and biologists). This paper presents an easy-to-implement optical setup to perform full-field and contactless super-resolution measurements of nanostructured media or biological elements. For this purpose, a classical microscope was enhanced by introducing a transparent microsphere. We show that this rather simple approach makes it possible to achieve a lateral resolution of 200 nm in air, i.e. the visualization of feature sizes of 100 nm."
"We considered a novel energy storage system based on the compression of air through pumped water. Differently from CAES on trial, the proposed indirect compression leaves the opportunity to choose the kind of compression from adiabatic to isothermal. The energy storage process could be both fast or slow leading to different configuration and applications. These novel storage system are modular and could be applied in different scales for different locations and applications, being very flexible in charge and discharge process. The system may offer an ideal energy buffer for wind and solar storage with no (or negligible) environment hazard. The main features of this novel energy storage system will be showed together with overall energy and power data."
"Dissociative electron attachment, that is, the cleavage of chemical bonds induced by low-energy electrons, is difficult to model with standard quantum-chemical methods because the involved anions are not bound but subject to autodetachment. We present here a new computational development for simulating the dynamics of temporary anions on complex-valued potential energy surfaces. The imaginary part of these surfaces describes electron loss, whereas the gradient of the real part represents the force on the nuclei. In our method, the forces are computed analytically based on Hartree-Fock theory with a complex absorbing potential.   $Ab\ initio$ molecular dynamics simulations for the temporary anions of dinitrogen, ethylene, chloroethane, and the five mono- to tetrachlorinated ethylenes show qualitative agreement with experiments and offer mechanistic insights into dissociative electron attachments. The results also demonstrate how our method evenhandedly deals with molecules that may undergo dissociation upon electron attachment and those which only undergo autodetachment."
"We describe a sounding technique that allows us to improve spatial resolution of Irkutsk Incoherent Scatter Radar without loosing spectral resolution. The technique is based on transmitting of rectangle pulses of different duration in various sounding runs and subtracting correlation matrixes. Theoretically and experimentally we have shown, that subtraction of the mean-square parameters of the scattered signal for different kinds of the sounding signal one from another allows us to solve the problem within the framework of quasi-static ionospheric parameters approximation."
"Trans-ionospheric pulse pairs are the most powerful natural radio signals on the Earth and associated with lightning. They have been discovered for two decades by satellites, but their origin still remains elusive. Here we attribute these radio signals to relativistic electrons produced by cloud-to-ground lightning. When these electrons strike the ground, radio bursts are emitted towards space in a narrow cone. This model naturally explains the interval, duration, polarization, coherence and bimodal feature of the pulse pairs. Based on electron parameters inferred from x-ray observation of lightning, the calculated signal intensity agrees with the measurement of satellites. Our results are useful to develop global warning system of storms and hurricane based on GPS satellites."
"Modern machine learning force fields (ML-FF) are able to yield energy and force predictions at the accuracy of high-level $ab~initio$ methods, but at a much lower computational cost. On the other hand, classical molecular mechanics force fields (MM-FF) employ fixed functional forms and tend to be less accurate, but considerably faster and transferable between molecules of the same class. In this work, we investigate how both approaches can complement each other. We contrast the ability of ML-FF for reconstructing dynamic and thermodynamic observables to MM-FFs in order to gain a qualitative understanding of the differences between the two approaches. This analysis enables us to modify the generalized AMBER force field (GAFF) by reparametrizing short-range and bonded interactions with more expressive terms to make them more accurate, without sacrificing the key properties that make MM-FFs so successful."
"In 1923 Louis de Broglie (1892-1987) discovered the material waves and six years later received the Nobel price for this discovery. Apart these well known facts this French physicist nevertheless seems to be forgotten. Details of his life are as unknown as his efforts to describe quantum mechanics in a deterministic and objective way. Especially the actual discussion concerning the interpretation of quantum mechanics seems to justify a deeper occupation with the scientific work of Louis de Broglie. In this context the important influence of Albert Einstein is of special interest; for his photons announce the existence of material waves and it may surprise that Einstein himself did not postulate them. The basis of this short scientific biography are the publications of de Broglie (a complete bibliography is given at the end), some more or less short memories of his students and some unpublished documents found in the ""Archives de l'Academie des Sciences"" at Paris. The original text of the correspondence between de Broglie and Einstein is enclosed in German translation in the appendix."
"We argue that taxonomical concept development is vital for planetary science as in all branches of science, but its importance has been obscured by unique historical developments. The literature shows that the concept of planet developed by scientists during the Copernican Revolution was theory-laden and pragmatic for science. It included both primaries and satellites as planets due to their common intrinsic, geological characteristics. About two centuries later the non-scientific public had just adopted heliocentrism and was motivated to preserve elements of geocentrism including teleology and the assumptions of astrology. This motivated development of a folk concept of planet that contradicted the scientific view. The folk taxonomy was based on what an object orbits, making satellites out to be non-planets and ignoring most asteroids. Astronomers continued to keep primaries and moons classed together as planets and continued teaching that taxonomy until the 1920s. The astronomical community lost interest in planets ca. 1910 to 1955 and during that period complacently accepted the folk concept. Enough time has now elapsed so that modern astronomers forgot this history and rewrote it to claim that the folk taxonomy is the one that was created by the Copernican scientists. Starting ca. 1960 when spacecraft missions were developed to send back detailed new data, there was an explosion of publishing about planets including the satellites, leading to revival of the Copernican planet concept. We present evidence that taxonomical alignment with geological complexity is the most useful scientific taxonomy for planets. It is this complexity of both primary and secondary planets that is a key part of the chain of origins for life in the cosmos."
"The available enthalpy is an early form of the modern thermodynamic concept of exergy, which is the generic name for the amount of work obtainable when some matter is brought to a state of equilibrium with its surroundings by means of reversible processes.   It is shown in this paper that a study of the hydrodynamic properties of available enthalpy leads to a generalization of the global meteorological available energies previously introduced by Lorenz, Dutton and Pearce. A local energy cycle is derived without approximation. Moreover, static instabilities or topography do not prevent this theory from having practical applications. The concept of available enthalpy is also presented in terms of the potential change in total entropy. Using the hydrostatic assumption, limited-area energetics is then rigorously defined, including new boundary fluxes and new energy components. This innovative approach is especially suitable for the study of energy conversions between isobaric layers of an open limited atmospheric domain.   Numerical evaluations of various energy components are presented for a hemispheric field of zonal-average temperature. It is further shown that this new energetic scheme realizes a hierarchical partition of the components so that the smallest of those available enthalpy reservoirs are almost of the same magnitude as the kinetic energy. This is actually the fundamental property that induced Margules to define the primary concept of available kinetic energy in meteorology."
"We briefly review some of the scientific challenges and epistemological issues related to climate science. We discuss the formulation and testing of theories and numerical models, which, given the presence of unavoidable uncertainties in observational data, the non-repeatability of world-experiments, and the fact that relevant processes occur in a large variety of spatial and temporal scales, require a rather different approach than in other scientific contexts. A brief discussion of the intrinsic limitations of geo-engineering solutions to global warming is presented, and a framework of investigation based upon non-equilibrium thermodynamics is proposed. We also critically discuss recently proposed perspectives of development of climate science based purely upon massive use of supercomputer and centralized planning of scientific priorities."
"It is a widely accepted view that COVID 19 is either transmitted via surface contamination or via close contact of an un-infected person with an infected person. Surface contamination usually happens when infected water droplets from exhalation/sneeze/cough of COVID sick person settle on nearby surfaces. To curb this, social distancing and good hand hygiene advise is advocated by World health Organization (WHO). We argue that COVID 19 coronovirus can also be airborne in a puff cloud loaded with infected droplets generated by COVID sick person. An elementary calculation shows that a $5~\mu m$ respiratory infected droplet can remain suspended for about 9.0 minutes and a $2~\mu m$ droplet can remain suspended for about an hour! And social distancing advise of 3 feet by WHO and 6 feet by CDC (Centers for Disease Control and Prevention) may not be sufficient in some circumstances as discussed in the text."
"In a Forum published in EOS Transactions AGU (2009) entitled ""Lies, damned lies and statistics (in Geology)"", Vermeesch (2009) claims that ""statistical significant is not the same as geological significant"", in other words, statistical tests may be misleading. In complete contradiction, we affirm that statistical tests are always informative. We detail the several mistakes of Vermeesch in his initial paper and in his comments to our reply. The present text is developed in the hope that it can serve as an illuminating pedagogical exercise for students and lecturers to learn more about the subtleties, richness and power of the science of statistics."
"The main reason for the generation of tsunamis is the deformation of the bottom of the ocean caused by an underwater earthquake. Usually, only the vertical bottom motion is taken into account while the horizontal co-seismic displacements are neglected in the absence of landslides. In the present study we propose a methodology based on the well-known Okada solution to reconstruct in more details all components of the bottom coseismic displacements. Then, the sea-bed motion is coupled with a three-dimensional weakly nonlinear water wave solver which allows us to simulate a tsunami wave generation. We pay special attention to the evolution of kinetic and potential energies of the resulting wave while the contribution of the horizontal displacements into wave energy balance is also quantified. Such contribution of horizontal displacements to the tsunami generation has not been discussed before, and it is different from the existing approaches. The methods proposed in this study are illustrated on the July 17, 2006 Java tsunami and some more recent events."
"In the present article we consider the problem of wave interaction with a partially immersed, but floating body. We assume that the motion of the body is prescribed. The general mathematical formulation for this problem is presented in the framework of a hierarchy of mathematical models. Namely, in this first part we formulate the problem at every hierarchical level. The special attention is payed to fully nonlinear and weakly dispersive models since they are most likely to be used in practice. For this model we have to consider separately the inner (under the body) and outer domains. Various approached to the gluing of solutions at the boundary is discussed as well. We propose several strategies which ensure the global conservation or continuity of some important physical quantities."
"A new diagnosis method for high energy ions utilizing a single CR-39 detector mounted on plastic plates is demonstrated to identify the presence of the high energy component beyond the CR-39's detection threshold limit. On irradiation of the CR-39 detector unit with a 25 MeV per nucleon He ion beam from conventional rf-accelerators, a large number of etch pits having elliptical opening shapes are observed on the rear surface of the CR-39. Detailed investigations reveal that these etch pits are created by heavy ions inelastically backscattered from the plastic plates. This ion detection method is applied to laser-driven ion acceleration experiments using cluster-gas targets, and ion signals with energies up to 50 MeV per nucleon are identified."
"We present a coupled fluid-dynamic and electromagnetic model for volcanic ash plumes. In a forward approach, the model is able to simulate the plume dynamics from prescribed input flow conditions and generate the corresponding synthetic thermal infrared (TIR) image, allowing a comparison with field-based observations. An inversion procedure is then developed to retrieve ash plume properties from TIR images.   The adopted fluid-dynamic model is based on a one-dimensional, stationary description of a self-similar (top-hat) turbulent plume, for which an asymptotic analytical solution is obtained. The electromagnetic emission/absorption model is based on the Schwarzschild's equation and on Mie's theory for disperse particles, assuming that particles are coarser than the radiation wavelength and neglecting scattering. [...]   Application of the inversion procedure to an ash plume at Santiaguito volcano (Guatemala) has allowed us to retrieve the main plume input parameters, namely the initial radius $b_0$, velocity $U_0$, temperature $T_0$, gas mass ratio $n_0$, entrainment coefficient $k$ and their related uncertainty. Moreover, coupling with the electromagnetic model, we have been able to obtain a reliable estimate of the equivalent Sauter diameter $d_s$ of the total particle size distribution.   The presented method is general and, in principle, can be applied to the spatial distribution of particle concentration and temperature obtained by any fluid-dynamic model, either integral or multidimensional, stationary or time-dependent, single or multiphase. The method discussed here is fast and robust, thus indicating potential for applications to real-time estimation of ash mass flux and particle size distribution, which is crucial for model-based forecasts of the volcanic ash dispersal process."
"The absolute L subshell specific electron impact ionization cross sections near the ionization threshold (16 < E < 45 keV) of Lead and Thorium are obtained from the measured L X-ray production cross sections. Monte Carlo simulation is done to account for the effect of the backscattered electrons and the final experimental results are compared with calculations performed using distorted wave Born approximation and the modified relativistic binary encounter Bethe model.The sensitivity of the results on the atomic parameters is explored. Observed agreements and discrepancies between the experimental results and theoretical estimates, and their dependence on the specific atomic parameters are reported."
"The origin of ice slipperiness has been a matter of great controversy for more than a century, but an atomistic understanding of ice friction is still lacking. Here, we perform computer simulations of an atomically smooth substrate sliding on ice. Our results show that a very small extent of interfacial premelting is sufficient to provide a lubricating quasi-liquid layer with rheological properties similar to bulk undercooled water. Upon shearing, one single water-like monolayer sandwiched between adsorption layers is able to display a pattern consistent with lubricating Couette flow. For hydrophobic walls, the flow exhibits large slip, while hydrophilic walls obey stick boundary conditions with small negative slip. By compressing ice, the lubricating layer grows continuously, and the rheological properties approach bulk--like behavior. In either case, the energy dissipated by sliding under skating conditions is sufficient to melt an ice bilayer in the scale of decades of nanoseconds. Our results show the atomic scale frictional behavior is a combination of spontaneous ice premelting, pressure melting and frictional heating."
"Attosecond nonlinear Fourier transform (NFT) pump probe spectroscopy is an experimental technique which allows investigation of the electronic excitation, ionization, and unimolecular dissociation processes. The NFT spectroscopy utilizes ultrafast multiphoton ionization in the extreme ultraviolet spectral range and detects the dissociation products of the unstable ionized species. In this paper, a quantum mechanical description of NFT spectra is suggested, which is based on the second order perturbation theory in molecule-light interaction and the high level ab initio calculations of CO2 and CO2+ in the Franck-Condon zone. The calculations capture the characteristic features of the available experimental NFT spectra of CO2. Approximate analytic expressions are derived and used to assign the calculated spectra in terms of participating electronic states and harmonic photon frequencies. The developed approach provides a convenient framework within which the origin and the significance of near harmonic and non-harmonic NFT spectral lines can be analyzed. The framework is scalable and the spectra of di- and triatomic species as well as the dependences on the control parameters can by predicted semi-quantitatively."
"This paper offers a new technique of incoherent scattering signal processing. The technique is based on the experimentally observed comb structure of the spectral power of separate realizations. The technique implies determining the positions and amplitudes of peaks in separate realizations, the formation - on their basis - of the spectral power of an individual realization not distorted by the smoothing function, and a subsequent summation of such spectra for the realizations. The technique has been tested using data from the Irkutsk incoherent scatter radar, both for the case of the incoherent scattering from thermal irregularities of plasma and for the case of the aspect scattering from instabilities elongated with the geomagnetic field."
"In the paper the step-by-step principles for making local model of electron density are described. They are based on modulation principle - electron density dependence on time is a product of a number of temporal variations caused by solar radiation, magnetic activity, Earth orientation and unknown additional periodical processes (not a sum, as they suppose sometimes when making such models). A multiranges modulation principle is also suggested, that allows automatically extend the set of parameters by using new ones, obtained by filtration (or averaging) of basic set of parameters over the time. In the paper we describe two approaches to the model creation - descriptional and predictional ones.   To test the approach three different models were created for daily electron density logarithm using the described principles. We have used the data of Irkutsk digisonde over the period 2003-2007 years for testing. It becomes clear that a non-optimal choice of the number of model parameters could increase prediction error, inspite the error over the set, used for analysis, will decrease. It is shown that one year prediction has accuracy about 9-23% depending on the height, and the highest error corresponds to the height about 200km. From the modelling we could also see that with increasing of the height the number of parameters increases, and this could be caused by inaccuracy of the model or by not taking additional physical mechanisms into consideration."
"The collective dynamics of annulus dusty plasma formed between a co-centric conducting (non-conducting) disk and ring configuration is studied in a strongly magnetized radio-frequency (rf) discharge. A superconducting electromagnet is used to introduce a homogeneous magnetic field to the dusty plasma medium. In absence of the magnetic field, dust grains exhibit thermal motion around their equilibrium position. The dust grains start to rotate in anticlockwise direction with increasing magnetic field (B $>$ 0.02 T), and the constant value of the angular frequency at various strengths of magnetic field confirms the rigid body rotation. The angular frequency of dust grains linearly increases up to a threshold magnetic field (B $>$ 0.6 T) and after that its value remains nearly constant in a certain range of magnetic field. Further increase in magnetic field (B $>$ 1 T) lowers the angular frequency. Low value of angular frequency is expected by reducing the width of annulus dusty plasma or the input rf power. The azimuthal ion drag force due to the magnetic field is assumed to be the energy source which drives the rotational motion. The resultant radial electric field in the presence of magnetic field determines the direction of rotation. The variation of floating (plasma) potential across the annular region at given magnetic field explains the rotational properties of the annulus dusty plasma in the presence of magnetic field."
"A global effort to redefine our International System of Units (SI) is underway and the change to the new system is expected to occur in 2018. Within the newly redefined SI, the present base units will still exist but be derived from fixed numerical values of seven reference constants. More specifically, the unit of mass, the kilogram, will be realized through a fixed value of the Planck constant $h$. For instance, a watt balance can be used to realize the kilogram unit of mass within a few parts in $10^8$. Such a balance has been designed and constructed at the National Institute of Standards and Technology. For educational outreach and to demonstrate the principle, we have constructed a LEGO tabletop watt balance capable of measuring a gram size mass to 1 % relative uncertainty. This article presents the design, construction, and performance of the LEGO watt balance and its ability to determine $h$"
"The enormous pressure lifting the column of oil in a leaking oil well can thwart efforts to seal the top of the well and prevent oil from rising. When the oil cannot be stopped completely, we propose to slow its flow by filling the well with a porous medium. That medium consists of countless small, dense, streamlined objects that are dropped into the well and descend through the rising oil at terminal velocity. The resulting heap of objects couples to the oil via viscous and drag forces, dissipating the oil's energy and upward momentum and significantly reducing its rate of flow."
"An electrostatic cryogenic storage ring, CSR, for beams of anions and cations with up to 300 keV kinetic energy per unit charge has been designed, constructed and put into operation. With a circumference of 35 m, the ion-beam vacuum chambers and all beam optics are in a cryostat and cooled by a closed-cycle liquid helium system. At temperatures as low as (5.5 $\pm$ 1) K inside the ring, storage time constants of several minutes up to almost an hour were observed for atomic and molecular, anion and cation beams at an energy of 60 keV. The ion-beam intensity, energy-dependent closed-orbit shifts (dispersion) and the focusing properties of the machine were studied by a system of capacitive pickups. The Schottky-noise spectrum of the stored ions revealed a broadening of the momentum distribution on a time scale of 1000 s. Photodetachment of stored anions was used in the beam lifetime measurements. The detachment rate by anion collisions with residual-gas molecules was found to be extremely low. A residual-gas density below 140 cm$^{-3}$ is derived, equivalent to a room-temperature pressure below 10$^{-14}$ mbar. Fast atomic, molecular and cluster ion beams stored for long periods of time in a cryogenic environment will allow experiments on collision- and radiation-induced fragmentation processes of ions in known internal quantum states with merged and crossed photon and particle beams."
"Space exemplifies the ultimate test-bed environment for any materials technology. The harsh conditions of space, with extreme temperature changes, lack of gravity and atmosphere, intense solar and cosmic radiation, and mechanical stresses of launch and deployment, represent a multifaceted set of challenges. The materials we engineer must not only meet these challenges, but they need to do so while keeping overall mass to a minimum and guaranteeing performance over long periods of time with no opportunity for repair. Nanophotonic materials -- materials that embody structural variations on a scale comparable to the wavelength of light -- offer opportunities for addressing some of these difficulties. Here, we examine how advances in nanophotonics and nanofabrication are enabling ultrathin and lightweight structures with unparalleled ability to shape light-matter interactions over a broad electromagnetic spectrum. From solar panels that can be fabricated in space to applications of light for propulsion, the next generation of lightweight and multifunctional photonic materials stands to both impact existing technologies and pave the way for new space technologies."
"The results obtained by the plasma physics community for the validation and the prediction of turbulence and transport in magnetized plasma come mainly from the use of very CPU-consuming particle-in-cell or (gyro)kinetic codes which naturally include non-Maxwellian kinetic effects. To date, fluid codes are not considered to be relevant for the description of these kinetic effects. Here, after revisiting the limitations of the current fluid theory developed in the 19th century, we generalize the fluid theory including kinetic effects such as non-Maxwellian super-thermal tails with as few fluid equations as possible. The collisionless and collisional fluid closures from the nonlinear Landau Fokker-Planck collision operator are shown for an arbitrary collisionality. Indeed, the first fluid models associated with two examples of collisionless fluid closures are obtained by assuming an analytic non-Maxwellian distribution function (e.g., the INMDF [O. Izacard, Phys. Plasmas 23, 082504 (2016)]). One of the main differences with the literature is our analytic representation of the distribution function in the velocity phase space with as few hidden variables as possible thanks to the use of non-orthogonal basis sets. These new non-Maxwellian fluid equations could initiate the next generation of fluid codes including kinetic effects and can be expanded to other scientific disciplines such as astrophysics, condensed matter, or hydrodynamics. As a validation test, we perform a numerical simulation based on a minimal reduced INMDF fluid model. The result of this test is the discovery of the origin of particle and heat diffusion. The diffusion is due to the competition between a growing INMDF on short time scales due to spatial gradients and the thermalization on longer time scales. The results shown here could draw the breaking of some unsolved understandings of the turbulence."
"The note presents a formula for the prediction of the rotation periods of the planets and asteroids. This formula, which is like the Titius-Bode law, gives a good agreement with the rotation periods of most planets, shows that Venus is retrograde, and that there must be five objects between Mars and Jupiter. This formula may be of some relevance in understanding the dynamics of the early solar system."
"In 1927, just after he obtained a full professorship, Enrico Fermi delivered his first course on Theoretical Physics at the Institute of Physics in Rome. The following year Edoardo Amaldi, Emilio Segre' and Ettore Majorana became students of that course. The lectures given in 1927-28, whose content may be easily reconstructed, probably had a deep impact in forming such young students, who participated actively in the Physics researches carried on in Rome soon after. In this perspective, the case of Ettore Majorana is certainly the most important one: as a lecturer, in 1933-36 he planned to give three advanced Physics courses in Rome, before obtaining his full professorship in Theoretical Physics ""for high and well-deserved repute"" in 1937. From the analysis of the lectures he delivered in Naples in 1938, we can conclude that, in part, Majorana referred to the Fermi lectures he followed as a student, but he also introduced some advanced (for that time) topics, rendering his course a very modern one. Much of these frontier topics were already cited in the programmes of the mentioned three courses he presented few years earlier."
"The observation that the shadows of objects change during the course of the day and also for a fixed time during a year led curious minds to realize that the Sun could be used as a timekeeper. However, the daily motion of the Sun has some subtleties, for example, with regards to the precise time at which it crosses the meridian near noon. When the Sun is on the meridian, a clock is used to ascertain this time and a vertical stick determines the angle the Sun is above the horizon. These two measurements lead to the construction of a diagram (called an analemma) as an extremely useful resource for the teaching of astronomy. In this paper we report on the construction of this diagram from roughly weekly observations during more than a year."
"A century after observing the deflection of light emitted by distant stars during the solar eclipse of 1919, it is interesting to know the concepts emerged from the experiment and the theoretical and observational consequences for modern cosmology and astrophysics. In addition to confirming Einstein's gravitational theory, its greatest legacy was the construction of a new research area to cosmos science dubbed gravitational lensing. The formation and magnification of multiple images (mirages) by the gravitational field of a compact or extended lens are among the most striking phenomena of nature. This article presents a pedagogical view of the first genuine gravitational lens effect, the double quasar QSO 0957 + 561. We also describe the formation of rings, giant arcs, arclets and multiple Supernova images. It is also surprising that the Hubble constant and the amount of dark matter in the Universe can be measured by the same technique. Finally, the lensing of gravitational waves, a possible but still not yet detected effect, is also briefly discussed."
"I describe a simple method to calculate Earth dimensions using only local measurements and observations. I used modern technology (a digital photo camera and Google Earth) but the exact same method can be used without any aid, with naked eye observations and distances measured by walking, and so it was perfectly accessible to Ancient Greek science."
"According to an ancient Indian text (c. 1700 BC?), the heavens are 1000 earth diameters away from the earth. Other texts took the sun to be halfway to the heavens, so this suggests a distance of the sun about 500 earth diameters from the earth. The confirmation for this supposition comes from the later theories (c. 500 AD) from the same region and from Greek ideas that speak of roughly the same distance. We suggest that this original conception was transformed by the later Indian and Greek theories in different ways to deal with contradictory data related to outer planet periods."
"Two problems from the Victorian Age, the subdivision of light and the determination of the leakage point in an undersea telegraphic cable are discussed and suggested as a concrete illustrations of the relationships between textbook physics and the real world. Ohm's law and simple algebra are the only tools we need to discuss them in the classroom."
"This note is an attempt to explain in simple words why the famous relation $E=mc^2$ misrepresents the essence of Einstein's relativity theory. The note is addressed to high-school teachers, and a part of it - to those university professors who permit themselves to say that the mass of a body increases with its velocity or momentum and thus mislead the teachers and their students.   -----   Contains both English and Russian versions."
"We study how the paradigm of Newton's science, based on the organization of scientific knowledge as a series of mathematical laws, was definitively accepted in science courses - in the last decades of the XVIII century, in England as well as in the Continent - by means of the ""universal"" dynamical machine invented by George Atwood in late 1770s just for this purpose. The spreading of such machine, occurred well before the appearance of Atwood's treatise where he described the novel machine and the experiments to be performed with it, is a quite interesting historical case, which we consider in some detail. In particular, we focus on the ""improvement"" introduced by the Italian Giuseppe Saverio Poli and the subsequent ""simplifications"" of the machine, underlying the ongoing change of perspective after the definitive success of Newtonianism. The case studied here allows to recognize the relevant role played by a properly devised instrument in the acceptance of a new paradigm by non-erudite scholars, in addition to the traditional ways involving erudite scientists, and thus the complementary role of machine philosophy with respect to mathematical, philosophical or even physical reasoning."
"Classes started in the newly established Physics Department of Calcutta University Science College in 1916. Raman, Bose and Saha were three young members of the small physics faculty consisting of barely half a dozen faculty members. Within about one decade, three extraordinary discoveries came from these young men---Saha ionization equation in 1920, Bose statistics in 1924, Raman effect in 1928. However, fortunes of Calcutta University quickly got intertwined with India's freedom struggle led by Mahatma Gandhi exactly at the same time and the physics group got tragically disrupted. Indian physics never succeeded in reaching that height again. This paper discusses the difficulties in reconstructing a critical history of this Calcutta school of physics during the very short epoch of unmatched brilliance."
"Quantum physics, which describes the strange behavior of light and matter at the smallest scales, is one of the most successful descriptions of reality, yet it is notoriously inaccessible. Here we provide an approachable explanation of quantum physics using simple thought experiments. We derive all relevant quantum predictions using minimal mathematics, without introducing the advanced calculations that are typically used to describe quantum physics. We focus on the two key surprises of quantum physics, namely wave-particle duality, a term that was introduced to capture the fact that single quantum particles in some respects behave like waves and in other respects like particles, and entanglement, which applies to two or more quantum particles and brings out the inherent contradiction between quantum physics and seemingly obvious assumptions regarding the nature of reality. Following arguments originally made by John Bell and Lucien Hardy, we show that the so-called local hidden variables are inadequate at explaining the behavior of entangled quantum particles. This means that one either has to give up on hidden variables, i.e., the idea that the outcomes of measurements on quantum particles are determined before an experiment is actually carried out, or one has to relinquish the principle of locality, which requires that no causal influences should be faster than the speed of light and is a cornerstone of Einstein's theory of relativity. Finally, we describe how these remarkable predictions of quantum physics have been confirmed in experiments. We have successfully used the present approach in a course that is open to all undergraduate students at the University of Calgary, without any prerequisites in mathematics or physics."
"We describe in detail an advanced project devised for outstanding High School (or undergraduate) students with appropriate abilities in physical reasoning (rather than with a good standard preparation), centered around the well-known historical case of Newton's theory of light and colours. The different action lines along which the project is developed are aimed to let the students involved to: 1) think as Newton did, by building step by step all his knowledge and reasoning; 2) work as Newton did, by performing the whole series of his original experiments with prisms; 3) deduce as Newton did about the nature of light and colours; 4) present the results of their activity (including physics demonstrations) to the general public, in order to test abilities in communicating what learned and discovered (including video realization published on YouTube youtube). Such didactic aim is complemented by the purpose to realize a historically informed activity, given the potential key role of the History of Physics in promoting science at a deeper level, especially when no particular training in mathematics or advanced education is required. The highly favourable reception of the project by the students involved, as well as that deserved by the uneducated people to the activities demonstrated by the students in public events, testify for the success of such work."
"Since the pioneering works of Newton $(1643-1727)$, Mechanics has been constantly reinventing itself: reformulated in particular by Lagrange $(1736-1813)$ then Hamilton $(1805-1865)$, it now offers powerful conceptual and mathematical tools for the exploration of the most complex dynamical systems, essentially via the action-angle variables formulation and more generally through the theory of canonical transformations. We give the reader an overview of these different formulations through the well-known example of Foucault's pendulum, a device created by Foucault $(1819-1868)$ and first installed in the Panth\'eon (Paris, France) in 1851 to display the Earth's rotation. The apparent simplicity of the Foucault pendulum is indeed an open door to the most contemporary ramifications of Classical Mechanics. We stress that the action-angle variable formalism is not necessary to understand Foucault's pendulum. The latter is simply taken as well-known simple dynamical system used to exemplify modern concepts that are crucial in order to understand more complicated dynamical systems. The Foucault pendulum installed in the collegiate church of Sainte-Waudru (Mons, Belgium) will allow us to numerically estimate the different quantities introduced. A free adaptation of excerpts from ""Alice's Adventures in Wonderland"" will offer the reader some poetic breaths."
"In Ojibwe the Morning Star is called Ikew Anung, which means the Womens Star. In Lakota the same planet Venus is called Anpetu Luta, the Red Day Star. Bother cultures have rich and interesting understandings of Venus that relate to other Indigenous cultures throughout the world. Venus is so often related to the feminine because native peoples carefully watched the movement of the star and saw it in the east at sunrise for nine months and then in the west at sunset for the following nine months. Nine months is exactly the time for human gestation. Yet, tragically, the native star knowledge is disappearing as elders pass. The Native Skywatchers project focuses on understanding the Ojibwe and Dakota importance of this and other celestial connections. Working closely with a team of culture teachers and language experts we are building community around the native star knowledge."
"For millenia, sailors have used the empirical rule that the elevation angle of Polaris, the North Star, as measured by sextant, quadrant or astrolabe, is approximately equal to latitude. Here, we show using elementary trigonometry that Empirical Law 1 can be converted from a heuristic to a theorem. A second ancient empirical law is that the distance in kilometers from the observer to the North Pole, the geodesic distance measured along the spherical surface of the planet, is the number of degrees of colatitude multiplied by 111.1 kilometers. Can Empirical Law 2 be similarly rendered rigorous? No; whereas as the shape of the planet is controlled by trigonometry, the size of our world is an accident of cosmological history. However, Empirical Law 2, can be rigorously verified by measurements. The association of 111 km of north-south distance to one degree of latitude trivially yields the circumference of the globe as 40,000 km. We also extend these ideas and the parallel ray approximation to three different ways of modeling a Flat Earth. We show that photographs from orbit, taken by a very expensive satellite, are unnecessary to render the Flat Earth untenable; simple mathematics proves Earth a sphere just as well."
"We present an analysis of seasonal cycle of the last 50 years of records of surface temperature in Italy. We consider two data sets which synthesize the surface temperature fields of Northern and Southern Italy. Such data sets consist of records of daily maximum and minimum temperature. We compute the best estimate of the seasonal cycle of the variables considered by adopting the cyclograms' technique. We observe that in general the minimum temperature cycle lags behind the maximum temperature cycle, and that the cycles of the Southern Italy temperatures records lag behind the corresponding cycles referring to Northern Italy. All seasonal cycles lag considerably behind the solar cycle. The amplitude and phase of the seasonal cycles do not show any statistically significant trend in the time interval considered."
"We present an intercomparison and verification analysis of several regional climate models (RCMs) nested into the same run of the same Atmospheric Global Circulation Model (AGCM) regarding their representation of the statistical properties of the hydrological balance of the Danube river basin for 1961-1990. We also consider the datasets produced by the driving AGCM, from the ECMWF and NCEP-NCAR reanalyses. The hydrological balance is computed by integrating the precipitation and evaporation fields over the area of interest. Large discrepancies exist among RCMs for the monthly climatology as well as for the mean and variability of the annual balances, and only few datasets are consistent with the observed discharge values of the Danube at its Delta, even if the driving AGCM provides itself an excellent estimate. Since the considered approach relies on the mass conservation principle and bypasses the details of the air-land interface modeling, we propose that the atmospheric components of RCMs still face difficulties in representing the water balance even on a relatively large scale. Their reliability on smaller river basins may be even more problematic. Moreover, since for some models the hydrological balance estimates obtained with the runoff fields do not agree with those obtained via precipitation and evaporation, some deficiencies of the land models are also apparent. NCEP-NCAR and ERA-40 reanalyses result to be largely inadequate for representing the hydrology of the Danube river basin, both for the reconstruction of the long-term averages and of the seasonal cycle, and cannot in any sense be used as verification. We suggest that these results should be carefully considered in the perspective of auditing climate models and assessing their ability to simulate future climate changes."
"The intrinsic difficulties in building realistic climate models and in providing complete, reliable and meaningful observational datasets, and the conceptual impossibility of testing theories against data imply that the usual Galilean scientific validation criteria do not apply to climate science. The different epistemology pertaining to climate science implies that its answers cannot be singular and deterministic; they must be plural and stated in probabilistic terms. Therefore, in order to extract meaningful estimates of future climate change from a model, it is necessary to explore the model' uncertainties. In terms of societal impacts of scientific knowledge, it is necessary to accept that any political choice in a matter involving complex systems is made under unavoidable conditions of uncertainty. Nevertheless, detailed probabilistic results in science can provide a baseline for a sensible process of decision making."
"Environmental science almost invariably proposes problems of extreme complexity, typically characterized by strongly nonlinear evolution dynamics. The systems under investigation have many degrees of freedom - which makes them complicated - and feature nonlinear interactions of several different components taking place on a vast range of time-space scales - which makes them complex. Such systems evolve under the action of macroscopic driving (typically the solar heating) and modulating (e.g. the Earth's rotation and gravitation) agents. The most comprehensive example is the entire climatic system. The description of the macroscopic dynamics of environmental systems is based on the systematic use of dominant balances derived on a phenomenological basis in order to specialize the dynamical equations. Such balances are suitable classes of approximate solutions of the evolution equations which represent a reasonably good approximation to the actual observed fields when sufficiently large spatial or temporal averages are considered. Actually, different balances have to be considered depending on the time and space scales we are focusing our interest on. Such an approach reflects the fundamentally heuristic-inductive nature of the scientific research in environmental sciences, where the traditional reductionistic scientific attitude is not always effective. In order to exemplify this procedure, we consider the very relevant case of the motion of the fluids that permit the existence of life on the Earth, air and water: the so-called geophysical fluids."
"This work presents a novel diagnostic tool for studying the thermodynamics of the climate systems with a wide range of applications, from sensitivity studies to model tuning. It includes a number of modules for assessing the internal energy budget, the hydrological cycle, the Lorenz Energy Cycle and the material entropy production, respectively. The routine receives as inputs energy fluxes at surface and at the Top-of-Atmosphere (TOA), for the computation of energy budgets at Top-of-Atmosphere (TOA), at the surface, and in the atmosphere as a residual. Meridional enthalpy transports are also computed from the divergence of the zonal mean energy budget fluxes; location and intensity of peaks in the two hemispheres are then provided as outputs. Rainfall, snowfall and latent heat fluxes are received as inputs for computing the water mass and latent energy budgets. If a land-sea mask is provided, the required quantities are separately computed over continents and oceans. The diagnostic tool also computes the Lorenz Energy Cycle (LEC) and its storage/conversion terms as annual mean global and hemispheric values. In order to achieve this, one needs to provide as input three-dimensional daily fields of horizontal wind velocity and temperature in the troposphere. Two methods have been implemented for the computation of the material entropy production, one relying on the convergence of radiative heat fluxes in the atmosphere (indirect method), one combining the irreversible processes occurring in the climate system, particularly heat fluxes in the boundary layer, the hydrological cycle and the kinetic energy dissipation as retrieved from the residuals of the LEC. A version of the diagnostic tool is included in the Earth System Model eValuation Tool (ESMValTool) community diagnostics, in order to assess the performances of soon available CMIP6 model simulations."
"An algorithm to estimate motion from satellite imagery is presented. Dense displacement fields are computed from time-separated images of of significant convective activity using a Bayesian formulation of the motion estimation problem. Ordinarily this motion estimation problem is ill-posed; there are far too many degrees of freedom than necessary to represent the motion. Therefore, some form of regularization becomes necessary and by imposing smoothness and non-divergence as desirable properties of the estimated displacement vector field, excellent solutions are obtained. Our approach provides a marked improvement over other methods in conventional use. In contrast to correlation based approaches, the displacement fields produced by our method are dense, spatial consistency of the displacement vector field is implicit, and higher-order and small-scale deformations can be easily handled. In contrast with optic-flow algorithms, we can produce solutions at large separations of mesoscale features between large time-steps or where the deformation is rapidly evolving."
"Distributed models to forecast the spatial and temporal occurrence of rainfall-induced shallow landslides are based on deterministic laws. These models extend spatially the static stability models adopted in geotechnical engineering, and adopt an infinite-slope geometry to balance the resisting and the driving forces acting on the sliding mass. An infiltration model is used to determine how rainfall changes pore-water conditions, modulating the local stability/instability conditions. A problem with the operation of the existing models lays in the difficulty in obtaining accurate values for the several variables that describe the material properties of the slopes. The problem is particularly severe when the models are applied over large areas, for which sufficient information on the geotechnical and hydrological conditions of the slopes is not generally available. To help solve the problem, we propose a probabilistic Monte Carlo approach to the distributed modeling of rainfall-induced shallow landslides. For the purpose, we have modified the Transient Rainfall Infiltration and Grid-Based Regional Slope-Stability Analysis (TRIGRS) code. The new code (TRIGRS-P) adopts a probabilistic approach to compute, on a cell-by-cell basis, transient pore-pressure changes and related changes in the factor of safety due to rainfall infiltration. Infiltration is modeled using analytical solutions of partial differential equations describing one-dimensional vertical flow in isotropic, homogeneous materials. Both saturated and unsaturated soil conditions can be considered. TRIGRS-P copes with the natural variability inherent to the mechanical and hydrological properties of the slope materials by allowing values of the TRIGRS model input parameters to be sampled randomly from a given probability distribution. [..]"
"The design of electromagnetic coils may require evaluation of several quantities that are challenging to compute numerically. These quantities include Lorentz forces, which may be a limiting factor due to stresses; the internal magnetic field, which is relevant for determining stress as well as a superconducting coil's proximity to its quench limit; and the inductance, which determines stored magnetic energy and dynamics. When computing the effect on one coil due to the current in another, these quantities can often be approximated quickly by treating the coils as infinitesimally thin. When computing the effect on a coil due to its own current (e.g., self-force or self-inductance), evaluation is difficult due to the presence of a singularity; coils cannot be treated as infinitesimally thin as each quantity diverges at zero conductor width. Here, we present novel and well-behaved methods for evaluating these quantities using non-singular integral formulae of reduced dimensions. These formulae are determined rigorously by dividing the domain of integration of the magnetic vector potential into two regions, exploiting appropriate approximations in each region, and expanding in high aspect ratio. Our formulae show good agreement to full finite-thickness calculations even at low aspect ratio, both analytically for a torus and numerically for a non-planar coil of a stellarator fusion device, the Helically Symmetric eXperiment (HSX). Because the integrands of these formulae develop fine structure as the minor radius becomes infinitely thin, we also develop a method of evaluating the self-force and self-inductance with even greater efficiency by integrating this sharp feature analytically. We demonstrate with this method that the self-force can be accurately computed for the HSX coil with as few as 12 grid points."
"For designing high-field electromagnets, the Lorentz force on coils must be computed to ensure a support structure is feasible, and the inductance should be computed to evaluate the stored energy. Also, the magnetic field and its variation inside the conductor is of interest for computing stress and strain, and due to superconducting quench limits. For these force, inductance, energy, and internal field calculations, the coils cannot be naively approximated as infinitesimally thin filaments due to divergences when the source and evaluation points coincide, so more computationally demanding calculations are usually required, resolving the finite cross-section of the conductors. Here, we present a new alternative method that enables the internal magnetic field vector, self-force, and self-inductance to be computed rapidly and accurately within a 1D filament model. The method is applicable to coils for which the curve center-line can have general noncircular shape, as long as the conductor width is small compared to the radius of curvature. This paper extends a previous calculation for circular-cross-section conductors [Hurwitz et al, arXiv:2310.09313 (2023)] to consider the case of rectangular cross-section. The reduced model is derived by rigorous analysis of the singularity, regularizing the filament integrals such that they match the true high-dimensional integrals at high coil aspect ratio. The new filament model exactly recovers analytic results for a circular coil, and is shown to accurately reproduce full finite-cross-section calculations for a non-planar coil of a stellarator magnetic fusion device. Due to the efficiency of the model here, it is well suited for use inside design optimization."
"The asymptotic derivation of a new family of one-dimensional, weakly nonlinear and weakly dispersive equations that model the flow of an ideal fluid in an elastic vessel is presented. Dissipative effects due to the viscous nature of the fluid are also taken into account. The new models validate by asymptotic reasoning other non-dispersive systems of equations that are commonly used, and improve other nonlinear and dispersive mathematical models derived to describe the blood flow in elastic vessels. The new systems are studied analytically in terms of their basic characteristic properties such as the linear dispersion characteristics, symmetries, conservation laws and solitary waves. Unidirectional model equations are also derived and analysed in the case of vessels of constant radius. The capacity of the models to be used in practical problems is being demonstrated by employing a particular system with favourable properties to study the blood flow in a large artery. Two different cases are considered: A vessel with constant radius and a tapered vessel. Significant changes in the flow can be observed in the case of the tapered vessel."
"In this study we compare the representation of the southern hemisphere midlatitude winter variability in the NCEP-NCAR and ERA40 reanalyses. We use the classical Hayashi spectral technique, recently applied to compare the description of the atmospheric variability in the northern hemisphere on different spectral sub-domains. We test the agreement of the two reanalysis systems in the representation of the atmospheric activity. In the southern hemisphere, even in the satellite period, the assimilated data are relatively scarce, predominately over the oceans, and they provide a weaker constraint to the model dynamics. We find relevant discrepancies in the description of the variability at different spatial and temporal scales. ERA40 is generally characterised by a larger variance, especially in the high frequency spectral region. In the pre-satellite period the discrepancies between the two reanalyses are large and randomly distributed while after the 1979 the discrepancies are systematic. Moreover, a sudden jump in the VTPR period (1973-1978) is observed, mostly in the ERA40 reanalysis. Our results suggest that today we do not have a well-defined picture of the properties of the winter mid-latitude variability in the southern hemisphere to be used in the evaluation of the realism of climate models and demand for an intercomparison study for the assessment of the self-consistency of the IPCC models in the representation of the analysed properties."
"A baroclinic model for the atmospheric jet at middle-latitudes is used as stochastic generator of non-stationary time series of the total energy of the system. A linear time trend is imposed on the parameter $T_E$, descriptive of the forced equator-to-pole temperature gradient and responsible for setting the average baroclinicity in the model. The focus lies on establishing a theoretically sound framework for the detection and assessment of trend at extreme values of the generated time series. This problem is dealt with by fitting time-dependent Generalized Extreme Value (GEV) models to sequences of yearly maxima of the total energy. A family of GEV models is used in which the location $\mu$ and scale parameters $\sigma$ depend quadratically and linearly on time, respectively, while the shape parameter $\xi$ is kept constant. From this family, a model is selected by using diagnostic graphical tools, such as probability and quantile plots, and by means of the likelihood ratio test. The inferred location and scale parameters are found to depend in a rather smooth way on time and, therefore, on $T_E$. In particular, power-law dependences of $\mu$ and $\sigma$ on $T_E$ are obtained, in analogy with the results of a previous work where the same baroclinic model was run with fixed values of $T_E$ spanning the same range as in this case. It is emphasized under which conditions the adopted approach is valid."
"This study presents a new formulation for the norms and scalar products used in tangent linear or adjoint models to determine forecast errors and sensitivity to observations and to calculate singular vectors. The new norm is derived from the concept of moist-air available enthalpy, which is one of the availability functions referred to as exergy in general thermodynamics. It is shown that the sum of the kinetic energy and the moist-air available enthalpy can be used to define a new moist-air squared norm which is quadratic in: 1) wind components; 2) temperature; 3) surface pressure; and 4) water vapor content. Preliminary numerical applications are performed to show that the new weighting factors for temperature and water vapor are significantly different from those used in observation impact studies, and are in better agreement with observed analysis increments. These numerical applications confirm that the weighting factors for water vapor and temperature exhibit a large increase with height (by several orders of magnitude) and a minimum in the middle troposphere, respectively."
"Nudging is an important data assimilation technique where partial field measurements are used to control the evolution of a dynamical system and/or to reconstruct the entire phase-space configuration of the supplied flow. Here, we apply it to the toughest problem in fluid dynamics: three dimensional homogeneous and isotropic turbulence. By doing numerical experiments we perform a systematic assessment of how well the technique reconstructs large- and small-scales features of the flow with respect to the quantity and the quality/type of data supplied to it. The types of data used are: (i) field values on a fixed number of spatial locations (Eulerian nudging), (ii) Fourier coefficients of the fields on a fixed range of wavenumbers (Fourier nudging), or (iii) field values along a set of moving probes inside the flow (Lagrangian nudging). We present state-of-the-art quantitative measurements of the scale-by-scale {\it transition to synchronization} and a detailed discussion of the probability distribution function of the reconstruction error, by comparing the nudged field and the {\it truth} point-by-point. Furthermore, we show that for more complex flow configurations, like the case of anisotropic rotating turbulence, the presence of cyclonic and anticyclonic structures leads to unexpectedly better performances of the algorithm. We discuss potential further applications of nudging to a series of applied flow configurations, including the problem of field-reconstruction in thermal Rayleigh-B\'enard convection and in magnetohydrodynamics (MHD), and to the determination of optimal parametrisation for small-scale turbulent modeling. Our study fixes the standard requirements for future applications of nudging to complex turbulent flows."
"The extratropical meridional energy transport in the atmosphere is fundamentally intermittent in nature, having extremes large enough to affect the net seasonal transport. Here, we investigate how these extreme transports are associated with the dynamics of the atmosphere at multiple spatial scales, from planetary to synoptic. We use the ERA5 reanalysis data to perform a wavenumber decomposition of meridional energy transport in the Northern Hemisphere mid-latitudes during winter and summer. We then relate extreme transport events to atmospheric circulation anomalies and dominant weather regimes, identified by clustering 500hPa geopotential height fields. In general, planetary-scale waves determine the strength and meridional position of the synoptic-scale baroclinic activity with their phase and amplitude, but important differences emerge between seasons. During winter, large wavenumbers ($k=2-3$) are key drivers of the meridional energy transport extremes, and planetary and synoptic-scale transport extremes virtually never co-occur. In summer, extremes are associated with higher wavenumbers ($k=4-6$), identified as synoptic-scale motions. We link these waves and the transport extremes to recent results on exceptionally strong and persistent co-occurring summertime heat waves across the Northern Hemisphere mid-latitudes. We show that the weather regime structures associated with these heat wave events are typical for extremely large poleward energy transport events."
"It is pointed out that controlled release of thermal energy from fission type nuclear reactors can be used to alter weather patterns over significantly large geographical regions. (1) Nuclear heat creates a low pressure region, which can be used to draw moist air from oceans, onto deserts. (2) Creation of low pressure zones over oceans using Nuclear heat can lead to Controlled Cyclone Creation (CCC).(3) Nuclear heat can also be used to melt glaciers and control water flow in rivers."
"We compare, for the overlapping time frame 1962-2000, the estimate of the northern hemisphere (NH) mid-latitude winter atmospheric variability within the XX century simulations of 17 global climate models (GCMs) included in the IPCC-4AR with the NCEP and ECMWF reanalyses. We compute the Hayashi spectra of the 500hPa geopotential height fields and introduce an integral measure of the variability observed in the NH on different spectral sub-domains. Only two high-resolution GCMs have a good agreement with reanalyses. Large biases, in most cases larger than 20%, are found between the wave climatologies of most GCMs and the reanalyses, with a relative span of around 50%. The travelling baroclinic waves are usually overestimated, while the planetary waves are usually underestimated, in agreement with previous studies performed on global weather forecasting models. When comparing the results of various versions of similar GCMs, it is clear that in some cases the vertical resolution of the atmosphere and, somewhat unexpectedly, of the adopted ocean model seem to be critical in determining the agreement with the reanalyses. The GCMs ensemble is biased with respect to the reanalyses but is comparable to the best 5 GCMs. This study suggests serious caveats with respect to the ability of most of the presently available GCMs in representing the statistics of the global scale atmospheric dynamics of the present climate and, a fortiori, in the perspective of modelling climate change."
"The Intergovernmental Panel on Climate Change reports indicate that the global mean temperature is about one-degree Celsius higher than pre-industrial levels, that this increase is anthropogenic, and that there is a causal relationship between this higher temperature and an increase in frequency and magnitude of extreme weather events. This causal relationship seems at odds with common sense, and may be difficult to explain to non-experts. Thus to appreciate the significance of a one-degree increase in global mean temperature, we perform back-of-the-envelope calculations relying on simple physics. We estimate the excess thermal energy trapped in the climate system (oceans, land, atmosphere) from a one-degree Celsius increase in global mean temperature, and show that it is thousands of times larger than the estimated energy required to form and maintain a hurricane. Our estimates show that global warming is forming a very large pool of excess energy that could in principle power heatwaves, heavy precipitation, droughts, and hurricanes. The arguments presented here are sufficiently simple to be presented in introductory physics classes, and can serve as plausibility arguments showing that even a seemingly small increase in global mean temperature can potentially lead to extreme weather events."
"Since their discovery in 1896, x-rays have had a profound impact on science, medicine and technology. Here we show that the x-rays from a novel tabletop source of bright coherent synchrotron radiation can be applied to phase contrast imaging of biological specimens, yielding superior image quality and avoiding the need for scarce or expensive conventional sources."
"A benchmark study has been carried out on the ground-state potential curve of the hydroxyl anion, OH^{-}, including detailed calibration of both the 1-particle and n-particle basis sets. The CCSD(T) basis set limit overestimates $\omega_e$ by about 10 cm^{-1}, which is only remedied by inclusion of connected quadruple excitations in the coupled cluster expansion --- or, equivalently, the inclusion of the $2\pi$ orbitals in the active space of a multireference calculation. Upon inclusion of scalar relativistic effects (-3 cm^{-1} on $\omega_e$), a potential curve of spectroscopic quality (sub-cm^{-1} accuracy) is obtained. Our best computed EA(OH), 1.828 eV, agrees to three decimal places with the best available experimental value. Our best computed dissociation energies, D_0(OH^-)=4.7796 eV and D_0(OH)=4.4124 eV, suggest that the experimental D_0(OH)=4.392 eV may possibly be about 0.02 eV too low."
"Biologically relevant exposure to environmental pollutants often shows a non-linear relationship. For their assessment, as a rule short term concentrations have to be determined instead of long term mean values. This is also the case for the perception of odour. Regulatory dispersion models like AUSTAL2000 calculate long term mean concentration values (one-hour), but provide no information on the fluctuation from this mean. The ratio between a short term mean value (relevant for odour perception) and the long term mean value (calculated by the dispersion model), called the peak-to-mean value, is usually used to describe these fluctuations. In general, this ratio can be defined in different ways. M\""uller et al. (2012), in a comment to Schauberger et al. (2012) which includes a statement that AUSTAL2000 uses a constant factor of 4, argue that AUSTAL2000 does not apply a peak-to-mean factor and does not calculate odour exceedance probabilities. Instead it calculates the frequency of so-called odour-hours by applying the relation between the 90-percentile of the instantaneous concentration and the hourly mean (Janicke and Janicke, 2007a), not between some peak value and the mean. According to Janicke and Janicke (2007a), the 90-percentile of the instantaneous concentration can in practice be estimated with sufficient accuracy from the hourly mean by using a factor of 4. Having so far replied to M\""uller et al. (2012) we take additionally the opportunity to elaborate a little more on the peak-to-mean concept, especially pointing out that a constant factor independent of the stability of the atmosphere, the distance from and the geometry of the source, is not appropriate. On the contrary it shows a sophisticated structure which cannot be described by only one single value."
"A priori, cosmic-ray measurements offer a unique capability to determine the vertical profile of atmospheric temperatures directly from ground. However, despite the increased understanding of the impact of the atmosphere on cosmic-ray rates, attempts to explore the technological potential of the latter for atmospheric physics remain very limited. In this paper we examine the intrinsic limits of the process of cosmic-ray data inversion for atmospheric temperature retrieval, by combining a detection station at ground with another one placed at an optimal depth, and making full use of the angular information. With that aim, the temperature-induced variations in c. r. rates have been simulated resorting to the theoretical temperature coefficients $W_T(h, \theta, E_{th})$ and the temperature profiles obtained from the ERA5 atmospheric reanalysis. Muon absorption and Poisson statistics have been included to increase realism. The resulting c.r. sample has been used as input for the inverse problem and the obtained temperatures compared to the input temperature data. Relative to early simulation works, performed without using angular information and relying on underground temperature coefficients from a sub-optimal depth, our analysis shows a strong improvement in temperature predictability for all atmospheric layers up to 50 hPa, nearing a factor 2 error reduction. Furthermore, the temperature predictability on 6 h-intervals stays well within the range 0.8-2.2 K. Most remarkably, we show that it can be achieved with small-area m$^2$-scale muon hodoscopes, amenable nowadays to a large variety of technologies. For mid-latitude locations, the optimal depth of the underground station is around 20 m."
"We present a novel optomechanical inertial sensor for low frequency applications and corresponding acceleration measurements. This sensor has a resonant frequency of 4.7Hz, a mechanical quality factor of 476k, a test mass of 2.6 gram, and a projected noise floor of approximately 5E-11 m s-2. per root-Hz at 1Hz. Such performance, together with its small size, low weight, reduced power consumption, and low susceptibility to environmental variables such as magnetic field or drag conditions makes it an attractive technology for future geodesy missions. In this paper, we present an experimental demonstration of low-frequency ground seismic noise detection by direct comparison with a commercial seismometer, anda data analysis algorithms for the identification, characterization, and correction of several noise sources."
"The Electron Loss and Fields Investigation with a Spatio-Temporal Ambiguity-Resolving option (ELFIN-STAR, or simply: ELFIN) mission comprises two identical 3-Unit (3U) CubeSats on a polar (~93deg inclination), nearly circular, low-Earth (~450 km altitude) orbit. Launched on September 15, 2018, ELFIN is expected to have a >2.5 year lifetime. Its primary science objective is to resolve the mechanism of storm-time relativistic electron precipitation, for which electromagnetic ion cyclotron (EMIC) waves are a prime candidate. From its ionospheric vantage point, ELFIN uses its unique pitch-angle-resolving capability to determine whether measured relativistic electron pitch-angle and energy spectra within the loss cone bear the characteristic signatures of scattering by EMIC waves or whether such scattering may be due to other processes. Pairing identical ELFIN satellites with slowly-variable along-track separation allows disambiguation of spatial and temporal evolution of the precipitation over minutes-to-tens-of-minutes timescales, faster than the orbit period of a single low-altitude satellite (~90min). Each satellite carries an energetic particle detector for electrons (EPDE) that measures 50keV to 5MeV electrons with deltaE/E<40% and a fluxgate magnetometer (FGM) on a ~72cm boom that measures magnetic field waves (e.g., EMIC waves) in the range from DC to 5Hz Nyquist (nominally) with <0.3nT/sqrt(Hz) noise at 1Hz. The spinning satellites (T_spin~3s) are equipped with magnetorquers that permit spin-up/down and reorientation maneuvers. The spin axis is placed normal to the orbit plane, allowing full pitch-angle resolution twice per spin. An energetic particle detector for ions (EPDI) measures 250keV-5MeV ions, addressing secondary science. Funded initially by CalSpace and the University Nanosat Program, ELFIN was selected for flight with joint support from NSF and NASA between 2014 and 2018."
"An overview of research on laser-plasma based acceleration of ions is given. The experimental state of the art is summarized and recent progress is discussed. The basic acceleration processes are briefly reviewed with an outlook on hybrid mechanisms and novel concepts. Finally, we put focus on the development of engineered targets for enhanced acceleration and of all-optical methods for beam post-acceleration and control."
"Streaking of photoelectrons with optical lasers has been widely used for temporal characterization of attosecond extreme ultraviolet pulses. Recently, this technique has been adapted to characterize femtosecond x-ray pulses in free-electron lasers with the streaking imprinted by farinfrared and Terahertz (THz) pulses. Here, we report successful implementation of THz streaking for time-stamping of an ultrashort relativistic electron beam of which the energy is several orders of magnitude higher than photoelectrons. Such ability is especially important for MeV ultrafast electron diffraction (UED) applications where electron beams with a few femtosecond pulse width may be obtained with longitudinal compression while the arrival time may fluctuate at a much larger time scale. Using this laser-driven THz streaking technique, the arrival time of an ultrashort electron beam with 6 fs (rms) pulse width has been determined with 1.5 fs (rms) accuracy. Furthermore, we have proposed and demonstrated a non-invasive method for correction of the timing jitter with femtosecond accuracy through measurement of the compressed beam energy, which may allow one to advance UED towards sub-10 fs frontier far beyond the ~100 fs (rms) jitter."
"We demonstrate a non-invasive time-sorting method for ultrafast electron diffraction (UED) experiments with radio-frequency (rf) compressed electron beams. We show that electron beam energy and arrival time at the sample after rf compression are strongly correlated such that the arrival time jitter may be corrected through measurement of the beam energy. The method requires minimal change to the infrastructure of most of the UED machines and is applicable to both keV and MeV UED. In our experiment with ~3 MeV beam, the timing jitter after rf compression is corrected with 35 fs root-mean-square (rms) accuracy, limited by the 3x10^-4 energy stability. For keV UED with high energy stability, sub-10 fs accuracy in time-sorting should be readily achievable. This time-sorting technique allows us to retrieve the 2.5 THz oscillation related to coherent A1g phonon in laser excited Bismuth film and extends the temporal resolution of UED to a regime far beyond the 100-200 fs rms jitter limitation."
"$^{222}$Rn is a noble radioactive gas produced along the $^{238}$U decay chain, which is present in the majority of soils and rocks. As $^{222}$Rn is the most relevant source of natural background radiation, understanding its distribution in the environment is of great concern for investigating the health impacts of low-level radioactivity and for supporting regulation of human exposure to ionizing radiation in modern society. At the same time, $^{222}$Rn is a widespread atmospheric tracer whose spatial distribution is generally used as a proxy for climate and pollution studies. Airborne gamma-ray spectroscopy (AGRS) always treated $^{222}$Rn as a source of background since it affects the indirect estimate of equivalent $^{238}$U concentration. In this work the AGRS method is used for the first time for quantifying the presence of $^{222}$Rn in the atmosphere and assessing its vertical profile. High statistics radiometric data acquired during an offshore survey are fitted as a superposition of a constant component due to the experimental setup background radioactivity plus a height dependent contribution due to cosmic radiation and atmospheric $^{222}$Rn. The refined statistical analysis provides not only a conclusive evidence of AGRS $^{222}$Rn detection but also a (0.96 $\pm$ 0.07) Bq/m$^{3}$ $^{222}$Rn concentration and a (1318 $\pm$ 22) m atmospheric layer depth fully compatible with literature data."
"In this paper we present the results of a $\sim$5 hour airborne gamma-ray survey carried out over the Tyrrhenian sea in which the height range (77-3066) m has been investigated. Gamma-ray spectroscopy measurements have been performed by using the AGRS_16L detector, a module of four 4L NaI(Tl) crystals. The experimental setup was mounted on the Radgyro, a prototype aircraft designed for multisensorial acquisitions in the field of proximal remote sensing. By acquiring high-statistics spectra over the sea (i.e. in the absence of signals having geological origin) and by spanning a wide spectrum of altitudes it has been possible to split the measured count rate into a constant aircraft component and a cosmic component exponentially increasing with increasing height. The monitoring of the count rate having pure cosmic origin in the >3 MeV energy region allowed to infer the background count rates in the $^{40}$K, $^{214}$Bi and $^{208}$Tl photopeaks, which need to be subtracted in processing airborne gamma-ray data in order to estimate the potassium, uranium and thorium abundances in the ground. Moreover, a calibration procedure has been carried out by implementing the CARI-6P and EXPACS dosimetry tools, according to which the annual cosmic effective dose to human population has been linearly related to the measured cosmic count rates."
"We provide a physical explanation for enhancement of the low-energy electron production by sensitizing nanoparticles due to irradiation by fast ions. It is demonstrated that a significant increase in the number of emitted electrons arises from the collective electron excitations in the nanoparticle. We predict a new mechanism of the yield enhancement due to the plasmon excitations and quantitatively estimate its contribution to the electron production. Revealing the nanoscale mechanism of the electron yield enhancement, we provide an efficient tool for evaluating the yield of emitted electron from various sensitizers. It is shown that the number of low-energy electrons generated by the gold and platinum nanoparticles of a given size exceeds that produced by the equivalent volume of water and by other metallic (e.g., gadolinium) nanoparticles by an order of magnitude. This observation emphasizes the sensitization effect of the noble metal nanoparticles and endorses their application in novel technologies of cancer therapy with ionizing radiation."
"The yield of electrons generated by gold nanoparticles due to irradiation by fast charged projectiles is estimated. The results of calculations are compared to those obtained for pure water medium. It is demonstrated that a significant increase in the number of emitted electrons arises from collective electron excitations in the nanoparticle. The dominating enhancement mechanisms are related to the formation of (i) plasmons excited in a whole nanoparticle, and (ii) atomic giant resonances due to excitation of d electrons in individual atoms. Decay of the collective electron excitations in a nanoparticle embedded in a biological medium thus represents an important mechanism of the low-energy electron production. Parameters of the utilized model approach are justified through the calculation of the photoabsorption spectra of several gold nanoparticles, performed by means of time-dependent density-functional theory."
"The transport of low-energy electrons through the coating of a radiosensitizing metallic nanoparticle under fast ion irradiation is analyzed theoretically and numerically. As a case study, we consider a poly(ethylene glycol)-coated gold nanoparticle of diameter 1.6~nm excited by a carbon ion in the Bragg peak region in water as well as by more energetic carbon ions. The diffusion equation for low-energy electrons emitted from a finite-size spherical source representing the surface of the metal core is solved to obtain the electron number density as a function of radial distance and time. Information on the atomistic structure and composition of the coating is obtained from molecular dynamics simulations performed with the MBN Explorer software package. Two mechanisms of low-energy electron production by the metallic core are considered: the relaxation of plasmon excitations and collective excitations of valence $d$ electrons in individual atoms of gold. Diffusion coefficients and characteristic lifetimes of electrons propagating in gold, water, and poly(ethylene glycol) are obtained from relativistic partial wave analysis and the dielectric formalism, respectively. On this basis, the number of electrons released through the organic coating into the surrounding aqueous medium and the number of hydroxyl radicals produced are evaluated. The largest increase of the radical yield due to low-energy electrons is observed when the nanoparticle is excited by an ion with energy significantly exceeding that in the Bragg peak region. It is also shown that the water content of the coating, especially near the surface of the metal core, is crucial for the production of hydroxyl radicals."
"We introduce a novel in-situ strong field ionization tomography approach for characterizing the spatial density distribution of gas jets. We show that for typical intensities in high harmonic generation experiments, the strong field ionization mechanism used in our approach provides an improvement in the resolution close to factor of 2 (resolving about 8 times smaller voxel volume), when compared to linear/single-photon imaging modalities.   We find, that while the depth of scan in linear tomography is limited by resolution loss due to the divergence of the driving laser beam, in the proposed approach the depth of focus is localized due to the inherent physical nature of strong-field interaction and discuss implications of these findings. We explore key aspects of the proposed method and compare it with commonly used single- and multi-photon imaging mechanisms. The proposed method will be particularly useful for strong field and attosecond science experiments."
"Network Extraction algorithms from X-ray microcomputed tomography have become a routine method to obtain pore connectivity and pore morphology information from porous media. The main approaches for this extraction are either based on Max Ball Algorithm or Medial Axis Extraction. The first is a robust method to separate the pore space into pore and throats, which provides the pore and throat sizes distributions directly. The second simplifies the medium by thinning the volume into a one voxel wide centerline that preserves the volume's topological information. Since each method has drastically different implementations, it is not usual to use both to characterize the porous structure. This work presents a method to extract a simplified centerline of porous materials by adding few more steps to the well-established Max Ball algorithm. Results for sandstones and carbonate rock samples show that this Medial-Axis network can be used for single phase flow simulations, as well as preserves the mediums morphology."
"In magnetized plasma physics, almost all developed analytic theories assume a Maxwellian distribution function (MDF) and in some cases small deviations are described using the perturbation theory. The deviations with respect to the Maxwellian equilibrium, called kinetic effects, are required to be taken into account specially for fusion reactor plasmas. Generally, because the perturbation theory is not consistent with observed steady-state non-Maxwellians, these kinetic effects are numerically evaluated by very CPU-expensive codes, avoiding the analytic complexity of velocity phase space integrals. We develop here a new method based on analytic non-Maxwellian distribution functions constructed from non-orthogonal basis sets in order to (i) use as few parameters as possible, (ii) increase the efficiency to model numerical and experimental non-Maxwellians, (iii) help to understand unsolved problems such as diagnostics discrepancies from the physical interpretation of the parameters, and (iv) obtain analytic corrections due to kinetic effects given by a small number of terms and removing the numerical error of the evaluation of velocity phase space integrals. This work does not attempt to derive new physical effects even if it could be possible to discover one from the better understandings of some unsolved problems, but here we focus on the analytic prediction of kinetic corrections from analytic non-Maxwellians. As applications, examples of analytic kinetic corrections are shown for the secondary electron emission, the Langmuir probe characteristic curve, and the entropy. This is done by using three analytic representations of the distribution function: the Kappa (KDF), the bi-modal or a new interpreted non-Maxwellian (INMDF) distribution function. [...]"
"The Reynolds stress, or equivalently the average of the momentum flux, is key to understanding the statistical properties of turbulent flows. Both typical and rare fluctuations of the time averaged momentum flux are needed to fully characterize the slow flow evolution. The fluctuations are described by a large deviation rate function that may be calculated either from numerical simulation, or from theory. We show that, for parameter regimes in which a quasilinear approximation is accurate, the rate function can be found by solving a matrix Riccati equation. Using this tool we compute for the first time the large deviation rate function for the Reynolds stress of a turbulent flow. We study a barotropic flow on a rotating sphere, and show that the fluctuations are highly non-Gaussian. This work opens up new perspectives for the study of rare transitions between attractors in turbulent flows."
"Crater has shown that, for two particles (with masses $m_1$ and $m_2$) in a Coulombic bound state, the charge distribution is equal to the sum of the two charge distributions obtained by taking $m_1\to\infty$ and $m_2\to\infty$ respectively, while keeping the same Coulombic potential. We provide a simple scaling criterion to determine whether an arbitrary Hamiltonian possesses this property. In particular we show that, for a Coulombic system, fine structure corrections preserve this Crater property while two-particle relativistic corrections and/or hyperfine corrections may destroy it."
The thermodynamic properties of 59 TeF6 clusters that undergo temperature-driven phase transitions have been calculated with a canonical J-walking Monte Carlo technique. A parallel code for simulations has been developed and optimized on SUN3500 and CRAY-T3E computers. The Lindemann criterion shows that the clusters transform from liquid to solid and then from one solid structure to another in the temperature region 60-130 K.
"Phase transitions (liquid-solid, solid-solid) triggered by temperature changes are studied in free nanosized clusters of TeF_6 (SF_6) with different negative charges assigned to the fluorine atoms. Molecular dynamics simulations at constant energy show that the charge increase from q_F=0.1 e to q_F=0.25 e shifts the melting temperature towards higher values and some of the metastable solid states disappear. The increased repulsive interaction maintains the order in molecular systems at higher temperatures."
"In this paper, we present a unified computational method based on pseudospectral approximations for the design of optimal pulse sequences in open quantum systems. The proposed method transforms the problem of optimal pulse design, which is formulated as a continuous time optimal control problem, to a finite dimensional constrained nonlinear programming problem. This resulting optimization problem can then be solved using existing numerical optimization suites. We apply the Legendre pseudospectral method to a series of optimal control problems on open quantum systems that arise in Nuclear Magnetic Resonance (NMR) spectroscopy in liquids. These problems have been well studied in previous literature and analytical optimal controls have been found. We find an excellent agreement between the maximum transfer efficiency produced by our computational method and the analytical expressions. Moreover, our method permits us to extend the analysis and address practical concerns, including smoothing discontinuous controls as well as deriving minimum energy controls. The method is not restricted to the systems studied in this article but is universal to every open quantum system whose performance is limited by dissipation."
"Complex absorbing potentials (CAPs) are artificial potentials added to electronic Hamiltonians to make the wavefunction of metastable electronic states square-integrable. This makes the electronic structure problem of electronic resonances comparable to that of electronic bound states, thus reducing the complexity of the problem. CAPs depend on two types of parameters: the coupling parameter $\eta$ and a set of spatial parameters which define the onset of the CAP. It has been a common practice over the years to minimize the CAP perturbation on the physical electronic Hamiltonian by running an $\eta-$trajectory, whereby one fixes the spatial parameters and varies $\eta$. The optimal $\eta$ is chosen according to the minimum log-velocity criterion. But the effectiveness of an $\eta-$trajectory strongly depends on the values of the fixed spatial parameters.   In this work, we propose a more general criterion, called the $\xi-$criterion, which allows one to minimize any CAP parameter, including the CAP spatial parameters. Indeed, we show that fixing $\eta$ and varying the spatial parameters according to a scheme (i.e., running a spatial trajectory) is a more efficient and reliable way of minimizing the CAP perturbations (which is assessed using the $\xi-$criterion). We illustrate the method by determining the resonance energy and width of the temporary anion of dinitrogen, at the Hartree-Fock and EOM-EA-CCSD levels, using two different types of CAPs: the box- and the smooth Voronoi-CAPs."
"The UNBSSI is a long-term effort for the development of astronomy and space science through regional and international cooperation in this field on a worldwide basis. A series of workshops on BSS was held from 1991 to 2004 (India 1991, Costa Rica and Colombia 1992, Nigeria 1993, Egypt 1994, Sri Lanka 1995, Germany 1996, Honduras 1997, Jordan 1999, France 2000, Mauritius 2001, Argentina 2002, and China 2004; http://www.seas.columbia.edu/~ah297/un-esa/) and addressed the status of astronomy in Asia and the Pacific, Latin America and the Caribbean, Africa, and Western Asia. One major recommendation that emanated from these workshops was the establishment of astronomical facilities in developing nations for research and education programmes at the university level. Such workshops on BSS emphasized the particular importance of astrophysical data systems and the virtual observatory concept for the development of astronomy on a worldwide basis. Pursuant to resolutions of the United Nations Committee on the Peaceful Uses of Outer Space (UNCOPUOS) and its Scientific and Technical Subcommittee, since 2005, these workshops focused on the International Heliophysical Year 2007 (UAE 2005, India 2006, Japan 2007, Bulgaria 2008, Ro Korea 2009; http://www.unoosa.org/oosa/SAP/bss/ihy2007/index.html). Starting in 2010, the workshops focus on the International Space Weather Initiative (ISWI) as recommended in a three-year-work plan as part of the deliberations of UNCOPUOS (http://www.stil.bas.bg/ISWI/). Workshops on the ISWI have been scheduled to be hosted by Egypt in 2010 for Western Asia, Nigeria in 2011 for Africa, and Ecuador in 2012 for Latin America and the Caribbean. Currently, 14 IHY/ISWI instrument arrays with > 600 instruments are operational in 95 countries."
"It is shown that the most important effects of special and general theory of relativity can be understood in a simple and straightforward way. The system of units in which the speed of light $c$ is the unit of velocity allows to cast all formulas in a very simple form.The Pythagorean theorem graphically relates energy, momentum and mass. The paper is addressed to those who teach and popularize the theory of relativity."
"If Kermit's What-Happens-Next-Machine had functioned, you would not have seen much, because it would have gone too quickly. In this article it is shown that putting up and solving the equations of motion of a seemingly simple mechanical apparatus presents a challenging problem. The simulation can, however, be quite instructive and also entertaining."
"We have designed, built and operated a physical pendulum which allows one to demonstrate experimentally the behaviour of the pendulum under any equation of motion for such a device for any initial conditions. All parameters in the equation of motion can be defined by the user. The potential of the apparatus reaches from demonstrating simple undamped harmonic oscillations to complex chaotic behaviour of the pendulum. The position data of the pendulum as well as derived kinematical quantities like velocity and acceleration can be stored for later offline analysis."
"I reconsider the problem of a raindrop falling through mist, collecting mass, and generalize it to allow an arbitrary power-law form for the accretion rate. I show that the coupled differential equations can be solved by the simple trick of temporarily eliminating time (t) in favor of the raindrop's mass (m) as the independent variable"
"Experiments in mechanics can often be timed by the sounds they produce. In such cases, digital audio recordings provide a simple way of measuring time intervals with an accuracy comparable to that of photogate timers. We illustrate this with an experiment in the physics of sports: to measure the speed of a hard-kicked soccer ball."
"The evaluation of variation in oscillation time period of a simple pendulum as its mass varies proves a rich source of discussion in a physics class-room, overcoming erroneous notions carried forward by students as to what constitutes a pendulum's length due to picking up only the results of approximations and ignoring the rigorous definition. The discussion also presents a exercise for evaluating center of mass of geometrical shapes and system of bodies. In all, the pedagogical value of the problem is worth both theoretical and experimental efforts. This article discusses the theoretical considerations."
"A vertically hanging chain is released from rest and falls due to gravity on a scale pan. We discuss the various experimental and theoretical aspects of this classic problem. Careful time-resolved force measurements allow us to determine the differences between the idealized and its implementation in the laboratory problem. We observe that, in spite of the upward force exerted by the pan on the chain, the free end at the top falls faster than a freely falling body. Because a real chain exhibits a finite minimum radius of curvature, the contact at the bottom results in a tensional force which pulls the falling part downward."
"We present visual calculations in special relativity using spacetime diagrams drawn on graph paper that has been rotated by 45 degrees. The rotated lines represent lightlike directions in Minkowski spacetime, and the boxes in the grid (called ""light-clock diamonds"") represent units of measurement modeled on the ticks of an inertial observer's lightclock. We show that many quantitative results can be read off a spacetime diagram by counting boxes, using a minimal amount of algebra. We use the Doppler Effect, in the spirit of the Bondi k-calculus, to motivate the method."
"The problem of a cylinder of mass m and radius r, with its center of mass out of the cylinder axis, rolling on an incline that makes an angle with respect to the horizontal is analyzed. The equation of motion is partially solved to obtain the site where the cylinder loses contact with the incline (jumps). Several simplifications are made: the analyzed system consists of an homogeneous disc with a one dimensional straight line of mass parallel to the disc axis at a distance y < r of the center of the cylinder. To compare our results with experimental data, we use a Styrofoam cylinder to which a long brass rod was imbibed parallel to the disc axis at a distance y < r from it, so the center of mass lies at a distance d from the center of the cylinder. Then the disc rolls without slipping on a long wooden ramp inclined at 15, 30 and 45 degrees with respect to the horizontal. To determine the jumping site, the motion was recorded with a high-speed video camera (Casio EX ZR100) at 200 and 480 frames per second. The experimental results agree well with the theoretical predictions."
"A slinky is an example of a tension spring: in an unstretched state a slinky is collapsed, with turns touching, and a finite tension is required to separate the turns from this state. If a slinky is suspended from its top and stretched under gravity and then released, the bottom of the slinky does not begin to fall until the top section of the slinky, which collapses turn by turn from the top, collides with the bottom. The total collapse time t_c (typically ~0.3 s for real slinkies) corresponds to the time required for a wave front to propagate down the slinky to communicate the release of the top end. We present a modification to an existing model for a falling tension spring (Calkin 1993) and apply it to data from filmed drops of two real slinkies. The modification of the model is the inclusion of a finite time for collapse of the turns of the slinky behind the collapse front propagating down the slinky during the fall. The new finite-collapse time model achieves a good qualitative fit to the observed positions of the top of the real slinkies during the measured drops. The spring constant k for each slinky is taken to be a free parameter in the model. The best-fit model values for k for each slinky are approximately consistent with values obtained from measured periods of oscillation of the slinkies."
"Inspired by Edgar Allan Poe's The Pit and the Pendulum, we investigate a radially driven, lengthening pendulum. We first show that increasing the length of an undriven pendulum at a uniform rate does not amplify the oscillations in a manner consistent with the behavior of the scythe in Poe's story. We discuss parametric amplification and the transfer of energy (through the parameter of the pendulum's length) to the oscillating part of the system. In this manner radial driving may easily and intuitively be understood, and the fundamental concept applied in many other areas. We propose and show by a numerical model that appropriately timed radial forcing can increase the oscillation amplitude in a manner consistent with Poe's story. Our analysis contributes a computational exploration of the complex harmonic motion that can result from radially driving a pendulum, and sheds light on a mechanism by which oscillations can be amplified parametrically. These insights should prove especially valuable in the undergraduate physics classroom, where investigations into pendulums and oscillations are commonplace."
"This paper provides a simplified explanation of the vacuum bazooka through diagrams and builds a theoretical model only using concepts found in introductory mechanics. Our theory suggests that the velocity of the projectile is proportional to the hyperbolic tangent of time, and experimental measurements support this claim. We also find that the vacuum bazooka could be used to demonstrate the concept of terminal velocity in a classroom setting."
"An exhaustive analysis of the leaky tank car problem is presented, pointing out its intriguing physical properties, which well serve to students and teachers for illustrating standard Newtonian mechanics in a highly non-standard fashion. Calculus (at a leading undergraduate level) is effectively required only to examine some details concerning the solution of the equation of motion and interesting limiting cases. Instead we let any student to appreciate all the physical content of the problem, within a proper simple model and its generalization, ranging from the motion of the leaky tank car to that of the water flowing from the car with their peculiarities. Generalizations of the problem to more than one draining hole (even with different sizes), as well as to ""two-dimensional"" geometries, reveal further intriguing results, culminating into a no-rotation theorem and its corollaries, thus rendering unique the problem at hand for allowing students to fully recognize the power of physical analysis."
"We explain simple laboratory experiments for making quantitative measurements of the Doppler effect from sources with acceleration. We analyze the spectra and clarify the conditions for the Doppler effect to be experimentally measurable, which turn out to be non-trivial when acceleration is involved. The experiments use sources with gravitational acceleration, in free fall and in motion as a pendulum, so that the results can be checked against fundamental physics principles. The experiments can be easily set up from ``off the shelf'' components only. The experiments are suitable for a wide range of students, including undergraduates not majoring in science or engineering."
"When explaining to a lay audience the magnitude of forces or accelerations imparted to vehicles or experienced by vehicle occupants during a motor vehicle collision, it is often helpful to recast the critical results in terms of other physical systems or impact configurations which will reproduce the equivalent dynamics of the subject accident to serve as a conceptual aid for the audience. In this article, we present the basis for such equivalents and explicitly demonstrate, using two physics simulation software packages, that such equivalents are based on nothing more than the application of the laws of physics."
"This work focuses on a theoretical analysis of a well-known inertia demonstration, which uses a weight suspended by a string with an extra string that hangs below the weight. The point in question is which string, the upper or lower, will break when pulling down on the bottom edge of the lower string at a constant pulling speed. An analytic solution for the equation of motion allows us to identify the critical value of the pulling speed, beyond which the string breaking varies from one to the other. The analysis also provides us a phase diagram that illustrates the interplay between the pulling speed and the string's elasticity in the pull-or-jerk experiment."
"Molecular absorption and photo-electron spectra can be efficiently predicted with real-time time-dependent density-functional theory (TDDFT). We show here how these techniques can be easily extended to study time-resolved pump-probe experiments in which a system response (absorption or electron emission) to a probe pulse, is measured in an excited state. This simulation tool helps to interpret the fast evolving attosecond time-resolved spectroscopic experiments, where the electronic motion must be followed at its natural time-scale. We show how the extra degrees of freedom (pump pulse duration, intensity, frequency, and time-delay), which are absent in a conventional steady state experiment, provide additional information about electronic structure and dynamics that improve a system characterization. As an extension of this approach, time-dependent 2D spectroscopies can also be simulated, in principle, for large-scale structures and extended systems."
"Zhang and coworkers have recently reported results of experiments involving irradiation of argon clusters doped with bromofluorene chromophores by nanosecond-long pulses of 532 nm laser light. Multiply-charged ions of atomic argon (charge states, n, up to 7) and carbon (charge states up to 4) are observed which are sought to be rationalised using an evaporation model. The distinguishing facet of exploding clusters being progenitors of energetic ions and electrons constitutes the key driver for contemporary research in laser-cluster interactions; it is, therefore, important to point out inconsistencies that are intrinsic to the model of Zhang and coworkers. In light of similar reports already in the literature, we show that their model is of limited utility in describing the dynamics that govern how fast, multiply-charged atomic ions result from laser irradiation of gas-phase clusters. We posit that it is plasma behaviour that underpins cluster heating and cluster explosion dynamics."
"The electromagnetic radiation pressure becomes dominant in the interaction of the ultra-intense electromagnetic wave with a solid material, thus the wave energy can be transformed efficiently into the energy of ions representing the material and the high density ultra-short relativistic ion beam is generated. This regime can be seen even with present-day technology, when an exawatt laser will be built. As an application, we suggest the laser-driven heavy ion collider."
"The pseudo-spectral analytical time-domain (PSATD) particle-in-cell (PIC) algorithm solves the vacuum Maxwell's equations exactly, has no Courant time-step limit (as conventionally defined), and offers substantial flexibility in plasma and particle beam simulations. It is, however, not free of the usual numerical instabilities, including the numerical Cherenkov instability, when applied to relativistic beam simulations. This paper derives and solves the numerical dispersion relation for the PSATD algorithm and compares the results with corresponding behavior of the more conventional pseudo-spectral time-domain (PSTD) and finite difference time-domain (FDTD) algorithms. In general, PSATD offers superior stability properties over a reasonable range of time steps. More importantly, one version of the PSATD algorithm, when combined with digital filtering, is almost completely free of the numerical Cherenkov instability for time steps (scaled to the speed of light) comparable to or smaller than the axial cell size."
"Transmission of highly infectious respiratory diseases, including SARS-CoV-2 are facilitated by the transport of tiny droplets and aerosols (harboring viruses, bacteria, etc.) that are breathed out by individuals and can remain suspended in air for extended periods of time in confined environments. A passenger car cabin represents one such situation in which there exists an elevated risk of pathogen transmission. Here we present results from numerical simulations of the potential routes of airborne transmission within a model car geometry, for a variety of ventilation configurations representing different combinations of open and closed windows. We estimate relative concentrations and residence times of a non-interacting, passive scalar -- a proxy for infectious pathogenic particles -- that are advected and diffused by the turbulent airflows inside the cabin. Our findings reveal that creating an airflow pattern that travels across the cabin, entering and existing farthest from the occupants can potentially reduce the transmission."
"This work explores the possibility of atomic cluster beams as a probe for neutral atom microscopy (NAM) measurements. Using a beam of Kr clusters with mean size $\sim$ 10$^4$ atoms/cluster we demonstrate that topographical contrast can be obtained, similar to that in the case of monoatomic beams. Further, using atomically thin films of MoS$_2$ grown on SiO$_2$/Si substrate we show that NAM imaging using Kr clusters is also possible in domains where topographical contrast is not expected. Surprisingly, these images show an inverted contrast pattern when compared to the case of monoatomic beams. We attempt to understand these observations on the basis of angular distributions resulting from cluster-surface scattering. Finally, we discuss the implications of these results towards achieving a high lateral resolution neutral atom microscope using atomic cluster beams."
"Slow extraction of beam from synchrotrons or storage rings as required by many fixed target experiments is performed by controlled excitation and feeding of a structural lattice resonance. Due to the sensitive nature of this resonant extraction process, the temporal structure of the extracted beam is modulated by the minuscule current fluctuations present on the quadrupole magnet power supplies. Such a modulation lead to pile-ups in detectors and a significant reduction in accumulated event statistics. This contribution proposes and experimentally demonstrates that by an introduction of further modulation on quadrupole currents with a specific amplitude and frequency, the inherent power supply fluctuations are mitigated leading to a smoothening of the beam temporal structure. The slow extraction beam dynamics associated with this method are explained along with the operational results."
"The World Year of Physics (2005) is an international celebration to commemorate the one hundredth anniversary of Einstein's ""Annus Mirabilis"". The United Nations has officially declared 2005 the International Year of Physics. However, the impact of Einstein's ideas was not restricted to physics. Among numerous other disciplines, Einstein also made significant and specific contributions to Earth Sciences. His geosciences-related letters, comments, and scientific articles, are dispersed, not easily accesible and are poorly known. The present review attempts to integrate them, as a tribute to Einstein in commemoration of this centenary. These contributions can be classified into three basic areas: geodynamics, geological (planetary) catastrophism and fluvial geomorphology."
"The obscuration of a celestial body that covers another one in the background will be called a ``hierarchical eclipse''. The most obvious case is that a star or a planet will be hidden from sight by the moon during a lunar eclipse. Four objects of the solar system will line up then. We investigate this phenomenon with respect to the region of visibility and periodicity. There exists a parallax field constraining the chances for observation. A historic account from the Middle Ages is preserved that we analyse from different viewing angles. Furthermore, we provide a list of events from 0 to 4000 AD. From this, it is apparent that Jupiter is most often involved in such spectacles because its orbit inclination is small. High-inclination orbits reduce the probability to have a coincidence of an occultation of that object with a lunar eclipse."
"Planetary and magnetohydrodynamic drift-wave turbulence is observed to self-organize into large scale structures such as zonal jets and coherent vortices. In this Letter we present a non-equilibrium statistical theory, the Stochastic Structural Stability theory (SSST), that can make predictions for the formation and finite amplitude equilibration of non-zonal and zonal structures (lattice and stripe patterns) in homogeneous turbulence. This theory reveals that the emergence of large scale structure is the result of an instability of the interaction between the coherent flow and the associated turbulent field. Comparison of the theory with nonlinear simulations of a barotropic flow in a beta-plane channel with turbulence sustained by isotropic random stirring, demonstrates that SSST predicts the threshold parameters at which the coherent structures emerge as well as the characteristics of the emerging structures (scale, amplitude, phase speed). It is shown that non-zonal structures (lattice states or zonons) emerge at lower energy input rates of the stirring compared to zonal flows (stripe states) and their emergence affects the dynamics of jet formation."
"We analyse the nonlinear dynamics of the large scale flow in Rayleigh-B\'enard convection in a two-dimensional, rectangular geometry of aspect ratio $\Gamma$. We impose periodic and free-slip boundary conditions in the streamwise and spanwise directions, respectively. As Rayleigh number Ra increases, a large scale zonal flow dominates the dynamics of a moderate Prandtl number fluid. At high Ra, in the turbulent regime, transitions are seen in the probability density function (PDF) of the largest scale mode. For $\Gamma = 2$, the PDF first transitions from a Gaussian to a trimodal behaviour, signifying the emergence of reversals of the zonal flow where the flow fluctuates between three distinct turbulent states: two states in which the zonal flow travels in opposite directions and one state with no zonal mean flow. Further increase in Ra leads to a transition from a trimodal to a unimodal PDF which demonstrates the disappearance of the zonal flow reversals. On the other hand, for $\Gamma = 1$ the zonal flow reversals are characterised by a bimodal PDF of the largest scale mode, where the flow fluctuates only between two distinct turbulent states with zonal flow travelling in opposite directions."
"Volcanic jet flows in explosive eruptions emit radio frequency signatures, indicative of their fluid dynamic and electrostatic conditions. The emissions originate from sparks supported by an electric field built up by the ejected charged volcanic particles. When shock-defined, low-pressure regions confine the sparks, the signatures may be limited to high-frequency content corresponding to the early components of the avalanche-streamer-leader hierarchy. Here, we image sparks and a standing shock together in a transient supersonic jet of micro-diamonds entrained in argon. Fluid dynamic and kinetic simulations of the experiment demonstrate that the observed sparks originate upstream of the standing shock. The sparks are initiated in the rarefaction region, and cut off at the shock, which would limit their radio frequency emissions to a tell-tale high-frequency regime. We show that sparks transmit an impression of the explosive flow, and open the way for novel instrumentation to diagnose currently inaccessible explosive phenomena."
"Symmetry Adapted Perturbation Theory (SAPT) has become an important tool when predicting and analyzing intermolecular interactions. Unfortunately, DFT-SAPT, which uses Density Functional Theory (DFT) for the underlying monomers, has some arbitrariness concerning the exchange-correlation potential and the exchange-correlation kernel involved. By using ab initio Brueckner Doubles densities and constructing Kohn-Sham orbitals via the Zhao-Morrison-Parr (ZMP) method, we are able to lift the dependence of DFT-SAPT on DFT exchange-correlation potential models in first order. This way, we can compute the monomers at the Coupled-Cluster level of theory and utilize SAPT for the intermolecular interaction energy. The resulting ZMP-SAPT approach is tested for small dimer systems involving rare gas atoms, cations, and anions and shown to compare well with the Tang-Toennies model and coupled cluster results."
"Energy storage is needed to enable dispatchable renewable energy supply and thereby full decarbonization of the grid. However, this can only occur with drastic cost reductions compared to current battery technology, with predicted targets for the cost per unit energy (CPE) below $20/kWh. Notably, for full decarbonization, long duration storage up to 100 hrs will be needed at such low costs, and prior analyses have shown that in such high renewable penetration scenarios, CPE is more critical than other parameters such as round trip efficiency or cost per unit power when comparing the costs of different technologies. Here, we introduce an electricity storage concept that stores electricity as sensible heat in graphite storage blocks and uses multi-junction thermophotovoltaics (TPV) as a heat engine to convert it back to electricity on demand. This design is an outgrowth of the system proposed by Amy et al. in 2019, which has been modified here to use a solid graphite medium and molten tin as a heat transfer fluid rather than silicon as both. The reason for this is two-fold: (1) the CPE of graphite is almost 10X lower than that of silicon, which derives from the lower cost per unit mass (i.e., $0.5/kg vs. $1.5/kg) and the higher heat capacity per unit mass (2000 J/kg-K vs. 950 J/kg-K); and (2) the melting point tin and solubility of tin in graphite are much lower than that of silicon, which lessens the number of issues that have to overcome along the research and development pathway. The usage of graphite also eliminates the need for a second tank, but the main disadvantage of using a solid medium is that one cannot easily provide a steady discharge rate, as the power output from the storage will change with time, as the graphite cools during discharge. Thus, the objective of this work is to examine how these changes in the system design effect the overall technoeconomics."
"This dissertation explores the interaction between high-intensity lasers and plasmas to accelerate electrons and produce radiation via experimental and computational efforts. The laser pulses used in this dissertation have ultrashort duration (< 100 fs), near-infrared to mid-infrared wavelength (0.8 $\mu$m, 2 $\mu$m, or 3.9 $\mu$m), millijoules of energy, and high repetition rates (480 Hz or 20 Hz). The plasma sources applied are from solid-density targets (overdense) or gaseous targets (underdense). With the high-repetition-rate capability, statistical methods are employed to optimize certain aspect of the experiments and to interpret the physics."
"The widespread diffusion of precision radiotherapy techniques, geared toward the release of larger dose gradients in shorter time frames, is leading to new challenges in dosimetry. Accurate dose measurements are essential to check for beam anomalies and inaccuracies to ensure treatment efficacy and patient safety during radiotherapy. This work describes the main features of a diamond dosimeter coupled to an extremely compact front-end electronics. The detection system was tested under the X-ray pulses generated by a medical LINAC for both the 6 MV and the 18 MV accelerating voltages. Located in the LINAC's bunker, it eliminates the need for a long cable connection between the detector and the electronics, detrimental for the system response speed. Signal acquisition was performed synchronously with the impinging X-ray pulses with a sampling period as low as 20 us, allowing for a real-time beam monitoring. The dosimeter demonstrated a very good stability despite the high value of the absorbed dose during the performed experiments (about 100 Gy). The measured dose-per-pulse values of 278 uGy and 556 uGy at 6 MV and 18 MV, respectively, are in excellent agreement with the nominal values expected for the LINAC apparatus used for the tests. In addition to single-pulse measurements, fundamental for dynamic radiotherapy, the proposed system also allows for the calculation of both the total collected charge and the photocurrent generated by the detector. In this regards, despite the compactness, it demonstrates its effectiveness as a tool for source diagnostics in terms of both beam intensity and emission timing."
"Magnetic pickup loops or B-dot probes are one of the oldest known sensors of time-varying magnetic fields. The operating principle is based on Faraday's law of electromagnetic induction. However, obtaining accurate measurements of time-varying magnetic fields using these kinds of probes is a challenging task. A B-dot probe and its associated circuit are prone to electrical oscillations. As a result, the measured signal may not faithfully represent the magnetic field sampled by the B-dot probe. In this paper, we have studied the transient response of a B-dot probe and its associated circuit to a time-varying magnetic field. Methods of removing the oscillations pertaining to the detector structure are described. After removing the source of the oscillatory signal, we have shown that the time-integrated induced emf measured by the digitiser is linearly proportional to the magnetic field sampled by the B-dot probe, thus verifying the faithfulness of the measured signal."
"Benford's Law describes the finding that the distribution of leading (or leftmost) digits of innumerable datasets follows a well-defined logarithmic trend, rather than an intuitive uniformity. In practice this means that the most common leading digit is 1, with an expected frequency of 30.1%, and the least common is 9, with an expected frequency of 4.6%. The history and development of Benford's Law is inexorably linked to physics, yet there has been a dearth of physics-related Benford datasets reported in the literature. Currently, the most common application of Benford's Law is in detecting number invention and tampering such as found in accounting-, tax-, and voter-fraud. We demonstrate that answers to end-of-chapter exercises in physics and chemistry textbooks conform to Benford's Law. Subsequently, we investigate whether this fact can be used to gain advantage over random guessing in multiple-choice tests, and find that while testbank answers in introductory physics closely conform to Benford's Law, the testbank is nonetheless secure against such a Benford's attack for banal reasons."
"Spontaneous symmetry breaking occurs when the symmetry that a physical system possesses, is not preserved for the ground state of the system. Although the procedure of symmetry breaking is quite clear from the mathematical point of view, the physical interpretation of the phenomenon is worth to be better understood. In this note we present a simple and instructive example of the symmetry breaking in a mechanical system. It demonstrates that the spontaneous symmetry breaking can occur for the spatially extended solutions in a potential characterised by a single minimum."
"The paper describes two general problems encountered in computational assignments at the introductory level. First, novice students often treat computer code as almost magic incantations, and like novices in many fields, have trouble creating new algorithms or procedures to solve novel problems. Second, the nature of computational studies often means that the results generated are interpreted via theoretically devised quantities, which may not meet a student's internal standards for proof when compared to an experimental measurement.   The paper then offers a lab/programming assignment, used in a calculus-based physics course, which was devised to address these problems. In the assignment, students created a computational model of the heat flow involved in heating an iron rod in a blacksmith's forge. After creating the simulation, students attended a blacksmithing seminar and had a chance to work with iron and take data on its heating in a coke-fueled forge. On their return to campus, students revised their computational models in light of their experimental data."
"The tree-based rope swing is a popular recreation facility, often installed in outdoor areas, giving pleasure to thrill-seekers. In the setting, one drops down from a high platform, hanging from a rope, then swings at a great speed like ""Tarzan"", and finally jumps ahead to land on the ground. The question now arises: How far can Tarzan jump by the swing? In this article, I present an introductory analysis of the Tarzan swing mechanics, a big pendulum-like swing with Tarzan himself attached as weight. The analysis enables determination of how farther forward Tarzan can jump using a given swing apparatus. The discussion is based on elementary mechanics and, therefore, expected to provide rich opportunities for investigations using analytic and numerical methods."
"A theory to predict the steady state position of a dissipative, flow-controlled system, as defined by a control volume, is developed based on the Maximum Entropy (MaxEnt) principle of Jaynes, involving minimisation of a generalised free energy-like potential. The analysis provides a theoretical justification of a local, conditional form of the Maximum Entropy Production (MEP) principle, which successfully predicts the observable properties of many such systems. The analysis reveals a very different manifestation of the second law of thermodynamics in steady state flow systems, which {provides a driving force for} the formation of complex systems, including life."
"A strong inhomogeneous static electric field is used to spatially disperse a supersonic beam of polar molecules, according to their quantum state. We show that the molecules residing in the lowest-lying rotational states can be selected and used as targets for further experiments. As an illustration, we demonstrate an unprecedented degree of laser-induced 1D alignment $(<\cos^2\theta_{2D}>=0.97)$ and strong orientation of state-selected iodobenzene molecules. This method should enable experiments on pure samples of polar molecules in their rotational ground state, offering new opportunities in molecular science."
"We have systematically investigated the energy resolution of a magnetic micro-calorimeter (MMC) for atomic and molecular projectiles at impact energies ranging from $E\approx13$ to 150 keV. For atoms we obtained absolute energy resolutions down to $\Delta E \approx 120$ eV and relative energy resolutions down to $\Delta E/E\approx10^{-3}$. We also studied in detail the MMC energy-response function to molecular projectiles of up to mass 56 u. We have demonstrated the capability of identifying neutral fragmentation products of these molecules by calorimetric mass spectrometry. We have modeled the MMC energy-response function for molecular projectiles and conclude that backscattering is the dominant source of the energy spread at the impact energies investigated. We have successfully demonstrated the use of a detector absorber coating to suppress such spreads. We briefly outline the use of MMC detectors in experiments on gas-phase collision reactions with neutral products. Our findings are of general interest for mass spectrometric techniques, particularly for those desiring to make neutral-particle mass measurements."
"We describe a macroscopic beam splitter for polar neutral molecules. A complex electrode structure is required for the beam splitter which would be very difficult to produce with traditional manufacturing methods. Instead, we make use of a nascent manufacturing technique: 3D printing of a plastic piece, followed by electroplating. This fabrication method opens a plethora of avenues for research, since 3D printing imposes practically no limitations on possible shapes, and the plating produces chemically robust, conductive construction elements with an almost free choice of surface material; it has the added advantage of dramatically reduced production cost and time. Our beam splitter is an electrostatic hexapole guide that smoothly transforms into two bent quadrupoles. We demonstrate the correct functioning of this device by separating a supersonic molecular beam of ND3 into two correlated fractions. It is shown that this device can be used to implement experiments with differential detection wherein one of the fractions serves as a probe and the other as a reference. Reverse operation would allow to merging of two beams of neutral polar molecules."
"We describe the design, assembly, and testing of a magnet intended to deflect beams of paramagnetic nanoclusters, molecules, and atoms. It is energized by high-grade permanent neodymium magnets. This offers a convenient option in terms of cost, portability, and scalability of the construction, while providing field and gradient values (1.1 T, 330 T/m) which are fully comparable with commonly used electromagnet deflectors."
"Magnetic guidance of cochlear implants is a promising technique to reduce the risk of physical trauma during surgery. In this approach, a magnet attached to the tip of the implant electrode array is guided within the scala tympani using a magnetic field. After surgery, the magnet must be detached from the implant electrode array via localized heating and removed from the scala tympani which may cause thermal trauma. Objectives: The objective of this work is to experimentally validate a three-dimensional (3D) heat transfer model of the scala tympani which will enable accurate predictions of the maximum safe input power to avoid localized hyperthermia when detaching the magnet from the implant electrode array. Methods: Experiments are designed using a rigorous scale analysis and performed by measuring transient temperatures in a 3D-printed scala tympani phantom subjected to a sudden change in its thermal environment and localized heating via a small heat source. Results: The measured and predicted temperatures are in good agreement with an error less than 6$\%$. Conclusions: The validated 3D heat transfer model of the scala tympani is finally applied to evaluate the maximum safe input power to avoid localized hyperthermia when detaching the magnet. For the most conservative case where all boundaries except the insertion opening are adiabatic, the power required to release the magnet attached to the implant electrode array by 1 mm$^3$ of paraffin is approximately half of the predicted maximum safe input power. Significance: This work will enable the design of a thermally safe magnetic cochlear implant surgery procedure."
"We present a description of the capabilities and performance of the NAval Ultra-Trace Isotope Laboratory's Universal Spectrometer (NAUTILUS) at the U.S. Naval Research Laboratory. The NAUTILUS combines secondary ion mass spectrometry (SIMS) and single-stage accelerator mass spectrometry (SSAMS) into a single unified instrument for spatially resolved trace element and isotope analysis. The NAUTILUS instrument is essentially a fully functional SIMS instrument with an additional molecule-filtering detector, the SSAMS. The combination of these two techniques mitigates the drawbacks of each and enables new measurement paradigms for SIMS-like microanalysis. Highlighted capabilities include molecule-free raster ion imaging for direct spatially resolved analysis of heterogeneous materials with or without perturbed isotopic compositions. The NAUTILUS' sensitivity to trace elements is at least 10x better than commercial SIMS instruments due to near-zero background conditions. We describe the design and construction of the NAUTILUS, and its performance applied to topics in nuclear materials analysis, cosmochemistry, and geochemistry."
In this paper we present a pedestrian review of the theoretical fact that all relativistic wave equations possess solutions of arbitrary velocities $0 \leq v < \infty$. We discuss some experimental evidences of $v \geq c$ transmission of electromagnetic field configurations and the importance of these facts with regard to the principle of relativity.
"For more than a century, physics has known of a puzzling conflict between the T-asymmetry of thermodynamic phenomena and the T-symmetry of the underlying microphysics on which these phenomena depend. This paper provides a guide to the current status of this puzzle, distinguishing the central issue from various issues with which it may be confused. It is shown that there are two competing conceptions of what is needed to resolve the puzzle of the thermodynamic asymmetry, which differ with respect to the number of distinct T-asymmetries they take to be manifest in the physical world. On the preferable one-asymmetry conception, the remaining puzzle concerns the ordered distribution of matter in the early universe. The puzzle of the thermodynamic arrow thus becomes a puzzle for cosmology."
"This paper expounds the relations between continuous symmetries and conserved quantities, i.e. Noether's ``first theorem'', in both the Lagrangian and Hamiltonian frameworks for classical mechanics. This illustrates one of mechanics' grand themes: exploiting a symmetry so as to reduce the number of variables needed to treat a problem.   I emphasise that, for both frameworks, the theorem is underpinned by the idea of cyclic coordinates; and that the Hamiltonian theorem is more powerful. The Lagrangian theorem's main ``ingredient'', apart from cyclic coordinates, is the rectification of vector fields afforded by the local existence and uniqueness of solutions to ordinary differential equations. For the Hamiltonian theorem, the main extra ingredients are the asymmetry of the Poisson bracket, and the fact that a vector field generates canonical transformations iff it is Hamiltonian."
"A discussion is given of Robert Brown's original observations of particles ejected by pollen of the plant \textit{Clarkia pulchella} undergoing what is now called Brownian motion. We consider the nature of those particles, and how he misinterpreted the Airy disc of the smallest particles to be universal organic building blocks. Relevant qualitative and quantitative investigations with a modern microscope and with a ""homemade"" single lens microscope similar to Brown's, are presented."
"The air density on earth decays as a function of altitude $z$ approximately according to an $\exp(-w\,z/\theta)$-law, where $w$ denotes the weight of a nitrogen molecule and $\theta=\kB T$ where $k_B$ is a constant and $T$ the thermodynamic temperature. To derive this law one usually invokes the Boltzmann factor, itself derived from statistical considerations. We show that this (barometric) law may be derived solely from the democritian concept of corpuscles moving in vacuum. We employ a principle of simplicity, namely that this law is \emph{independent} of the law of corpuscle motion. This view-point puts aside restrictive assumptions that are source of confusion. Similar observations apply to the ideal-gas law. In the absence of gravity, when a cylinder terminated by a piston, containing a single corpuscle and with height $h$ has temperature $\theta$, the average force that the corpuscle exerts on the piston is: $\ave{F}=\theta/h$. This law is valid at any temperature, except at very low temperatures when quantum effects are significant and at very high temperatures because the corpuscle may then split into smaller parts. It is usually derived under the assumption that the temperature is proportional to the corpuscle kinetic energy, or else, from a form of the quantum theory. In contradistinction, we show that it follows solely from the postulate this it is independent of the law of corpuscle motion. On the physical side we employ only the concept of potential energy. A consistent picture is offered leading to the barometric law when $w\,h\gg\theta$, and to the usual ideal-gas law when $w\,h\ll\theta$. The mathematics is elementary. The present paper should accordingly facilitate the understanding of the physical meaning of the barometric and ideal-gas laws."
"Physicists are very familiar with forced and parametric resonance, but usually not with self-oscillation, a property of certain dynamical systems that gives rise to a great variety of vibrations, both useful and destructive. In a self-oscillator, the driving force is controlled by the oscillation itself so that it acts in phase with the velocity, causing a negative damping that feeds energy into the vibration: no external rate needs to be adjusted to the resonant frequency. The famous collapse of the Tacoma Narrows bridge in 1940, often attributed by introductory physics texts to forced resonance, was actually a self-oscillation, as was the swaying of the London Millennium Footbridge in 2000. Clocks are self-oscillators, as are bowed and wind musical instruments. The heart is a ""relaxation oscillator,"" i.e., a non-sinusoidal self-oscillator whose period is determined by sudden, nonlinear switching at thresholds. We review the general criterion that determines whether a linear system can self-oscillate. We then describe the limiting cycles of the simplest nonlinear self-oscillators, as well as the ability of two or more coupled self-oscillators to become spontaneously synchronized (""entrained""). We characterize the operation of motors as self-oscillation and prove a theorem about their limit efficiency, of which Carnot's theorem for heat engines appears as a special case. We briefly discuss how self-oscillation applies to servomechanisms, Cepheid variable stars, lasers, and the macroeconomic business cycle, among other applications. Our emphasis throughout is on the energetics of self-oscillation, often neglected by the literature on nonlinear dynamical systems."
"A demonic being, introduced by Maxwell, to miraculously create thermal non-equilibrium and violate the Second law of thermodynamics, has been among the most intriguing and elusive wishful concepts for over 150 years. Maxwell and his followers focused on 'effortless gating' a molecule at a time, but overlooked simultaneous interference of other chaotic molecules, while the demon exorcists tried to justify impossible processes with misplaced 'compensations' by work of measurements and gate operation, and information storage and memory erasure with entropy generation. The illusive and persistent Maxwell's demon fallacies by its advocates, as well as its exorcists, are scrutinized and resolved here. Based on holistic, phenomenological reasoning, it is deduced here that a Maxwell's demon operation, against natural forces and without due work effort to suppress interference of competing thermal particles while one is selectively gated, is not possible at any scale, since it would be against the physics of the chaotic thermal motion, the latter without consistent molecular directional preference for selective timing to be possible. Maxwell's demon would have miraculous useful effects, but also some catastrophic consequences."
"Using classical thermodynamics, we argue that Maxwell's demon loses its battle against Clausius as any temperature difference or other thermodynamic forces it creates is immediately compensated by spontaneous counterbalancing flows that bring about equilibration by slower particles in principle. Being constrained by these spontaneously generated equilibration processes in which he actively but unwittingly participates, the demon is incapable of destroying equilibrium and violating the second law. In fact, our investigation shows that he is unintentionally designed to support it, and does not alter the temperature."
"Photophysical and photochemical dynamics of ground state and excited state proton transfer reaction is reported for Pyrrole 2-Carboxyldehyde (PCL). Steady state absorption and emission measurements are conducted in PCL. The theoretical investigation is done by using different quantum mechanical methods (e.g. Hartree Fock, DFT, MP2, CCSD etc.). The reaction pathway and two dimensional potential energy surfaces are computed in various level of theory. A transition state is also reported in gas phase and reaction filed calculation. It is established that PCL forms different emitting species in different media. A large Stokes shifted emission band, which is attributed to species undergoing excited state intramolecular proton transfer, is observed in hydrocarbon solvent. Intermolecular proton transfer is observed in hydroxylic polar solvent. Experimental observations yield all possible signatures of intramolecular and intermolecular proton transfer in excited state of PCL. The origins of these signatures have been explained successfully using corresponding quantum mechanical theories."
"Laser-induced fluorescence spectra of the $c^3\Sigma^+(v_{c},J_{c}=N_{c})\rightarrow a^3\Sigma^+(v_{a},N_{a} = J_{c} \pm 1)$ transitions excited from the ground $X^1\Sigma^+$ state of $^{39}$K$^{133}$Cs molecule were recorded with Fourier-transform spectrometer IFS125-HR (Bruker) at the highest achievable spectral resolution of 0.0063 cm${}^{-1}$. Systematic study of the hyperfine structure (HFS) of the $a^3\Sigma^+$ state for levels with $v_{a} \in [0, 27]$ and $N_{a} \in [24, 90]$ shows that the splitting monotonically increases with $v_{a}$. The spectroscopic study was supported by ab initio calculations of the magnetic hyperfine interaction in $X^1\Sigma^+$ and $a^3\Sigma^+$ states. The discovered variation of the electronic matrix elements with the internuclear distance $R$ is in a good agreement with the observed $v_{a}$-dependencies of the HFS. Overall set of available experimental data on the $a^3\Sigma^+$ state was used to improve the potential energy curve particularly near a bottom, providing the refined dissociation energy $D_e$=267.21(1) cm${}^{-1}$. The ab initio HFS matrix elements, combined with the empirical $X^1\Sigma^+$ and $a^3\Sigma^+$ PECs in the framework of the invented coupled-channel deperturbation model, reproduce the experimental term values of both ground states within 0.003 cm${}^{-1}$ accuracy up to their common dissociation limit."
"Nonadiabatic transition (NAT) drives a variety of x-ray-induced photochemistry and photophysics used in nature and various fields. To clarify the x-ray-induced NAT dynamics, we performed nonadiabatic molecular dynamics simulations on electronically excited tropone (Tr) dications created by the carbon $KLL$ normal Auger decay. The Tr$^{2+}$ undergoes the NAT cascade via 10-10$^2$ states with time constants of 200-400 fs. We observed population traps in the highly excited states in 100 fs during the NAT cascade. The fingerprint of this population trap can be extracted from C($1s$) edge pump O($1s$) pre-edge probe femtosecond transient x-ray absorption spectra measured by the O($1s$) Auger electron yield method (TR-AEYS) using intense narrow band femtosecond x-ray free electron laser pulses. Our coupled ionization rate equation model demonstrates that selective and saturable C($1s$) core-ionization of Tr realizes background-free measurement. These results indicate that the importance of NAT in x-ray photochemistry and photophysics in large molecules. The real-time tracking of the NAT dynamics using TR-AEYS shall be a powerful approach for deeper insight."
"Relativistic coupled-cluster calculations of the ionization potential, dissociation energy, and excited electronic states under 35,000 cm$^{-1}$ are presented for the actinium monofluoride (AcF) molecule. The ionization potential is calculated to be IP$_e=48,866$ cm$^{-1}$, and the ground state is confirmed to be a closed-shell singlet and thus strongly sensitive to the $\mathcal{T}$,$\mathcal{P}$-violating nuclear Schiff moment of the Ac nucleus. Radiative properties and transition dipole moments from the ground state are identified for several excited states, achieving an uncertainty of $\sim$450 cm$^{-1}$ for the excitation energies. For higher-lying states that are not directly accessible from the ground state, possible two-step excitation pathways are proposed. The calculated branching ratios and Franck-Condon factors are used to investigate the suitability of AcF for direct laser cooling. The lifetime of the metastable $(1)^3\Delta_1$ state, which can be used in experimental searches of the electric dipole moment of the electron, is estimated to be of order 1 ms."
"The extension of laser cooling and trapping methods to polyatomic molecular ions, including those with Ra--N bonds, would have advanced scientific applications such as quantum sensors for fundamental physics, high resolution spectroscopy, and testing predictions of the Standard Model. The essential prerequisite for laser coolability is that molecule is able to scatter hundreds of photons without changing its initial rovibronic state. Thus laser-cooled molecules exhibit a probability of population leak out of the multitude of working vibronic levels (``optical leak'') typically less than 10$^{-2}$. This probability is highly sensitive to small variations of electronic density in the vicinity of its optical cycling center. In the present work we employed the Fock space relativistic coupled cluster approach to obtain information on electronic states and potential energy surfaces of RaNCH$^+$, RaNH$_3^+$ and RaNCCH$_3^+$ molecular ions. Laser coolability of these species was estimated through evaluating Frank-Condon factors, and the peculiarities of unpaired-electron distributions were analyzed to assess coolability from the point of view of the molecular electronic structure. RaNH$_3^+$ and RaNCCH$_3^+$ are the first symmetric top molecular ions found to be promising candidates for direct laser cooling."
The efficiency of $^{99}$Mo nuclei trapping by clinoptilolite particles using Monte Carlo simulation was studied. The simulation showed the carrier particle traps almost all of the incident $^{99}$Mo nuclei for the photon energies 12-18 MeV. The ratio of the $^{99}$Mo nuclei reaching the carrier to the total number of $^{99}$Mo nuclei escaping the nanoparticle is below 1.5% in for the photon energies 12-18 MeV. High specific activity of produced $^{99}$Mo nuclide requires optimization of the clinoptilolite carrier particles concentration in the suspension.
"It is possible to find the optimized radiation dose per session for a radiotherapy (RT) treatment, using a population dynamics model. This has already been done in a previous work for a protocol with 30 sessions and a fixed dose per session. Extending this model to other protocols, with a variable number of sessions, we could change the radiation dosage while keeping the success probability of treatment at its maximum value. This could help the RT oncology service managers to plan the sequence of patients and treatments adapting it to the facilities of the oncology service. Besides, if tumor surrounding tissue is not able to afford a high dosage, it could be useful to extend the treatment to a higher number of low dose radiation sessions, keeping an optimal treatment."
"Space radiation research has progressed rapidly in recent years, but there remain large uncertainties in predicting and extrapolating biological responses to humans. Exposure to cosmic radiation and Solar Particle Events may pose a critical health risk to future spaceflight crews and can have a serious impact to all biomedical aspects of space exploration. The relatively minimal shielding of the cancelled 1960's Manned Orbiting Laboratory program's space vehicle and the high inclination polar orbits would have left the crew susceptible to high exposures of cosmic radiation and high dose-rate SPEs that are mostly unpredictable in frequency and intensity. In this study, we have modeled the nominal and off-nominal radiation environment that a MOL-like spacecraft vehicle would be exposed to during a 30-day mission using high performance, multi-core computers. Projected doses from a historically large SPE (e.g. the August 1972 solar event) have been analyzed in the context of the MOL orbit profile, providing an opportunity to study its impact to crew health and subsequent contingencies.It is reasonable to presume that future commercial, government, and military spaceflight missions in low-Earth orbit will have vehicles with similar shielding and orbital profiles. Studying the impact of cosmic radiation to the mission's operational integrity and the health of MOL crewmembers provides an excellent surrogate and case-study for future commercial and military spaceflight missions."
"Electron bunches with charge densities $\rho$ of the order of $10^2$ to $10^3$ [nC/cm$^3$], energies between $20.$ and $100.$ [MeV], peak current $>100$ [A], bunch lengths between 0.3 and 1.8 [cm], and bunch charge of 2.0 to $20.$ [nC] are relevant to the design of Free Electron Lasers and future linear colliders. In this paper we present the results of numerical simulations performed with a particle in a cell (pic) code of an electron bunch in a drift tube. The electron bunch has cylindrical symmetry with the $z$-axis oriented in the direction of motion. The charge density distribution is constant in the radial and Gaussian in the longitudinal direction, respectively. The electron bunch experiences both a radial pinch in the middle of the pulse, corresponding to the peak electron density, and a significant growth of the correlated emittance. This behavior is explained, and an approximate scaling law is identified. Comparisons of the results from the pic and PARMELA codes are presented."
"Gamma-Ray Bursts (GRBs) are likely to have made a number of significant impacts on the Earth during the last billion years. We have used a two-dimensional atmospheric model to investigate the effects on the Earth's atmosphere of GRBs delivering a range of fluences, at various latitudes, at the equinoxes and solstices, and at different times of day. We have estimated DNA damage levels caused by increased solar UVB radiation, reduction in solar visible light due to $\mathrm{NO_2}$ opacity, and deposition of nitrates through rainout of $\mathrm{HNO_3}$. For the ``typical'' nearest burst in the last billion years, we find globally averaged ozone depletion up to 38%. Localized depletion reaches as much as 74%. Significant global depletion (at least 10%) persists up to about 7 years after the burst. Our results depend strongly on time of year and latitude over which the burst occurs. We find DNA damage of up to 16 times the normal annual global average, well above lethal levels for simple life forms such as phytoplankton. The greatest damage occurs at low to mid latitudes. We find reductions in visible sunlight of a few percent, primarily in the polar regions. Nitrate deposition similar to or slightly greater than that currently caused by lightning is also observed, lasting several years. We discuss how these results support the hypothesis that the Late Ordovician mass extinction may have been initiated by a GRB."
"In this essay we give an overview on the problem of rogue or freak wave formation in the ocean. The matter of the phenomenon is a sporadic occurrence of unexpectedly high waves on the sea surface. These waves cause serious danger for sailing and sea use. A number of huge wave accidents resulted in damages, ship losses and people injuries and deaths are known. Now marine researchers do believe that these waves belong to a specific kind of sea waves, not taken into account by conventional models for sea wind waves. This paper addresses to the nature of the rogue wave problem from the general viewpoint based on the wave process ideas. We start introducing some primitive elements of sea wave physics with the purpose to pave the way for the further discussion. We discuss linear physical mechanisms which are responsible for high wave formation, at first. Then, we proceed with description of different sea conditions, starting from the open deep sea, and approaching the sea cost. Nonlinear effects which are able to cause rogue waves are emphasised. In conclusion we briefly discuss the generality of the physical mechanisms suggested for the rogue wave explanation; they are valid for rogue wave phenomena in other media such as solid matters, superconductors, plasmas and nonlinear optics"
"More often than not a lunch time conversation will veer off into bizarre and uncharted territories. In rare instances these frontiers of conversation can lead to deep insights about the Universe we inhabit. This paper details the fruits of one such conversation. In this paper we will answer the question: How many cows do you need to form a planetoid entirely comprised of cows, which will support a methane atmoosphere produced by the planetary herd? We will not only present the necessary assumptions and theory underpinning the cow-culations, but also present a thorough (and rather robust) discussion of the viability of, and implications for accomplishing, such a feat."
"Criteria for the incipient motion of a rigid body initially resting on a rigid surface are formulated from first principles in this work. A modified Coulomb friction model and an associated distribution of reaction forces are proposed. There exists a surprisingly large category of general motions, however, which subscribe to a more conventional analysis; an analysis made possible by identifying so-called ``significant reaction surfaces''. In this way a model which caters for the majority of combined translations and rotations is devised. Some introductry results demonstrate the accuracy with which fluids can be numerically modelled for the purposes of entrainment. This work is an extension of previous work by the same author."
"In classical tsunami-generation techniques, one neglects the dynamic sea bed displacement resulting from fracturing of a seismic fault. The present study takes into account these dynamic effects. Earth's crust is assumed to be a Kelvin-Voigt material. The seismic source is assumed to be a dislocation in a viscoelastic medium. The fluid motion is described by the classical nonlinear shallow water equations (NSWE) with time-dependent bathymetry. The viscoelastodynamic equations are solved by a finite-element method and the NSWE by a finite-volume scheme. A comparison between static and dynamic tsunami-generation approaches is performed. The results of the numerical computations show differences between the two approaches and the dynamic effects could explain the complicated shapes of tsunami wave trains."
"The last decade has seen the success of stochastic parameterizations in short-term, medium-range and seasonal forecasts: operational weather centers now routinely use stochastic parameterization schemes to better represent model inadequacy and improve the quantification of forecast uncertainty. Developed initially for numerical weather prediction, the inclusion of stochastic parameterizations not only provides better estimates of uncertainty, but it is also extremely promising for reducing longstanding climate biases and relevant for determining the climate response to external forcing. This article highlights recent developments from different research groups which show that the stochastic representation of unresolved processes in the atmosphere, oceans, land surface and cryosphere of comprehensive weather and climate models (a) gives rise to more reliable probabilistic forecasts of weather and climate and (b) reduces systematic model bias. We make a case that the use of mathematically stringent methods for the derivation of stochastic dynamic equations will lead to substantial improvements in our ability to accurately simulate weather and climate at all scales. Recent work in mathematics, statistical mechanics and turbulence is reviewed, its relevance for the climate problem demonstrated, and future research directions outlined."
"We present a discrete element method (DEM) model to simulate the mechanical behavior of sea ice in response to ocean waves. The interaction of ocean waves and sea ice can potentially lead to the fracture and fragmentation of sea ice depending on the wave amplitude and period. The fracture behavior of sea ice is explicitly modeled by a DEM method, where sea ice is modeled by densely packed spherical particles with finite size. These particles are bonded together at their contact points through mechanical bonds that can sustain both tensile and compressive forces and moments. Fracturing can be naturally represented by the sequential breaking of mechanical bonds. For a given amplitude and period of incident ocean wave, the model provides information for the spatial distribution and time evolution of stress and micro-fractures and the fragment size distribution. We demonstrate that the fraction of broken bonds, , increases with increasing wave amplitude. In contrast, the ice fragment size l decreases with increasing amplitude. This information is important for the understanding of breakup of individual ice floes and floe fragment size."
"A class of A.L.E. time discretisations which inherit key energetic properties (nonlinear dissipation in the absence of forcing and long-term stability under conditions of time dependent loading), irrespective of the time increment employed, is established in this work. These properties are intrinsic to real flows and the conventional Navier-Stokes equations.   A description of an incompressible, Newtonian fluid, which reconciles the differences between the various schools of A.L.E. thought in the literature is derived for the purposes of this investigation. The issue of whether these equations automatically inherit the afore mentioned energetic properties must first be resolved. In this way natural notions of nonlinear, exponential-type dissipation in the absence of forcing and long-term stability under conditions of time dependent loading are also formulated.   The findings of this analysis have profound consequences for the use of certain classes of finite difference schemes in the context of deforming references. It is significant that many algorithms presently in use do not automatically inherit the fundamental qualitative features of the dynamics.   The main conclusions are drawn on in the simulation of a driven cavity flow, a driven cavity flow with various, included rigid bodies, a die-swell problem, and a Stokes second order wave. The improved, second order accuracy of a new scheme for the linearised approximation of the convective term is proved for the purposes of these simulations. A somewhat novel method to generate finite element meshes automatically about included rigid bodies, and which involves finite element mappings, is also described."
"An algorithm to compute forces at the sea bed from a finite element solution to the mild slope wave equation is devised in this work. The algorithm is best considered as consisting of two logical parts: The first is concerned with the computation of the derivatives to a finite element solution, given the associated mesh; the second is a bi-quadratic least squares fit which serves to model the sea bed locally in the vicinity of a node. The force at the sea bed can be quantified in terms of either lift and drag, the likes of Stokes' formula or traction. While the latter quantity is the most desireable, the direct computation of tractions at the sea bed is controversial in the context of the mild slope wave equation as a result of the irrotationality implied by the use of potentials. This work ultimately envisages a ``Monte Carlo'' approach using wave induced forces to elucidate presently known heavy mineral placer deposits and, consequently, to predict the existance of other deposits which remain as yet undiscovered."
"We connect an appropriate feedback loop to a model of 2D vertical eddy of airflow which unfolds a wide range of vorticity behavior. Computational fluid dynamics of the twisted roll display a class of long lifespan 3D vortices. On the one hand, the infinitely stable columnar vortex simulated describes waterspouts and tornadoes with extended lifetime. On the other hand, a light modification of the retroaction exhibits strong similarities to tropical cyclones. Moreover, we investigate the outcome of the twisting process vertically shifted. This modelisation leads to the simulation of simultaneous vortices associated to this other class of 3D vortices with short lifespan. Our heuristic dynamical systems lay the foundations of a comprehensive modelisation of vortices since it joins theory and numerical simulations."
"We reconsider the problem of the stability of the thermohaline circulation as described by a two-dimensional Boussinesq model with mixed boundary conditions. We determine how the stability properties of the system depend on the intensity of the hydrological cycle. We define a two-dimensional parameters' space descriptive of the hydrology of the system and determine, by considering suitable quasi-static perturbations, a bounded region where multiple equilibria of the system are realized. We then focus on how the response of the system to finite-amplitude surface freshwater forcings depends on their rate of increase. We show that it is possible to define a robust separation between slow and fast regimes of forcing. Such separation is obtained by singling out an estimate of the critical growth rate for the anomalous forcing, which can be related to the characteristic advective time scale of the system."
"The purpose of this article is numerical verification of the theory of weak turbulence. We performed numerical simulation of an ensemble of nonlinearly interacting free gravity waves (swell) by two different methods: solution of primordial dynamical equations describing potential flow of the ideal fluid with a free surface and, solution of the kinetic Hasselmann equation, describing the wave ensemble in the framework of the theory of weak turbulence.   In both cases we observed effects predicted by this theory: frequency downshift, angular spreading and formation of Zakharov-Filonenko spectrum $I_{\omega} \sim \omega^{-4}$. To achieve quantitative coincidence of the results obtained by different methods, one has to supply the Hasselmann kinetic equation by an empirical dissipation term $S_{diss}$ modeling the coherent effects of white-capping. Using of the standard dissipation terms from operational wave predicting model ({\it WAM}) leads to significant improvement on short times, but not resolve the discrepancy completely, leaving the question about optimal choice of $S_{diss}$ open. In a long run {\it WAM} dissipative terms overestimate dissipation essentially."
The Boussinesq model of convection in a flat layer with heating from below is considered. We analyze the effects of anisotropy caused by rapid rotation in physical and wave spaces and demonstrate the suppression of energy transfer by rotation. We also examine the structure of the wave triangle in nonlinear interaction. The range of parameters is adapted to the models of convection in the geodynamo.
"This paper presents a general formulation of the CIP/multi-moment finite volume method (CIP/MM FVM) for arbitrary order of accuracy. Reconstruction up to arbitrary order can be built on single cell by adding extra derivative moments at the cell boundary. The volume integrated average (VIA) is updated via a flux-form finite volume formulation, whereas the point-based derivative moments are computed as local derivative Riemann problems by either direct interpolation or approximate Riemann solvers."
We present a novel meshless tsunami propagation and inundation model. We discretize the nonlinear shallow-water equations using a well-balanced scheme relying on radial basis function based finite differences. The inundation model relies on radial basis function generated extrapolation from the wet points closest to the wet-dry interface into the dry region. Numerical results against standard one- and two-dimensional benchmarks are presented.
"Non-equilibrium wall turbulence with mean-flow three-dimensionality is ubiquitous in geophysical and engineering flows. Under these conditions, turbulence may experience a counter-intuitive depletion of the turbulent stresses, which has important implications for modelling and control. Yet, current turbulence theories have been established mainly for statistically two-dimensional equilibrium flows and are unable to predict the reduction in the Reynolds stress magnitude. In the present work, we propose a multiscale model which explains the response of non-equilibrium wall-bounded turbulence under the imposition of three-dimensional strain. The analysis is performed via direct numerical simulation of transient three-dimensional turbulent channels subjected to a sudden lateral pressure gradient at friction Reynolds numbers up to 1,000. We show that the flow regimes and scaling properties of the Reynolds stress are consistent with a model comprising momentum-carrying eddies with sizes and time scales proportional to their distance to the wall. We further demonstrate that the reduction in Reynolds stress follows a spatially and temporally self-similar evolution caused by the relative horizontal displacement between the core of the momentum-carrying eddies and the flow layer underneath. Inspection of the flow energetics reveals that this mechanism is associated with lower levels of pressure-strain correlation which ultimately inhibits the generation of Reynolds stress. Finally, we assess the ability of the state-of-the-art wall-modelled large-eddy simulation to predict non-equilibrium, three-dimensional flows."
"The propagation of focused wave groups in intermediate water depth and the shoaling zone is experimentally and numerically considered in this paper. The experiments are carried out in a two-dimensional wave flume and wave trains derived from Pierson-Moskowitz and JONSWAP spectrum are generated. The peak frequency does not change during the wave train propagation for Pierson-Moskowitz waves; however, a downshift of this peak is observed for JONSWAP waves. An energy partitioning is performed in order to track the spatial evolution of energy. Four energy regions are defined for each spectrum type. A nonlinear energy transfer between different spectral regions as the wave train propagates is demonstrated and quantified. Numerical simulations are conducted using a modified Boussinesq model for long waves in shallow waters of varying depth. Experimental results are in satisfactory agreement with numerical predictions, especially in the case of wave trains derived from JONSWAP spectrum."
"Flooding due to Hurricane Florence led to billions of dollars in damage and nearly a hundred deaths in North Carolina. These damages and fatalities can be avoided with proper prevention and preparation. Modelling such flooding events can provide insight and precaution based on principles of fluid dynamics and GIS technology. Using topography and other geographic data from USGS, HEC-RAS can solve the Shallow Water Equations over flooding areas to assist the study of inundation patterns. Simulation results from HEC-RAS agree with observations from NOAA in the flooding area studied. Modeled results from specific locations affected by Hurricane Florence near Neuse River, NC are compared with observations. While overall pattern of inundation is agreeable between model results and observations, there are also differences at very specific locations. Higher resolution topography data and precipitation data over the flooding area may improve the simulation result and reduce the differences."
"For the understanding of planetary and stellar dynamos an overview of the major parameter dependences of convection driven dynamos in rotating spherical fluid shells is desirable. Although the computationally accessible parameter space is limited, earlier work is extended with emphasis on higher Prandtl numbers and uniform heat flux condition at the outer boundary. The transition from dynamos dominated by non-axisymmetric components of the magnetic field to those dominated by the axisymmetric components depends on the magnetic Prandtl number as well as on the ordinary Prandtl number for higher values of the rotation parameter $\tau$. The dependence of the transition on the latter parameter is also discussed. A variety of oscillating dynamos is presented and interpreted in terms of dynamo waves, standing oscillation or modified relaxation oscillations."
"The theory of planetary dynamos and its applications to observed phenomena of planetary magnetism are outlined. It is generally accepted that convection flows driven by thermal or compositional buoyancy are the most likely source for the sustenance of global planetary magnetic fields. While the existence of dynamos in electrically conducting fluid planetary cores provides constraints on properties of the latter, the lack of knowledge about time dependences of the magnetic fields and about their toroidal components together with the restricted parameter regions accessible to theory have prevented so far a full understanding of the phenomena of planetary magnetism."
"Modern science is dependent on imaging on the nanoscale, often achieved through processes that detect secondary electrons created by a highly focused incident charged particle beam. Scanning electron microscopy is employed in applications such as critical-dimension metrology and inspection for semiconductor devices, materials characterization in geology, and examination of biological samples. With its applicability to non-conducting materials (not requiring sample coating before imaging), helium ion microscopy (HIM) is especially useful in the high-resolution imaging of biological samples such as animal organs, tumor cells, and viruses. However, multiple types of measurement noise limit the ultimate trade-off between image quality and the incident particle dose, which can preclude useful imaging of dose-sensitive samples. Existing methods to improve image quality do not fundamentally mitigate the noise sources. Furthermore, barriers to assigning a physically meaningful scale make these modalities qualitative. Here we introduce ion count-aided microscopy (ICAM), which is a quantitative imaging technique that uses statistically principled estimation of the secondary electron yield. With a readily implemented change in data collection, ICAM nearly eliminates the influence of source shot noise -- the random variation in the number of incident ions in a fixed time duration. In HIM, we demonstrate 3x dose reduction; based on a good match between these empirical results and theoretical performance predictions, the dose reduction factor is larger when the secondary electron yield is higher. ICAM thus facilitates imaging of fragile samples and may make imaging with heavier particles more attractive."
"Turbulence is the most common state of astrophysical flows. In typical astrophysical fluids, turbulence is accompanied by strong magnetic fields, which has a large impact on the dynamics of the turbulent cascade. Recently, there has been a significant breakthrough on the theory of magnetohydrodynamic (MHD) turbulence. For the first time we have a scaling model that is supported by both observations and numerical simulations. We review recent progress in studies of both incompressible and compressible turbulence. We compare Iroshnikov-Kraichnan and Goldreich-Sridhar models, and discuss scalings of Alfv\'en, slow, and fast waves. We also discuss the completely new regime of MHD turbulence that happens below the scale at which hydrodynamic turbulent motions are damped by viscosity. In the case of the partially ionized diffuse interstellar gas the viscosity is due to neutrals and truncates the turbulent cascade at $\sim$parsec scales. We show that below this scale magnetic fluctuations with a shallow spectrum persist and discuss the possibility of a resumption of the MHD cascade after ions and neutrals decouple. We discuss the implications of this new insight into MHD turbulence for cosmic ray transport, grain dynamics, etc., and how to test theoretical predictions against observations."
"Negative ions have been detected in abundance in recent years by spacecraft across the solar system. These detections were, however, made by instruments not designed for this purpose and, as such, significant uncertainties remain regarding the prevalence of these unexpected plasma components. In this article, the phenomenon of photodetachment is examined and experimentally and theoretically derived cross-sections are used to calculate photodetachment rates for a range of atomic and molecular negative ions subjected to the solar photon spectrum. These rates are applied to negative ions outflowing from Europa, Enceladus, Titan, Dione and Rhea and their trajectories are traced to constrain source production rates and the extent to which negative ions are able to pervade the surrounding space environments. Predictions are also made for further negative ion populations in the outer solar system with Triton used as an illustrative example. This study demonstrates how, at increased heliocentric distances, negative ions can form stable ambient plasma populations and can be exploited by future missions to the outer solar system."
"We present a falling-sphere viscometer with a magnetized sphere and fluxgate magnetometers continuously measuring the magnetic field produced at the sensor positions by the falling sphere. With a fluid volume of 15 ml and within a few seconds, we directly measure dynamical viscosities in a range between 200 cP and 3000 cP with a precision of 3\%."
"The Gibbs Paradox is essentially a set of open questions as to how sameness of gases or fluids (or masses, more generally) are to be treated in thermodynamics and statistical mechanics. They have a variety of answers, some restricted to quantum theory (there is no classical solution), some to classical theory (the quantum case is different). The solution offered here applies to both in equal measure, and is based on the concept of particle indistinguishability (in the classical case, Gibbs' notion of 'generic phase'). Correctly understood, it is the elimination of sequence position as a labelling device, where sequences enter at the level of the tensor (or Cartesian) product of one-particle state spaces. In both cases it amounts to passing to the quotient space under permutations. 'Distinguishability', in the sense in which it is usually used in classical statistical mechanics, is a mathematically convenient, but physically muddled, fiction."
"We discuss epistemological and methodological aspects of the Bayesian approach in astrophysics and cosmology. The introduction to the Bayesian framework is given for a further discussion concerning the Bayesian inference in physics. The interplay between the modern cosmology, Bayesian statistics, and philosophy of science is presented. We consider paradoxes of confirmation, like Goodman's paradox, appearing in the Bayesian theory of confirmation. As in Goodman's paradox the Bayesian inference is susceptible to some epistemic limitations in the logic of induction. However Goodman's paradox applied to cosmological hypotheses seems to be resolved due to the evolutionary character of cosmology and accumulation new empirical evidences. We argue that the Bayesian framework is useful in the context of falsificability of quantum cosmological models, as well as contemporary dark energy and dark matter problem."
"This is a review of Lee Smolin's ""The Trouble with Physics"". The main gist of the review is that the physics of the past three decades has been rich with new discoveries in a large number of domains. The Standard Model, while providing a successful framework for the electroweak and strong interactions still needs to be developed further to fully account fot the mass of strong interaction data collected over the decades. A multitude of new phenomena have been discovered however, or predicted theoretically, in the physics of very low temperatures (e.g. fractional Hall effect, Bose-Einstein condensation) and quantum macroscopic effects (liquid crystals, high temperature superconductivity), some of which remain to be fully explained. Therefore if there is any ""Trouble with Physics"", as implied by the author of this book, it mainly concerns speculative work on Superstring theory, which may or may not turn out to agree with the results of observation. The subject as a whole does not appear to be in trouble."
"In Europe (e.g., in Italy), and in the States, the opinion is widely spreading that the negative consequences of modern progress are the fault of ""Science"". A lively debate on this topic took place among the famous writer Leonardo Sciascia and Italian physicists like Edoardo Amaldi and Emilio Segre', when Sciascia wrote his known book about the disappearance of Ettore Majorana, a book that presented Majorana (a very reserved theoretical physicist) as an example of the scientist that keeps his discoveries secret when afraid of possible evil applications. We wish to contribute now to such a question, since many special meetings did recently return to it, while celebrating the centenary of Majorana's birth (2006), or 30 years (2005) from Sciascia's book publication. It appeared natural to us to start with the figure of Einstein, who, even if extremely peaceable, supported the atomic bomb construction by his letter to Roosvelt. We discuss first the significance of part of Einsteins's scientific work (which flourished in particular one century ago): We seize this opportunity for presenting a derivation of the ""twins paradox"", so simple that it has been taught to Senior High School last-year students or University undergraduate students; all the present work, more in general, is addressed mainly to them. In the second part of this paper, we analyse the general meaning of Einstein's contributions to philosophy and pedagogy. The third part is entirely devoted to our main subject, i.e., to discussing the ""Responsibility of the Intellectuals"". The reader interested chiefly in this last part, can find it re-expounded in a self-contained way in the Appendix. More information in the Contents of this article."
"If the history of science has taught us anything, it's that persistence and creativity makes the once impossible possible. It has long been thought experimental tests of quantum gravity are impossible. But during the last decade, several different approaches have been proposed that allow us to test, if not the fundamental theory of quantum gravity itself, so at least characteristic features this theory can have. For the first time we can probe experimentally domains in which quantum physics and gravity cohabit, in spite of our failure so far to make a convincing marriage of them on a theoretical level."
"Protonated water clusters are a common species of atmospheric molecular cluster-ion, produced by cosmic rays throughout the troposphere and stratosphere. Under clear-sky conditions or periods of increased atmospheric ionisation, such as solar proton events, the IR absorption by atmospheric ions may affect climate through the radiative balance. Fourier Transform Infrared Spectrometry in a long path cell, of path length 545m, has been used to detect IR absorption by corona-generated positive molecular cluster-ions. The column concentration of ions in the laboratory spectroscopy experiment was estimated to be ~10^13 m-2; the column concentration of protonated atmospheric ions estimated using a simple model is ~10^14 m-2. Two regions of absorption, at 12.3 and 9.1 um are associated with enhanced ion concentrations. After filtering of the measured spectra to compensate for spurious signals from neutral water vapour and residual carbon dioxide, the strongest absorption region is at 9.5 to 8.8 um (1050 to 1140 cm-1) with a fractional change in transmissivity of 0.03 plus/minus 0.015, and the absorption at 12.5 to 12.1 um (800 to 825 cm-1) is 0.015 plus/minus 0.008."
"A mechanism of double strand breaking (DSB) in DNA due to the action of two electrons is considered. These are the electrons produced in the vicinity of DNA molecules due to ionization of water molecules with a consecutive emission of two electrons, making such a mechanism possible. This effect qualitatively solves a puzzle of large yields of DSBs following irradiation of DNA molecules. The transport of secondary electrons, including the additional electrons, is studied in relation to the assessment of radiation damage due to incident ions. This work is a stage in the inclusion of Auger mechanism and like effects into the multiscale approach to ion-beam cancer therapy."
"The ammonia dimer (NH3)2 has been investigated using high--level ab initio quantum chemistry methods and density functional theory (DFT). The structure and energetics of important isomers is obtained to unprecedented accuracy without resorting to experiment. The global minimum of eclipsed C_s symmetry is characterized by a significantly bent hydrogen bond which deviates from linearity by about 20 degrees. In addition, the so-called cyclic C_{2h} structure is extremely close in energy on an overall flat potential energy surface. It is demonstrated that none of the currently available (GGA, meta--GGA, and hybrid) density functionals satisfactorily describe the structure and relative energies of this nonlinear hydrogen bond. We present a novel density functional, HCTH/407+, designed to describe this sort of hydrogen bond quantitatively on the level of the dimer, contrary to e.g. the widely used BLYP functional. This improved functional is employed in Car-Parrinello ab initio molecular dynamics simulations of liquid ammonia to judge its performance in describing the associated liquid. Both the HCTH/407+ and BLYP functionals describe the properties of the liquid well as judged by analysis of radial distribution functions, hydrogen bonding structure and dynamics, translational diffusion, and orientational relaxation processes. It is demonstrated that the solvation shell of the ammonia molecule in the liquid phase is dominated by steric packing effects and not so much by directional hydrogen bonding interactions. In addition, the propensity of ammonia molecules to form bifurcated and multifurcated hydrogen bonds in the liquid phase is found to be negligibly small."
"To quantify the fate of respiratory droplets under different ambient relative humidities, direct numerical simulations of a typical respiratory event are performed. We found that, because small droplets (with initial diameter of 10um) are swept by turbulent eddies in the expelled humid puff, their lifetime gets extended by a factor of more than 30 times as compared to what is suggested by the classical picture by William F. Wells, for 50% relative humidity. With increasing ambient relative humidity the extension of the lifetimes of the small droplets further increases and goes up to around 150 times for 90% relative humidity, implying more than two meters advection range of the respiratory droplets within one second. Employing Lagrangian statistics, we demonstrate that the turbulent humid respiratory puff engulfs the small droplets, leading to many orders of magnitude increase in their lifetimes, implying that they can be transported much further during the respiratory events than the large ones. Our findings provide the starting points for larger parameter studies and may be instructive for developing strategies on optimizing ventilation and indoor humidity control. Such strategies are key in mitigating the COVID-19 pandemic in the present autumn and upcoming winter."
"An organoid is a three-dimensional (3D) in vitro cell culture emulating human organs. We applied 3D dynamic optical coherence tomography (DOCT) to visualize the intratissue and intracellular activities of human induced pluripotent stem cells (hiPSCs)-derived alveolar organoids in normal and fibrosis models. 3D DOCT data were acquired with an 840-nm spectral domain optical coherence tomography with axial and lateral resolutions of 3.8 {\mu}m (in tissue) and 4.9 {\mu}m, respectively. The DOCT images were obtained by the logarithmic-intensity-variance (LIV) algorithm, which is sensitive to the signal fluctuation magnitude. The LIV images revealed cystic structures surrounded by high-LIV borders and mesh-like structures with low LIV. The former may be alveoli with a highly dynamics epithelium, while the latter may be fibroblasts. The LIV images also demonstrated the abnormal repair of the alveolar epithelium."
"Imaging of the structure of single proteins or other biomolecules with atomic resolution would be enormously beneficial to structural biology. X-ray free-electron lasers generate highly intense and ultrashort x-ray pulses, providing a route towards imaging of single molecules with atomic resolution. The information on molecular structure is encoded in the coherent x-ray scattering signal. In contrast to crystallography there are no Bragg reflections in single molecule imaging, which means the coherent scattering is not enhanced. Consequently, a background signal from incoherent scattering deteriorates the quality of the coherent scattering signal. This background signal cannot be easily eliminated because the spectrum of incoherently scattered photons cannot be resolved by usual scattering detectors. We present an ab initio study of incoherent x-ray scattering from individual carbon atoms, including the electronic radiation damage caused by a highly intense x-ray pulse. We find that the coherent scattering pattern suffers from a significant incoherent background signal at high resolution. For high x-ray fluence the background signal becomes even dominating. Finally, based on the atomic scattering patterns, we present an estimation for the average photon count in single molecule imaging at high resolution. By varying the photon energy from 3.5 keV to 15 keV, we find that imaging at higher photon energies may improve the coherent scattering signal quality."
"The authors describe the general motion of radiation-pushed sails accelerated near the speed of light with directed energy propulsion. Practical applications of the model are also given, including the interstellar flyby mission to the Alpha Centauri star system envisioned by the Breakthrough Starshot program. Any misalignment between the driving light beam and the direction of the sail's motion is naturally swept away during acceleration toward relativistic speed, yet leads to a deviation of about 80 A.U. in the case of an initial misalignment of 1 arc sec for a sail accelerated up to 0.2c toward Alpha Centauri. Then, the huge proper acceleration felt by the probes (of order 2500 g), the tremendous energy cost (of about 13 kt per probe) for poor efficiency (of about 3 \%), the trip duration (between 22 and 33 years), the temperature at thermodynamic equilibrium (about 1500 K), and the time dilation aboard (about 160-days difference) are all presented and their variation with the sail's reflectivity is discussed. We also present an application to single trips within the Solar System using double-stage light sails. A spaceship of mass 24 tons can start from Earth and stop at Mars in about seven months with a peak velocity of 30 km/s but at the price of a huge energy cost of about $5.3\times 10^4$ GWh due to extremely low efficiency of the directed energy system, around $10^{-4}$ in this low-velocity case."
"A highly reflective sail provides a way to propel a spacecraft out of the solar system using solar radiation pressure. The closer the spacecraft is to the Sun when it starts its outward journey, the larger the radiation pressure and so the larger the final velocity. For a spacecraft starting on the Earth's orbit, closer proximity can be achieved via a retrograde impulse from a rocket engine. The sail is then deployed at the closest approach to the Sun. Employing the so-called Oberth effect, a second, prograde, impulse at closest approach will raise the final velocity further. Here I investigate how a fixed total impulse ({\Delta}v) can best be distributed in this procedure to maximize the sail's velocity at infinity. Once {\Delta}v exceeds a threshold that depends on the lightness number of the sail (a measure of its sun-induced acceleration), the best strategy is to use all of the {\Delta}v in the retrograde impulse to dive as close as possible to the Sun. Below the threshold the best strategy is to use all of the {\Delta}v in the prograde impulse and thus not to dive at all. Although larger velocities can be achieved with multi-stage impulsive transfers, this study shows some interesting and perhaps counter-intuitive consequences of combining impulses with solar sails."
Background: A solar sail presents a large sheet of low areal density membrane and is the most elegant propellant-less propulsion system for the future exploration of the Solar System and beyond. By today the study on sail membrane deployment strategies has attracted considerable attention.   Goal: In this work we present an idea of the deployment and stretching of the circular solar sail. We consider the superconducting current loop attached to the thin membrane %to develop a new (method) approach of deployment of a solar sail and and predict that a superconducting current loop can deploy and stretch the circular solar sail membrane.   Method: In the framework of a strict mathematical approach based on the classical electrodynamics and theory of elasticity the magnetic field induced by the superconducting current loop and elastic properties of a circular solar sail membrane and wire loop are analyzed. The formulas for the wire and sail membrane stresses and strains caused by the current in the superconducting wire are derived.   Results: The obtained analytical expressions can be applied to a wide range of solar sail sizes. Numerical calculations for the sail of radius of 5 m to 150 m made of CP1 membrane of the thickness of 3.5 $\mu m$ attached to Bi$-$2212 superconducting wire with the cross-section radius of 0.5 mm to 10 mm are presented. Calculations are performed for the engineering current densities of 100 A/mm$^{2}$ to 1000 A/mm$^{2}$.   Conclusion: Our calculations demonstrate the feasibility of the proposed idea for the solar sail deployment for the future exploration of the deep space by means of the light pressure propellant.
"Physicists seeking to understand complex biological systems often find it rewarding to create simple ""toy models"" that reproduce system behavior. Here a toy model is used to understand a puzzling phenomenon from the sport of track and field. Races are almost always won, and records set, in 400 m and 800 m running events by people who run the first half of the race faster than the second half, which is not true of shorter races, nor of longer. There is general agreement that performance in the 400 m and 800 m is limited somehow by the amount of anaerobic metabolism that can be tolerated in the working muscles in the legs. A toy model of anaerobic metabolism is presented, from which an optimal pacing strategy is analytically calculated via the Euler-Lagrange equation. This optimal strategy is then modified to account for the fact that the runner starts the race from rest; this modification is shown to result in the best possible outcome by use of an elementary variational technique that supplements what is found in undergraduate textbooks. The toy model reproduces the pacing strategies of elite 400 m and 800 m runners better than existing models do. The toy model also gives some insight into training strategies that improve performance."
"A novel characterization method to measure the pulse duration of ultrafast near-IR pulses is introduced, which uses simple tabletop optics, is relatively inexpensive, and is expected to work in a broad wavelength range. Our diagnostic tool quantitatively characterizes the laser pulse duration of any near-IR wavelength assuming a Gaussian pulse shape with a linear chirp. We negatively prechirp near-IR pulses with a home-built broadband pulse compressor (BPC) and send this prechirped beam through a cell filled with a low-molar solution of a fluorescent dye in a liquid. After two-photon absorption, this dye fluoresces in the visible, and we record this visible signal as a function of the propagation distance in the liquid cell. We calibrate the group velocity dispersion (GVD) of our home-built BPC device against the known GVD of the compressor of our 800 nm laser and confirm this value using geometric considerations. Now knowing the GVD of BPC and the recorded visible signal for various amounts of negative chirp, let us extract the smallest pulse duration of the near-IR pulse from this visible signal. As a useful corollary, our analysis also enables the direct measurement of the GVD for liquids and the indirect measurement of the absorption coefficient for liquids in the near-IR range, in contrast to indirect GVD measurements that rely on methods such as the double derivative of the refractive index."
"Urban areas are a high-stake target of climate change mitigation and adaptation measures. To understand, predict and improve the energy performance of cities, the scientific community develops numerical models that describe how they interact with the atmosphere through heat and moisture exchanges at all scales. In this review, we present recent advances that are at the origin of last decade's revolution in computer graphics, and recent breakthroughs in statistical physics that extend well established path-integral formulations to non-linear coupled models. We argue that this rare conjunction of scientific advances in mathematics, physics, computer and engineering sciences opens promising avenues for urban climate modeling and illustrate this with coupled heat transfer simulations in complex urban geometries under complex atmospheric conditions. We highlight the potential of these approaches beyond urban climate modeling, for the necessary appropriation of the issues at the heart of the energy transition by societies."
"The present article is the third part of a series of papers devoted to the shallow water wave modelling. In this part, we investigate the derivation of some long wave models on a deformed sphere. We propose first a suitable for our purposes formulation of the full Euler equations on a sphere. Then, by applying the depth-averaging procedure we derive first a new fully nonlinear weakly dispersive base model. After this step, we show how to obtain some weakly nonlinear models on the sphere in the so-called Boussinesq regime. We have to say that the proposed base model contains an additional velocity variable which has to be specified by a closure relation. Physically, it represents a dispersive correction to the velocity vector. So, the main outcome of our article should be rather considered as a whole family of long wave models."
"A dynamical analysis is presented that self-consistently takes into account the motion of the critical layer, in which the magnetic field reconnects, to describe how the m=n=1 resistive internal kink mode develops in the nonlinear regime. The amplitude threshold marking the onset of strong nonlinearities due to a balance between convective and mode coupling terms is identified. We predict quantitatively the early nonlinear growth rate of the m=n=1 mode below this threshold."
"Global surface temperature records (e.g. HadCRUT4) since 1850 are characterized by climatic oscillations synchronous with specific solar, planetary and lunar harmonics superimposed on a background warming modulation. The latter is related to a long millennial solar oscillation and to changes in the chemical composition of the atmosphere (e.g. aerosol and greenhouse gases). However, current general circulation climate models, e.g. the CMIP5 GCMs, to be used in the AR5 IPCC Report in 2013, fail to reconstruct the observed climatic oscillations. As an alternate, an empirical model is proposed that uses: (1) a specific set of decadal, multidecadal, secular and millennial astronomic harmonics to simulate the observed climatic oscillations; (2) a 0.45 attenuation of the GCM ensemble mean simulations to model the anthropogenic and volcano forcing effects. The proposed empirical model outperforms the GCMs by better hind-casting the observed 1850-2012 climatic patterns. It is found that: (1) about 50-60% of the warming observed since 1850 and since 1970 was induced by natural oscillations likely resulting from harmonic astronomical forcings that are not yet included in the GCMs; (2) a 2000-2040 approximately steady projected temperature; (3) a 2000-2100 projected warming ranging between 0.3 $^{o}C$ and 1.6 $^{o}C$, which is significantly lower than the IPCC GCM ensemble mean projected warming of 1.1 $^{o}C$ to 4.1 $^{o}C$; ; (4) an equilibrium climate sensitivity to $CO_{2}$ doubling centered in 1.35 $^{o}C$ and varying between 0.9 $^{o}C$ and 2.0 $^{o}C$."
"We present a proof-of-concept experiment to realize microwave primary power standard with a true-twin microcalorimeter. Double feeding line microcalorimeters are widely used by National Metrology Institutes. A drawback concerns the system calibration: traditional processes changes measurement conditions between system characterization and the measurement stage. Nevertheless, if the feeding lines are made twin, a measurement scheme that avoids separate characterization can be applied, equations simplify and time consumption is halved. Here we demonstrates the feasibility of the idea. The result of an effective efficiency spectroscopy of a thermoelectric power sensor is compared with figures obtained with well established methods."
"The plasma is an ionized gas that responses collectively to any external (or internal) perturbations. Introducing micron-sized solid dust grains into plasma makes it more interesting. The solid grains acquire large negative charges on their surface and exhibits collective behavior similar to the ambient plasma medium. Some remarkable features of the charged dust grain medium (dusty plasma) allow us to use it as a model system to understand some complex phenomena at a microscopic level. In this perspective paper, the author highlights the role of dusty plasma experiments as a learning tool at undergraduate and post-graduate physics programs. The students could have great opportunities to understand some basic physical phenomena as well as to learn many advanced data analysis tools and techniques by performing dusty plasma experiments. How a single dusty plasma experimental device at a physics laboratory can help undergraduate and post-graduate students in the learning process is discussed."
"Intense 3-cycle pulses (10 fs) of 800 nm laser light are utilized to measure energy distributions of ions emitted following Coulomb explosion of Ar$_n$ clusters ($n$=400-900) upon their irradiation by peak intensitis of 5$\times$10$^{14}$ W cm$^{-2}$. The 3-cycle pulses do not afford the cluster sufficient time to undergo Coulomb-driven expansion, resulting in overall dynamics that appear to be very different to those in the many-pulse regime. The peak ion energies are much lower than those obtained when 100 fs pulses of the same intensity are used; they are almost independent of the size of the cluster (over the range 400-900 atoms). Ion yields are a factor of 20 larger in the direction that is perpendicular to the laser polarization vector than along it. This unexpected anisotropy is qualitatively rationalized using molecular dynamics calculations in terms of shielding by an electronic charge cloud within the cluster that is spatially asymmetric."
"Intense, ultrashort pulses of 800 nm laser light (12 fs, $\sim$4 optical cycles) of peak intensity 5$\times$10$^{14}$ W cm$^{-2}$ have been used to irradiate gas-phase Xe$_n$ clusters ($n$=500-25,000) so as to induce multiple ionization and subsequent Coulomb explosion. Energy distributions of exploding ions are measured in the few-cycle domain that does not allow sufficient time for the cluster to undergo Coulomb-driven expansion. This results in overall dynamics that appear to be significantly different to those in the many-cycle regime. One manifestation is that the maximum ion energies are measured to be much lower than those obtained when longer pulses of the same intensity are used. Ion yields are cluster-size independent but polarization dependent in that they are significantly larger when the polarization is perpendicular to the detection axis than along it. This unexpected behavior is qualitatively rationalized in terms of a spatially anisotropic shielding effect induced by the electronic charge cloud within the cluster."
"Laser wakefield acceleration of electrons represents a basis for several types of novel X-ray sources based on Thomson scattering or betatron radiation. The latter provides a high photon flux and a small source size, both being prerequisites for high-quality X-ray imaging. Furthermore, proof-of-principle experiments have demonstrated its application for tomographic imaging. So far this required several hours of acquisition time for a complete tomographic data set. Based on improvements to the laser system, detectors and reconstruction algorithms, we were able to reduce this time for a full tomographic scan to 3 minutes. In this paper, we discuss these results and give a prospect to future imaging systems."
"Radiotherapy using very-high-energy electron (VHEE) beams (50-300 MeV) has attracted considerable attention due to its advantageous dose deposition characteristics, enabling deep penetration and the potential for ultra-high dose rate treatment. One promising approach to compactly delivering these high energy electron beams in a cost-effective manner is laser wakefield acceleration (LWFA), which offers ultra-strong accelerating gradients. However, the transition from this concept to a functional machine intended for tumor treatment is still being investigated. Here we present the first self-developed prototype for LWFA-based VHEE radiotherapy, exhibiting high compactness (occupying less than 5 square meters) and high operational stability (validated over a period of one month). Subsequently, we employed this device to irradiate a tumor implanted in a mouse model. Following a dose delivery of $5.8\pm0.2$ Gy with precise tumor conformity, all irradiated mice exhibited pronounced control of tumor growth. For comparison, this tumor-control efficacy was similar to that achieved using commercial X-ray radiotherapy equipment operating at equivalent doses. These results demonstrate the potential of a compact laser-driven VHEE system for preclinical studies involving small animal models and its promising prospects for future clinical translation in cancer therapy."
"The force exerted on nanoparticles and atomic clusters by fast passing electrons like those employed in transmission electron microscopes are calculated and integrated over time to yield the momentum transferred from the electrons to the particles. Numerical results are offered for metallic and dielectric particles of different sizes (0-500 nm in diameter) as well as for carbon nanoclusters. Results for both linear and angular momentum transfers are presented. For the electron beam currents commonly employed in electron microscopes, the time-averaged forces are shown to be comparable in magnitude to laser-induced forces in optical tweezers. This opens up the possibility to study optically-trapped particles inside transmission electron microscopes."
"How long does it take to fall down a tunnel through the center of the Earth to the other side? Assuming a uniformly dense Earth, it would take 42 minutes, but this assumption has not been validated. This paper examines the gravity tunnel without this restriction, using the internal structure of the Earth as ascertained by seismic data, and the dynamics are solved numerically. The time taken to fall along the diameter is found to be 38 rather than 42 minutes. The time taken to fall along a straight line between any two points is no longer independent of distance, but interpolates between 42 minutes for short trips and 38 minutes for long trips. The brachistochrone path (minimizing the fall time between any two points) is similar to the uniform density solution, but tends to reach a greater maximum depth and takes less time to traverse. Although the assumption of uniform density works well in many cases, the simpler assumption of a constant gravitational field serves as a better approximation to the true results."
"Since the last decades, there have been numerous reports about the interaction of magnetic field (MF) and cold atmospheric plasma (CAP) with the biological systems, separately. In this manuscript, we have investigated the combined effect of CAP with the static magnetic field (SMF) as an effective method for cancer cells treatment. MDA-MB-231 breast cancer cells were cultured and treated with CAP in different input power and different exposure time in the presence and absence of the SMF. Vitamin C is used in medium, and cell viability is investigated in the presence and absence of this antioxidant compound. The MTT assay has been employed to measure cell survival, and then T-test and one-way ANOVA are used to assess the significance level of quantitative data. In order to determine the migration rate of cancer cells, wound healing assay has been carried out. Results show that presence of the SMF and vitamin C as well as increasing the input power has a significant role on the attenuation of the survival and migration rate of the cells. The results of the present investigation will greatly contribute to improve the CAP efficiency in cancer therapy through using the SMF and vitamin C as a complement to conventional CAP therapies."
"Hydrogen-bonded mixtures with varying concentration are a complicated networked system that demands a detection technique with both time and frequency resolutions. Hydrogen-bonded pyridine-water mixtures are studied by a time-frequency resolved coherent Raman spectroscopic technique. Femtosecond broadband dual-pulse excitation and delayed picosecond probing provide sub-picosecond time resolution in the mixtures temporal evolution. For different pyridine concentrations in water, asymmetric blue versus red shifts (relative to pure pyridine spectral peaks) were observed by simultaneously recording both the coherent anti-Stokes and Stokes Raman spectra. Macroscopic coherence dephasing times for the perturbed pyridine ring modes were observed in ranges of 0.9 - 2.6 picoseconds for both 18 and 10 cm-1 broad probe pulses. For high pyridine concentrations in water, an additional spectral broadening (or escalated dephasing) for a triangular ring vibrational mode was observed. This can be understood as a result of ultrafast collective emissions from coherently excited ensemble of pairs of pyridine molecules bound to water molecules."
"We theoretically analyze the effect of density/refractive-index gradients on the measurement precision of Volumetric Particle Tracking Velocimetry (V-PTV) and Background Oriented Schlieren (BOS) experiments by deriving the Cramer-Rao lower bound (CRLB) for the 2D centroid estimation process. A model is derived for the diffraction limited image of a particle or dot viewed through a medium containing density gradients that includes the effect of various experimental parameters such as the particle depth, viewing direction and f-number. Using the model we show that non-linearities in the density gradient field lead to blurring of the particle/dot image. This blurring amplifies the effect of image noise on the centroid estimation process, leading to an increase in the CRLB and a decrease in the measurement precision. The ratio of position uncertainties of a dot in the reference and gradient images is a function of the ratio of the dot diameters and dot intensities. We term this parameter the Amplification Ratio (AR), and we propose a methodology for estimating the position uncertainties in tracking-based BOS measurements. The theoretical predictions of the particle/dot position estimation variance from the CRLB are compared to ray tracing simulations with good agreement. The uncertainty amplification is also demonstrated on experimental BOS images of flow induced by a spark discharge, where we show that regions of high amplification ratio correspond to regions of density gradients. This analysis elucidates the dependence of the position error on density and refractive-index gradient induced distortion parameters, provides a methodology for accounting its effect on uncertainty quantification and provides a framework for optimizing experiment design."
"Direct monitoring of singlet oxygen (1O2) luminescence is a particularly challenging infrared photodetection problem. 1O2, an excited state of the oxygen molecule, is a crucial intermediate in many biological processes. We employ a low noise superconducting nanowire single-photon detector to record 1O2 luminescence at 1270 nm wavelength from a model photosensitizer (Rose Bengal) in solution. Narrow band spectral filtering and chemical quenching is used to verify the 1O2 signal, and lifetime evolution with the addition of protein is studied. Furthermore, we demonstrate the detection of 1O2 luminescence through a single optical fiber, a marked advance for dose monitoring in clinical treatments such as photodynamic therapy."
"Recent advances in the high sensitivity spectroscopy have made it possible, in combination with accurate theoretical predictions, to observe for the first time very weak electric quadrupole transitions in a polar polyatomic molecule of water. Here we present accurate theoretical predictions of the complete quadrupole ro-vibrational spectrum of a non-polar molecule CO$_2$, important in atmospheric and astrophysical applications. Our predictions are validated by recent cavity enhanced absorption spectroscopy measurements and are used to assign few weak features in the recent ExoMars ACS MIR spectroscopic observations of the martian atmosphere. Predicted quadrupole transitions appear in some of the mid-infrared CO$_2$ and water vapor transparency regions, making them important for detection and characterization of the minor absorbers in water- and CO$_2$-rich environments, such as present in the atmospheres of Earth, Venus and Mars."
"Particle accelerators as photon sources are advanced tools in studying the structure and dynamical properties of matter. The present workhorses of these sources are storage ring-based synchrotron radiation facilities and linear accelerator-based free-electron lasers, delivering light with high repetition rate and high peak brilliance (power), respectively. The steady-state microbunching (SSMB) mechanism was proposed to bridge the gap of these two kinds of sources to generate high-average-power, high-repetition-rate coherent radiation in an electron storage ring. Such a novel light source promises new possibilities for accelerator photon science and industry applications, for example in ultra-high-energy-resolution angle-resolved photoemission spectroscopy and extreme ultraviolet lithography. The six orders of magnitude extrapolation of the electron bunch length in an SSMB storage ring compared to that of a conventional ring provides tremendous opportunities for accelerator physics research.   This dissertation is devoted to the theoretical and experimental investigations of SSMB, with important results achieved. The work presented can be summarized as: first, how to realize SSMB; second, what radiation characteristics can we obtain from the formed SSMB; and third, experimentally demonstrate the working mechanism of SSMB in a real machine."
"In microdosimetry, lineal energies y are calculated from energy depositions $\epsilon$ inside the microdosimeter divided by the mean chord length, whose value is based on geometrical assumptions on both the detector and the radiation field. This work presents an innovative two-stages hybrid detector (HDM: hybrid detector for microdosimetry) composed by a Tissue Equivalent Proportional Counter (TEPC) and a silicon tracker made of 4 Low Gain Avalanche Diode (LGAD). This design provides a direct measurement of energy deposition in tissue as well as particles tracking with a submillimeter spatial resolution. The data collected by the detector allow to obtain the real track length traversed by each particle in the TEPC and thus estimates microdosimetry spectra without the mean chord length approximation. Using Geant4 toolkit, we investigated HDM performances in terms of detection and tracking efficiencies when placed in water and exposed to protons and carbon ions in the therapeutic energy range. The results indicate that the mean chord length approximation underestimate particles with short track, which often are characterized by a high energy deposition and thus can be biologically relevant. Tracking efficiency depends on the LGAD configurations: 34 strips sensors have a higher detection efficiency but lower spatial resolution than 71 strips sensors. Further studies will be performed both with Geant4 and experimentally to optimize the detector design on the bases of the radiation field of interest. The main purpose of HDM is to improve the assessment of the radiation biological effectiveness via microdosimetric measurements, exploiting a new definition of the lineal energy ($y_{T}$), defined as the energy deposition $\epsilon$ inside the microdosimeter divided by the real track length of the particle."
"New condition Re>Re_th_min=124 of linear (exponential) instability of the Hagen-Poisseuille (HP) with respect to extremely small by magnitude axially-symmetric disturbances of the tangential component of the velocity field is obtained. For this, disturbances must necessarily have quasi-periodic longitude variability (not representable as a Fourier series or integral) along the pipe axis that complies with experimental data and differs from the usually considered idealized case of pure periodic disturbances for which HP flow is stable for arbitrary large Reynolds numbers Re. Obtained minimal threshold Reynolds number is related to the spatial structure of disturbances (having two radial modes with non-commensurable longitudinal periods) in which irrational value p=1.58 of the ratio of the two longitudinal periods is close to the value of the ""golden ratio"" equal to 1.618."
"We present a new cavity-based polarimetric scheme for highly sensitive and time-resolved measurements of birefringence and dichroism, linear and circular, that employs rapidly-pulsed single-frequency CW laser sources and extends current cavity-based spectropolarimetric techniques. We demonstrate how the use of a CW laser source allows for gains in spectral resolution, signal intensity and data acquisition rate compared to traditional pulsed-based cavity ring-down polarimetry (CRDP). We discuss a particular CW-CRDP modality that is different from intensity-based cavity-enhanced polarimetric schemes as it relies on the determination of the polarization-rotation frequency during a ring-down event generated by large intracavity polarization anisotropies. We present the principles of CW-CRDP and validate the applicability of this technique for measurement of the non-resonant Faraday effect in solid SiO$_2$ and CeF$_3$ and gaseous butane. We give a general analysis of the fundamental sensitivity limits for CRDP techniques and show how the presented frequency-based methodology alleviates the requirement for high finesse cavities to achieve high polarimetric sensitivities, and, thus, allows for the extension of cavity-based polarimetric schemes into different spectral regimes but most importantly renders the CW-CRDP methodology particularly suitable for robust portable polarimetric instrumentations."
"The distribution of electric current on an electrode surface in electrolyte varies with time due to charge accumulation at a capacitive interface, as well as due to electrode kinetics and concentration polarization in the medium. Initially, the potential at the electrode-electrolyte interface is uniform, resulting in a non-uniform current distribution due to the uneven ohmic drop of the potential in the medium. Over time, however, the non-uniform current density causes spatially varying rate of the charge accumulation at the interface, breaking down its equipotentiality. We developed an analytical model to describe such transition at a capacitive interface when the current is below the mass-transfer limitation, and demonstrated that the steady distribution of the current is achieved when the current density is proportional to the capacitance per unit area, which leads to linear voltage ramp at the electrode. More specific results regarding the dynamics of this transition are provided for a disk electrode, along with an experimental validation of the theoretical result. These findings are important for many electrochemical applications, and in particular, for proper design of the electro-neural interfaces."
"Theory and computations are provided for building of optimal (minimum weight) solid space towers (mast) up to one hundred kilometers in height. These towers can be used for tourism; scientific observation of space, observation of the Earth surface, weather and upper atmosphere experiment, and for radio, television, and communication transmissions. These towers can also be used to launch spaceships and Earth satellites.   These macroprojects are not expensive. They require strong hard material (steel). Towers can be built using present technology. Towers can be used (for tourism, communication, etc.) during the construction process and provide self-financing for further construction. The tower design does not require human work at high altitudes; the tower is separated into sections; all construction can be done at the Earth surface.   The transport system for a tower consists of a small engine (used only for friction compensation) located at the Earth surface.   Problems involving security, control, repair, and stability of the proposed towers are addressed in other cited publications."
"In recent years, industry has produced high-temperature fiber and whiskers. The author examined the atmospheric reentry of the USA Space Shuttles and proposed the use of high temperature tolerant parachute for atmospheric air braking. Though it is not large, a light parachute decreases Shuttle speed from 8 km/s to 1 km/s and Shuttle heat flow by 3 - 4 times. The parachute surface is opened with backside so that it can emit the heat radiation efficiently to Earth-atmosphere. The temperature of parachute is about 1000 - 1300 C. The carbon fiber is able to keep its functionality up to a temperature of 1500 - 2000 C. There is no conceivable problem to manufacture the parachute from carbon fiber. The proposed new method of braking may be applied to the old Space Shuttles as well as to newer spacecraft designs."
"A hybrid approach for the computation of performance limitations for implanted antennas is presented, taking radiation efficiency as the optimized metric. Ohmic loss in the antenna and surrounding tissue are both considered. The full-wave interaction of all parts of the system is taken into account. The method is thoroughly tested on a realistic implanted antenna design that is treated both experimentally and as a model in a commercial electromagnetic solver. A great agreement is reported. In addition, the fundamental bounds on radiation efficiency are compared to the performance of a loop and a dipole antenna showing the importance of various loss mechanisms during the designs. The trade-off between tissue loss and antenna ohmic loss indicates critical points in which the optimal solution drastically changes and in which the real designs should change their topology."
"We determine the Lorentz transformations and the kinematic content and dynamical framework of special relativity as purely an extension of Galileo's thoughts. No reference to light is ever required: The theories of relativity are logically independent of any properties of light. The thoughts of Galileo are fully realized in a system of Lorentz transformations with a parameter 1/c^2, some undetermined, universal constant of nature; and are realizable in no other. Isotropy of space plays a deep and pivotal role in all of this, since here three-dimensional space appears at first blush, and persists until the conclusion: Relativity can never correctly be fully developed in just one spatial dimension."
"According to Karl Popper assumptions are statements used to construct theories. During the construction of a theory whether the assumptions are either true or false turn out to be irrelevant in view of the fact that, actually, they gain their scientific value when the deductions derived from them suffice to explain observations. Science is enriched with assumptions of all kinds and physics is not exempted. Beyond doubt, some assumptions have been greatly beneficial for physics. They are usually embraced based on the kind of problems expected to be solved in a given moment of a science. Some have been quite useful and some others are discarded in a given moment and reconsidered in a later one. An illustrative example of this is the conception of light; first, according to Newton, as particle; then, according to Huygens, as wave; and then, again, according to Einstein, as particle. Likewise, once, according to Newton, a preferred system of reference (PSR) was assumed; then, according to Einstein, rejected; and then, here the assumption is reconsidered. It is claimed that the assumption that there is no PSR can be fundamentally wrong."
"The International System of Units (SI) is fundamental for the social, and not only the scientific, role of metrology, and as such its understandability is a crucial issue. According to the current draft of the new SI Brochure, the next edition of the SI will be significantly more complex in its conceptual structure than the previous ones. Identifying a strategy for effectively communicating its main contents is then a worthwhile endeavor, in order to increase the acceptance and thus the sustainability of the SI itself. Our proposal is to focus on the semantic structure of the definitions: this is instrumental to the awareness campaigns recommended by the General Conference on Weights and Measures to make the next edition of the SI understandable by a diverse readership without compromising scientific rigor."
"I show how Hamilton's philosophical commitments led him to a causal interpretation of classical mechanics. I argue that Hamilton's metaphysics of causation was injected into his dynamics by way of a causal interpretation of force. I then detail how forces remain indispensable to both Hamilton's formulation of classical mechanics and what we now call Hamiltonian mechanics (i.e., the modern formulation). On this point, my efforts primarily consist of showing that the orthodox interpretation of potential energy is the interpretation found in Hamilton's work. Hamilton called the potential energy function the force-function because he believed that it represents forces at work in the world. Multifarious non-historical arguments for this orthodox interpretation of potential energy are provided, and matters are concluded by showing that in classical Hamiltonian mechanics, facts about potential energies of systems are grounded in facts about forces. Thus, if one can tolerate the view that forces are causes of motions, then Hamilton provides one with a road map for transporting causation into one of the most mathematically sophisticated formulations of classical mechanics, viz., Hamiltonian mechanics."
"Water wave propagation can be attenuated by various physical mechanisms. One of the main sources of wave energy dissipation lies in boundary layers. The present work is entirely devoted to thorough analysis of the dispersion relation of the novel visco-potential formulation. Namely, in this study we relax all assumptions of the weak dependence of the wave frequency on time. As a result, we have to deal with complex integro-differential equations that describe transient behaviour of the phase and group velocities. Using numerical computations, we show several snapshots of these important quantities at different times as functions of the wave number. Good qualitative agreement with previous study [Dutykh2009] is obtained. Thus, we validate in some sense approximations made anteriorly. There is an unexpected conclusion of this study. According to our computations, the bottom boundary layer creates disintegrating modes in the group velocity. In the same time, the imaginary part of the phase velocity remains negative for all times. This result can be interpreted as a new kind of instability which is induced by the bottom boundary layer effect."
"The classical dam break problem has become the de facto standard in validating the Nonlinear Shallow Water Equations (NSWE) solvers. Moreover, the NSWE are widely used for flooding simulations. While applied mathematics community is essentially focused on developing new numerical schemes, we tried to examine the validity of the mathematical model under consideration. The main purpose of this study is to check the pertinence of the NSWE for flooding processes. From the mathematical point of view, the answer is not obvious since all derivation procedures assumes the total water depth positivity. We performed a comparison between the two-fluid Navier-Stokes simulations and the NSWE solved analytically and numerically. Several conclusions are drawn out and perspectives for future research are outlined."
"In this paper we exploit two equivalent formulations of the average rate of material entropy production in a planetary system to propose an approximate splitting between contributions due to vertical and eminently horizontal processes. Our approach is based only upon 2D radiative fields at the surface and at the top of atmosphere of a general planetary body. Using 2D fields at the top of atmosphere alone, we derive lower bounds to the rate of material entropy production and to the intensity of the Lorenz energy cycle. By introducing a measure of the efficiency of the planetary system with respect to horizontal thermodynamical processes, we provide insight on a previous intuition on the possibility of defining a baroclinic heat engine extracting work from the meridional heat flux. The approximate formula of the material entropy production is verified and used for studying the global thermodynamic properties of climate models (CMs) included in the PCMDI/CMIP3 dataset in pre-industrial climate conditions. It is found that about 90% of the material entropy production is due to vertical processes such as convection, whereas the large scale meridional heat transport contributes only about 10%. The total material entropy production is typically 55 mWK-1m-2, with discrepancies of the order of 5% and CMs' baroclinic efficiencies are clustered around 0.055. When looking at the variability and co-variability of the considered thermodynamical quantities, the agreement among CMs is worse, suggesting that the description of feedbacks is more uncertain."
"We discuss a less known aspect of Feynman's multifaceted scientific work, centered about his interest in molecular biology, which came out around 1959 and lasted for several years. After a quick historical reconstruction about the birth of molecular biology, we focus on Feynman's work on genetics with Robert S. Edgar in the laboratory of Max Delbruck, which was later quoted by Francis Crick and others in relevant papers, as well as in Feynman's lectures given at the Hughes Aircraft Company on biology, organic chemistry and microbiology, whose notes taken by the attendee John Neer are available. An intriguing perspective comes out about one of the most interesting scientists of the XX century."
"Sound cards, which count as standard equipment in today's computers, can be turned into measurement tools, making experimentation very efficient and cheap. The chief difficulties to overcome are the lack of proper hardware interfacing and processing software. Sound-card experimentation becomes really viable only if we demonstrate how to connect different sensors to the sound card and provide suitable open-source software to support the experiments. In our talk, we shall present a few applications of sound cards in measurements: photogates, stopwatches and an example of temperature measurement and registration. We also provide the software for these applications."
"Ubiquitously during experiments one encounters a situation where time lapse between two events has to measured. For example during the oscillations of a pendulum or a vibrating reed, the powering of a lamp and achieving of its full intensity. The powering of a relay and the closure of its contacts etc. Situations like these call for a time measuring device between two events. Hence this article describes a general Bi-Event timer that can be used in a physics lab for ubiquitous time lapse measurements during experiments. These measurements in turn can be used to interpret other parameters like velocity, acceleration etc. The timer described here is simple to build and accurate in performance. The Bi-event occurence can be applied as a signal to the inputs of the timer either on separate lines or along a single path in series as voltage pulses."
"We describe a hands-on accurate demonstrator for cosmic rays realized by six high school students, whose main aim is to show the relevance and the functioning of the principal parts of a cosmic rays telescope (muon detector), with the help of two large size wooden artifacts. The first one points out how cosmic rays can be tracked in a muon telescope, while the other one shows the key avalanche process of electronic ionization that effectively allows muon detection through a photomultiplier. Incoming cosmic rays are visualized in terms of laser beams, whose 3D trajectory is highlighted by the turning on of LEDs on two orthogonal matrices. Instead the avalanche ionization process is demonstrated through the avalanche falling of glass marbles on an inclined plane, finally turning on a LED. A pictured poster accompanying the demonstrator is as well effective in assisting cosmic rays demonstration and its detection. The success of the demonstrator has been fully proven by general public during a Science Festival, the corresponding project winning the Honorable Mention in a dedicated competition."
"A very low-cost, easy-to-make stopwatch is presented to support various experiments in mechanics. The high-resolution stopwatch is based on two photodetectors connected directly to the microphone input of the sound card. A dedicated free open-source software has been developed and made available to download. The efficiency is demonstrated by a free fall experiment."
"This is a very gentle introductory course on quantum mechanics aimed at the first years of the undergraduate level. The basic concepts are introduced, with many applications and illustrations. Contains 12 short chapters of equal length, ideal for a one term course. The license allows reuse of figures and text under the Attribution-Noncommercial-ShareAlike conditions."
A rational approach to identify polyatomic molecules that appear to be promising candidates for direct Doppler cooling with lasers is outlined. First-principle calculations for equilibrium structures and Franck--Condon factors of selected representatives with different point-group symmetries (including chiral non-symmetric group C$_1$) have been performed and high potential for laser-cooling of these molecules is indicated.
"Four-dimensional imaging of charge migration is crucial to the understanding of several ubiquitous processes in nature. The present work focuses on imaging of charge migration in an oriented epoxypropane: a chiral molecule. A linearly polarized pulse is used to induce the charge migration, which is imaged by time-resolved x-ray diffraction. It is found that the total time-resolved diffraction signals are significantly different for both enantiomers. Furthermore, a connection between time-resolved x-ray diffraction and the electronic continuity equation is discussed by analyzing the time-dependent diffraction signal and the time derivative of the total electron density in the momentum space."
"High-harmonic spectroscopy (HHS) is a nonlinear all-optical technique with inherent attosecond temporal resolution, which has been applied successfully to a broad variety of systems in the gas phase and solid state. Here, we extend HHS to the liquid phase, and uncover the mechanism of high-harmonic generation (HHG) for this phase of matter. Studying HHG over a broad range of wavelengths and intensities, we show that the cut-off ($E_c$) is independent of the wavelength beyond a threshold intensity, and find that $E_c$ is a characteristic property of the studied liquid. We explain these observations within an intuitive semi-classical model based on electron trajectories that are limited by scattering to a characteristic length, which is connected to the electron mean-free path. Our model is validated against rigorous multi-electron time-dependent density-functional theory calculations in, both, supercells of liquid water with periodic boundary conditions, and large clusters of a variety of liquids. These simulations confirm our interpretation and thereby clarify the mechanism of HHG in liquids. Our results demonstrate a new, all-optical access to effective mean-free paths of slow electrons ($\leq$10 eV) in liquids, in a regime that is inaccessible to accurate calculations, but is critical for the understanding of radiation damage to living tissue. Our work also establishes the possibility of resolving sub-femtosecond electron dynamics in liquids, which offers a novel, all-optical approach to attosecond spectroscopy of chemical processes in their native liquid environment."
"High-harmonic generation (HHG) in liquids is opening new opportunities for attosecond light sources and attosecond time-resolved studies of dynamics in the liquid phase. In gas-phase HHG, few-cycle pulses are routinely used to create isolated attosecond pulses and to extend the cut-off energy. Here, we study the properties of HHG in liquids, including water and several alcohols, by continuously tuning the pulse duration of a mid-infrared driver from the multi- to the sub-two-cycle regime. Similar to the gas phase, we observe the transition from discrete odd-order harmonics to continuous extreme-ultraviolet emission. However, the cut-off energy is shown to be entirely independent of the pulse duration. This observation is confirmed by ab-initio simulations of HHG in large clusters. Our results support the notion that the cut-off energy is a fundamental property of the liquid, independent of the driving-pulse properties. Combined with the recently reported wavelength-independence of the cutoff, these results confirm the direct sensitivity of HHG to the mean-free paths of slow electrons in liquids. Our results additionally imply that few-cycle mid-infrared laser pulses are suitable drivers for generating isolated attosecond pulses from liquids."
"The interaction of molecules with the orbital angular momentum of light has long been argued to benefit structural studies and quantum control of molecular ensembles. We derive a general description of the light-matter interaction in terms of the coupling between spherical gradients of the electric field and an effective molecular charge density that exactly reproduces molecular multipole moments. Our model can accommodate for an arbitrary complexity of the molecular structure and is applicable to any electric field, with the exception of tightly focused beams. Within this framework, we derive the general mechanism of angular momentum exchange between the spin and orbital angular momenta of light, molecular rotation and its center-of-mass motion. We demonstrate that vortex beams strongly enhance certain ro-vibrational transitions that are considered forbidden in the case of a non-helical light. Finally, we discuss the experimental requirements for the observation of novel transitions in state-of-the-art spatially-resolved spectroscopy measurements."
"We identified and computed the horizontal wavelengths of atmospheric gravity waves in clouds using a visible camera installed on a window of the Columbus module of the International Space Station (ISS) and controlled by a Raspberry Pi computer. The experiment was designed in the context of the Astro Pi challenge, a project run by ESA in collaboration with the Raspberry Pi Foundation, where students are allowed the opportunity to write a code to be executed at the ISS. A code was developed to maximize the probability of capturing images of clouds while the ISS is orbiting the Earth. Several constraints had to be fulfilled such as the experiment duration limit (3 hours) and the maximum data size (3 gigabytes). After receiving the data from the ISS, small-scale gravity waves were observed in different regions in the northern hemisphere with horizontal wavelengths in the range of 1.0 to 4.7 km."
"The impossibility of experiencing the molecular world with our senses hampers teaching and understanding chemistry because very abstract concepts (such as atoms, chemical bonds, molecular structure, reactivity) are required for this process. Virtual reality, especially when based on explicit physical modeling (potentially in real time), offers a solution to this dilemma. Chemistry teaching can make use of advanced technologies such as virtual-reality frameworks and haptic devices. We show how an immersive learning setting could be applied to help students understand the core concepts of typical chemical reactions by offering a much more intuitive approach than traditional learning settings. Our setting relies on an interactive exploration and manipulation of a chemical system; this system is simulated in real-time with quantum chemical methods, and therefore, behaves in a physically meaningful way."
"In this final report, I briefly reflect on two parallel teaching experiences as tutor. Besides, I briefly view such experiences in interaction with my research work, private life and new teaching position. In harmony with my conception of teaching, I avoid the standard formal style of reports and try an interactive dialogue with the reader."
"Modern physics is largely defined by fundamental symmetry principles and Noether's Theorem. Yet these are not taught, or rarely mentioned, to beginning students, thus missing an opportunity to reveal that the subject of physics is as lively and contemporary as molecular biology, and as beautiful as the arts. We prescribe a symmetry module to insert into the curriculum, of a week's length."
"During the past several years the authors have developed a new approach to the teaching of Physical Science, a general education course typically found in the curricula of nearly every college and university. This approach, called `Physics in Films', uses scenes from popular movies to illustrate physical principles and has excited student interest and improved student performance.   The analyses of many of the scenes in `Physics in Films' are a direct application of Fermi calculations -- estimates and approximations designed to make solutions of complex and seemingly intractable problems understandable to the student non-specialist. The intent of this paper is to provide instructors with examples they can use to develop skill in recognizing Fermi problems and making Fermi calculations in their own courses."
"We describe an initiative at the University of Bonn, where the students develop and perform a 2 hour physics show for school classes and the general public. The show is entertaining and educational and is aimed at children aged 10 and older. For the physics students this is a unique experience to apply their knowledge at an early stage and gives them the chance to develop skills in the public presentation of science, in front of 520 people per show. We have extended the activity to put on an elementary particle physics show for teenagers. Furthermore, local high schools have picked up the idea; their students put on similar shows for fellow students and parents. We would be interested in hearing about related activities elsewhere."
"In 2004, Tohoku University created a new introductory science experimental course for freshmen. The course is a compulsory subject for students in all natural science fields. The course is not designed for a professional education, but as a liberal education, in which students are trained to become familiar with nature and to discover natural laws for themselves. We present here one of 12 themes - ""science and culture: vibration of string instrument and music"", in which we expect students to study two aspects: 1) the universality of natural laws and 2) the variety of value judgments from the evidence."
"Since 2006, the Pigelleto's Summer School of Physics is an important appointment for orienting students toward physics. It is organized as a full immersion school on actual topics in physics or in fields rarely pursued in high school, i.e. quantum mechanics, new materials, energy resources. The students, usually forty, are engaged in many activities in laboratory and forced to become active participants. Furthermore, they are encouraged in cooperating in small groups in order to present and share the achieved results. In the last years, the school became a training opportunity for younger teachers which are involved in programming and realization of selected activities. The laboratory activities with students are usually supervised by a young and an expert teacher in order to fix the correct methodology."
"Physics is introduced as a basic matter in the curricula of professional schools (i.e. schools for agriculture, electronic or chemistry experts). Students meet physics in the early years of their training and then continue in vocational subjects where many physics' topics can be useful. Rarely, however, the connection between physics and professional matters is quite explicit. Students often feel physics as boring and useless, i.e. very far from their interests. In these schools it is almost always required the physics lab, but it does not always exist. The physics teachers of a local Agricultural Technical Institute asked us to realize a learning path in laboratory for their students, since in their school the physics lab was missing. This institute is the only public school in the Chianti area specializing in Viticulture and Enology, and attending a further year post diploma, allows the achievement of the qualification of Enologist. We report a learning path realized starting from thermal equilibrium to a full understanding of the measures made with the Malligand's ebulliometer, used for determining the alcoholic strength (alcohol concentration by volume) of an alcoholic beverage and water/alcohol solutions in general. The aim was to make interesting measures of physical quantities, calorimetry and state transitions connecting them to the functioning of an instrument that students use in their professional career. The feedback of students and the interests of their teachers convinced us to go further in this way. We intend in the next future to involve teachers of physics and vocational subjects in the design of a physics curriculum spread over two years in which the main physics topics will be introduced to explain the functioning of tools and equipment used, normally, in the winery."
"A scientifically literate society is important for many different reasons, some of which include democratic and scientific topics. This study was performed in order to identify topics in astronomy and science in general that may not be well understood by the general public. Approximately 1,000 adults at a popular science museum in Philadelphia, PA completed True-False survey questions about basic astronomy concepts. The participants were also asked to provide their age, gender, and highest degree obtained. Although 93 +/- 0.8% of the participants correctly answered that scientists can calculate the age of the Earth, only 58 +/- 2% provided the correct response that scientists can calculate the age of the Universe. Some participants (30 +/- 1%) responded that scientists have found life on Mars. Females scored an average total score of 78 +/- 2%, whereas males scored an average 85 +/- 1%. Participants with an age of 56 and over had an average score of 78 +/- 4% compared to participants under the age of 56 that were found to have an average score of 82 +/- 2%. Lastly, participants' highest degree obtained scaled with number of correct responses, with graduate level degree earners providing the largest amount of correct responses and an average score of 86 +/- 2%."
"Being aware of the motivation problems observed in many scientific oriented careers, we present two experiences to expose to college students to environments, methodologies and discovery techniques addressing contemporary problems. This experiences are developed in two complementary contexts: an Introductory Physics course, where we motivated to physics students to participate in research activities, and a multidisciplinary hotbed of research oriented to advanced undergraduate students of Science and Engineering (that even produced three poster presentations in international conferences). Although these are preliminary results and require additional editions to get statistical significance, we consider they are encouraging results. On both contexts we observe an increase in the students motivation to orient their careers with emphasizing on research. In this work, besides the contextualization support for these experiences, we describe six specific activities to link our students to research areas, which we believe can be replicated on similar environments in other educational institutions."
"In July 5th 2014 an occultation of Mars by the Moon was visible in South America. Citizen scientists and professional astronomers in Colombia, Venezuela and Chile performed a set of simple observations of the phenomenon aimed to measure the speed of light and lunar distance. This initiative is part of the so called ""Aristarchus Campaign"", a citizen astronomy project aimed to reproduce observations and measurements made by astronomers of the past. Participants in the campaign used simple astronomical instruments (binoculars or small telescopes) and other electronic gadgets (cell-phones and digital cameras) to measure occultation times and to take high resolution videos and pictures. In this paper we describe the results of the Aristarchus Campaign. We compiled 9 sets of observations from sites separated by distances as large as 2,500 km. We achieve at measuring the speed of light in vacuum and lunar distance with uncertainties of few percent. The goal of the Aristarchus Campaigns is not to provide improved values of well-known astronomical and physical quantities, but to demonstrate how the public could be engaged in scientific endeavors using simple instrumentation and readily available technological devices. These initiatives could benefit amateur communities in developing countries increasing their awareness towards their actual capabilities for collaboratively obtaining useful astronomical data. This kind of exercises would prepare them for facing future and more advanced observational campaigns where their role could be crucial."
"We describe an undergraduate course where physics students are asked to conceive an outreach project of their own. The course alternates between the project conception and teachings about pedagogy and outreach, and ends in a public show. We describe its practical implementation and benefits. Through a student survey and an analysis of their projects, we discuss the merits and flaws of this ""learning-by-doing"" teaching approach for physics."
"We describe a project-based physics lab, which we proposed to third-year university students. Theses labs are based on new open-source low-cost equipment (Arduino microcontrollers and compatible sensors). Students are given complete autonomy: they develop their own experimental setup and study the physics topic of their choice. The goal of these projects is to let students discover the reality of experimental physics. Technical specifications of the acquisition material and case studies are presented for practical implementation in other universities."
"Many aspects of solar energy and policies to tackle the energy transition have been neglected. Even though the earth is plenty of sun energy, our planet is not plenty of resources to transform that energy into electricity. This is a case between many others where an strongly optimistic bias is shadowing the white elephant in the room."
"This contribution describes the concept, main structure and goals, and some highlighted outcomes, of the AstroCamp -- an international academic excellence program in the field of astronomy and physics created in 2012 and organized by Centro de Astrof\'{\i}sica da Universidade do Porto (CAUP) together with the Paredes de Coura municipality and several national and international partners."
"The AstroCamp is an academic excellence program in the field of astronomy and physics for students in the last 3 years of pre-university education, which often includes a course (or a significant part thereof) on Relativity. After an introduction to the principles, goals and structure of the camp, I describe the approach followed by camp lecturers (myself and others) for teaching Special and General Relativity, and some lessons learned and feedback from the students. I also provide some thoughts on the differences between the physics and mathematics secondary school curricula in Portugal and in other countries, and on how these curricula could be modernized."
An analysis of barriers to women's participation in physics education is presented. It is expected that in undergraduate physics the most common situation for a women is that she is cisgender and one of a numerical minority in the classroom. The effects of other intersectional identities are not considered. The analysis is based on evidence from the author's lived experience as a transgender woman who transitioned as an undergraduate and on evidence from the literature on the effects of gender differences in academic disciplines. It is expected that the teaching philosophy and practice in physics classrooms favours men over women and these gender dynamics are partially responsible for the under-representation of women in physics. These expectations require further research to be substantiated.
"Audio Universe: Tour of the Solar System is an audio-visual show for planetariums and flatscreen viewing. It is designed in collaboration with members of the blind and vision impaired (BVI) community, BVI specialist teachers and their pupils. It aims to be suitable for audiences with all sight levels by representing key concepts through sound and using a carefully constructed narration. We present results from 291 audience evaluations from online viewers and audience members of several planetarium showings in the UK and Italy. We find a strong appreciation from BVI and non-BVI audiences, with ~90% scoring 4 or 5 (out of 5) for both how useful and enjoyable the sounds are. We also present results from surveying planetariums and communication leaders known to have downloaded the show. We find international success for special events, for BVI audiences and for those with other special educational needs and disabilities (SEND; including sensory needs and learning difficulties). Feedback suggests this is due to its multi-sensory, clearly narrated, and low sensory load (calm) production. However, we also describe limitations identified during this evaluation exercise, including the show's limited incorporation into regular (non-special) planetarium programmes. This highlights an ongoing challenge of creating a fully inclusive planetarium experience."
"The present article is devoted to the influence of sediment layers on the process of tsunami generation. The main scope here is to demonstrate and especially quantify the effect of sedimentation on vertical displacements of the seabed due to an underwater earthquake. The fault is modelled as a Volterra-type dislocation in an elastic half-space. The elastodynamics equations are integrated with a finite element method. A comparison between two cases is performed. The first one corresponds to the classical situation of an elastic homogeneous and isotropic half-space, which is traditionally used for the generation of tsunamis. The second test case takes into account the presence of a sediment layer separating the oceanic column from the hard rock. Some important differences are revealed. We conjecture that deformations in the generation region may be amplified by sedimentary deposits, at least for some parameter values. The mechanism of amplification is studied through careful numerical simulations."
"We present a theoretical profile of the Lyman Beta line of atomic hydrogen perturbed by collisions with neutral hydrogen atoms and protons. We use a general unified theory in which the electric dipole moment varies during a collision. A collision-induced satellite appears on Lyman Beta, correlated to the B''\barB 1Sigma+u - X 1Sigma+g asymptotically forbidden transition of H_2. As a consequence, the appearance of the line wing between Lyman Alpha and Lyman Beta is shown to be sensitive to the relative abundance of hydrogen ions and neutral atoms, and thereby to provide a temperature diagnostic for stellar atmospheres and laboratory plasmas."
"Ion beam analysis techniques are among the most powerful tools for advanced material characterization. Despite their growing relevance in a widening number of fields, most ion beam analysis facilities still rely on the oldest accelerator technologies, with severe limitations in terms of portability and flexibility. In this work we thoroughly address the potential of superintense laser-driven proton sources for this application. We develop a complete analytical and numerical framework suitable to describe laser-driven ion beam analysis, exemplifying the approach for Proton Induced X-ray/Gamma-ray emission, a technique of widespread interest. This allows us to propose a realistic design for a compact, versatile ion beam analysis facility based on this novel concept. These results can pave the way for ground-breaking developments in the field of hadron-based advanced material characterization."
"Since the first vacuum tube (X-ray tube) was invented by Wilhelm R\""ontgen in Germany, after more than one hundred years of development, the average power density of the vacuum tube microwave source has reached the order of 108 [MW][GHz]2. In the high-power microwave field, the vacuum devices are still the mainstream microwave sources for applications such as scientific instruments, communications, radars, magnetic confinement fusion heating, microwave weapons, etc. The principles of microwave generation by vacuum tube microwave sources include Cherenkov or Smith-Purcell radiation, transition radiation, and Bremsstrahlung. In this paper, the vacuum tube microwave sources based on Cherenkov radiation were reviewed. Among them, the multi-wave Cherenkov generators can produce 15 GW output power in X-band. Cherenkov radiation vacuum tubes that can achieve continuous-wave operation include Traveling Wave Tubes and Magnetrons, with output power up to 1MW. Cherenkov radiation vacuum tubes that can generate frequencies of the order of 100 GHz and above include Traveling Wave Tubes, Backward Wave Oscillators, Magnetrons, Surface Wave Oscillators, Orotrons, etc."
"FEbeam is a compact field emission data processing interface with the capability to analyze the field emission cathode performance in an rf injector by extracting the field enhancement factor, local field, and effective emission area from the Fowler-Nordheim equations. It also has the capability of processing beam imaging micrographs using its sister software, FEpic. The current version of FEbeam was designed for the Argonne Cathode Teststand (ACT) of the Argonne Wakefield Accelerator facility switch yard. With slight modifications, FEbeam can work for any rf field emission injector. This software is open-source and can be found at https://github.com/schne525/FEbeam"
"Ocean ventilation is the integrated effect of various processes that propagate surface properties to the ocean interior. Its precise understanding is the prerequisite for addressing essential questions such as oxygen supply, the storage of anthropogenic carbon and the heat budget of the ocean. Currently employed observational methods to infer ventilation utilise transient tracers, i.e. tritium, SF$_6$, CFCs and the radioisotope $^{14}$C. However, their dating ranges are not suitable to resolve the dynamics of the deep ocean. The noble gas isotope $^{39}$Ar with a half-life of 269 years fills this gap. Its broad application has previously been hindered by its very low abundance, requiring at least 1000 litres of water for dating. Here we report on successful $^{39}$Ar dating with only 5 litres of water based on the atom-optical technique Atom Trap Trace Analysis. Our data reveal previously not quantifiable ventilation patterns in the Eastern Tropical North Atlantic, where we find that advection is more important for the ventilation of the intermediate depth range than previously assumed. This result implies faster ventilation than estimated in other studies and thus a significantly higher anthropogenic CO$_2$-storage. The demonstrated analytical capabilities now allow for a global collection of $^{39}$Ar data, which will have significant impact on our understanding of ocean ventilation."
"As a result of nuclear power plant accidents, large areas receive radioactive inputs of Cs-137. This cesium accumulates in herbs growing in such territories. The problem is whether the herbs contaminated by radiocesium may be used as a raw material for medicine. The answer depends on the amount of Cs-137 transfered from the contaminated raw material to the medicine. We have presented new results of the transfer of Cs-137 from contaminated Digitalis grandiflora Mill. and Convallaria majalis L. to medicine. We found that the extraction of Cs-137 depends strongly on the hydrophilicity of the solvent. For example 96.5%(vol.) ethyl alcohol extracts less Cs-137 (11.6%) than 40%(vol.) ethyl alcohol or pure water (66.2%). The solubility of the cardiac glycosides is inverse to the solubility of cesium, which may be of use in the technological processes for manufacturing ecologically pure herbal medicine."
"Computational fluid dynamics (CFD) models of blood flow in the left ventricle (LV) and aorta are important tools for analyzing the mechanistic links between myocardial deformation and flow patterns. Typically, the use of image-based kinematic CFD models prevails in applications such as predicting the acute response to interventions which alter LV afterload conditions. However, such models are limited in their ability to analyze any impacts upon LV load or key biomarkers known to be implicated in driving remodeling processes as LV function is not accounted for in a mechanistic sense.   This study addresses these limitations by reporting on progress made towards a novel electro-mechano-fluidic (EMF) model that represents the entire physics of LV electromechanics (EM) based on first principles. A biophysically detailed finite element (FE) model of LV EM was coupled with a FE-based CFD solver for moving domains using an arbitrary Eulerian-Lagrangian (ALE) formulation. Two clinical cases of patients suffering from aortic coarctations (CoA) were built and parameterized based on clinical data under pre-treatment conditions. For one patient case simulations under post-treatment conditions after geometric repair of CoA by a virtual stenting procedure were compared against pre-treatment results. Numerical stability of the approach was demonstrated by analyzing mesh quality and solver performance under the significantly large deformations of the LV blood pool. Further, computational tractability and compatibility with clinical time scales were investigated by performing strong scaling benchmarks up to 1536 compute cores. The overall cost of the entire workflow for building, fitting and executing EMF simulations was comparable to those reported for image-based kinematic models, suggesting that EMF models show potential of evolving into a viable clinical research tool."
"In traditional cochlear implant surgery, physical trauma may occur during electrode array insertion. Magnetic guidance of the electrode array has been proposed to mitigate this medical complication. After insertion, the guiding magnet attached to the tip of the electrode array must be detached via a heating process and removed. This heating process may, however, cause thermal trauma within the cochlea. In this study, a validated three-dimensional finite element heat transfer model of the human cochlea is applied to perform an intracochlear thermal analysis necessary to ensure the safety of the magnet removal phase. Specifically, the maximum safe input power density to detach the magnet is determined as a function of the boundary conditions, heating duration, cochlea size, implant electrode array radius and insertion depth, magnet size, and cochlear fluid. A dimensional analysis and numerical simulations reveal that the maximum safe input power density increases with increasing cochlea size and the radius of the electrode array, whereas it decreases with increasing electrode array insertion depth and magnet size. The best cochlear fluids from the thermal perspective are perilymph and a soap solution. Even for the worst case scenario in which the cochlear walls are assumed to be adiabatic except at the round window, the maximum safe input power density is larger than that required to melt 1 $\rm{mm^3}$ of paraffin bonding the magnet to the implant electrode array. By combining the outcome of this work with other aspects of the design of the magnetic insertion process, namely the magnetic guidance procedure and medical requirements, it will be possible to implement a thermally safe patient-specific surgical procedure."
"Purpose: To propose a theory for the differential tissue sparing of FLASH ultra high dose rates (UHDR) through inter-track reaction-diffusion mechanism. Methods: We calculate the time-evolution of particle track-structures using a system of coupled reaction-diffusion equations on a random network designed for molecular transport in porous and disordered media. The network is representative of the intra- and inter-cellular diffusion channels in tissues. Spatial cellular heterogeneities over the scale of track spacing have been constructed by incorporating random fluctuations in the connectivity among network sites. Results: We demonstrate the occurrence of phase separation among the tracks as the complexity in intra- and inter-cellular structural increases. At the weak limit of disorder, such as in water and normal tissue, neighboring tracks melt into each other and form a percolated network of nonreactive species. In contrast, at the strong limit of disorder, tracks evolve individually like isolated islands with negligible inter-track overlap. Thus, the spatio-temporal correlation among the chemical domains decreases as the inter-cellular complexity of the tissue increases (e.g. from normal tissue to fractal-type malignant tissue). Conclusions: The differential sparing of FLASH UHDR in normal and tumor tissue may be explained by differences in inter- and intra-cellular structural complexities between the tissue types. The structural complexities of cancerous cells prevent clustering and chemical interaction of tracks, whereas this interaction prevails and thus leads to sparing in normal tissue."
"A scheme to generate highly collimated $\gamma$-ray pulse is proposed for the production of muon and electron pairs in $\gamma-\gamma$ collider. The $\gamma$-ray pulse, with high conversion efficiency, can be produced as the result of electron phase-locked acceleration in longitudinal electric field through the interaction between an ultra-intense laser pulse and a narrow tube target. Numerical simulation shows that 18\% energy of a 10-PW laser pulse is transferred into the forward $\gamma$-rays in a divergence angle less than $ 3^\circ$. The $\gamma$-ray pulse is applied in $\gamma-\gamma$ collider, in which muon pairs can be produced and electron pairs can be enhanced by more than 3 orders of magnitude. This scheme, which could be realized with the coming 10PW class laser pulses, would allow the observation of a $\gamma-\gamma$ collider for electron and muon pairs in laboratory."
"In this paper we discuss a compact, laser-plasma-based scheme for the generation of positron beams suitable to be implemented in an all-optical setup. A laser-plasma-accelerated electron beam hits a solid target producing electron-positron pairs via bremsstrahlung. The back of the target serves as a plasma mirror to in-couple a laser pulse into a plasma stage located right after the mirror where the laser drives a plasma wave (or wakefield). By properly choosing the delay between the laser and the electron beam the positrons produced in the target can be trapped in the wakefield, where they are focused and accelerated during the transport, resulting in a collimated beam. This approach minimizes the ballistic propagation time and enhances the trapping efficiency. The system can be used as an injector of positron beams and has potential applications in the development of a future, compact, plasma-based electron-positron linear collider."
"An all-optical centimeter-scale laser-plasma positron accelerator is modeled to produce quasi-monoenergetic beams with tunable ultra-relativistic energies. A new principle elucidated here describes the trapping of divergent positrons that are part of a laser-driven electromagnetic shower with a large energy spread and their acceleration into a quasi-monoenergetic positron beam in a laser-driven plasma wave. Proof of this principle using analysis and Particle-In-Cell simulations demonstrates that, under limits defined here, existing lasers can accelerate hundreds of MeV pC quasi-monoenergetic positron bunches. By providing an affordable alternative to kilometer-scale radio-frequency accelerators, this compact positron accelerator opens up new avenues of research."
"The general status of neutrino physics are given. The history of the neutrino, starting from Pauli and Fermi, is presented. The phenomenological V-A theory of the weak interaction and the unified theory of the weak and electromagnetic interactions, the so-called Standard Model, are discussed. The problems of of neutrino masses, neutrino mixing, and neutrino oscillations are discussed in some details."
"This paper introduces the necessity and significance of the investigation of cultural heritage objects. The multi-technique method is useful for the study of cultural heritage objects, but a comprehensive analytical instrument is a better choice since it can guarantee that different types of information are always obtained from the same analytical point on the surface of cultural heritage objects, which may be crucial for some situations. Thus, the X-ray fluorescence (XRF)/X-ray diffraction (XRD) and X-ray fluorescence (XRF)/Raman spectroscopy (RS) comprehensive analytical instruments are more and more widely used to study cultural heritage objects. The two types of comprehensive analytical instruments are discussed in detail and the XRF/XRD instruments are further classified into different types on the basis of structure, type and number of detectors. A new comprehensive analytical instrument prototype that can perform XRF, XRD and RS measurements simultaneously has been successfully developed by our team and the preliminary application has shown the analysis performance and application potential. This overview contributes to better understand the research progress and development tendency of comprehensive analytical instruments for the study of cultural heritage objects. The new comprehensive instruments will make researchers obtain more valuable information on cultural heritage objects and further promote the study on cultural heritage objects."
"We give, for the first time, the English translation of a manuscript by Ettore Majorana, which probably corresponds to the text for a seminar delivered at the University of Naples in 1938, where he lectured on Theoretical Physics. Some passages reveal a physical interpretation of the Quantum Mechanics which anticipates of several years the Feynman approach in terms of path integrals, independently of the underlying mathematical formulation."
"The characterization of individual nanoparticles in a liquid constitutes a critical challenge for environmental, material, and biological sciences. To detect nanoparticles, electronic approaches are especially desirable owing to their compactness and lower costs. Indeed, for single-molecule and single-nanoparticle detection, resistive pulse sensing has advanced significantly during the last two decades. While resistive pulse sensing was widely used to obtain the geometric size information, impedimetric measurements to obtain dielectric signatures of nanoparticles have scarcely been reported. To explore this orthogonal sensing modality, we developed an impedimetric sensor based on a microwave resonator with a nanoscale sensing gap surrounding a nanopore. The approach of single nanoparticles near the sensing region and their translocation through the nanopore induced sudden changes in the impedance of the structure. The impedance changes in turn were picked up by the phase response of the microwave resonator. We worked with 100 nm and 50 nm polystyrene nanoparticles to observe single-particle events. Our current implementation was limited by the non-uniform electric field at the sensing region. The work provides a complementary sensing modality for nanoparticle characterization where the dielectric response, rather than the ionic current, determines the signal."
"We present a 27-days long common view measurement of an absolute cold atom gravimeter (CAG) and a relative iGrav superconducting gravimeter, which we use to calibrate the iGrav scale factor. This allowed us to push the CAG long-term stability down to the level of 0.5~nm.s$^{-2}$. We investigate the impact of the duration of the measurement on the uncertainty in the determination of the correlation factor and show that it is limited to about 3\textperthousand~by the coloured noise of our cold atom gravimeter. A 3-days long measurement session with an additional FG5X absolute gravimeter allows us to directly compare the calibration results obtained with two different absolute meters. Based on our analysis, we expect that with an improvement of its long term stability, the CAG will allow to calibrate the iGrav scale factor to better than the per mille level (1$\sigma$ level of confidence) after only one-day of concurrent measurements for maximum tidal amplitudes."
"We present a new outlook on the climate system thermodynamics, studying some of its macroscopic properties in terms of the 1st and 2nd laws of thermodynamics. We review and clarify the notion of efficiency of the climate system by constructing formally an equivalent Carnot engine with efficiency eta, and show how the Lorenz energy cycle can be framed in a macro-scale thermodynamic context. Then, by exploiting the 2nd law, we prove that the lower bound to the entropy production is eta times the integrated absolute value of the internal entropy fluctuations. An exergetic interpretation is also proposed. Finally, the controversial maximum entropy production principle is re-interpreted as requiring the joint optimization of heat transport and mechanical work production. These results provide new tools for climate change analysis and for climate models' validation."
"Using a recently developed formalism, we present an in-depth analysis of how the thermodynamics of the climate system varies with CO2 concentration by performing experiments with a simplified yet Earth-like climate model. We find that, in addition to the globally averaged surface temperature, the intensity of the Lorenz energy cycle, the Carnot efficiency, the entropy production and the degree of irreversibility of the system are linear with the logarithm of the CO2 concentration. The generalized sensitivities proposed here suggest that the climate system becomes less efficient, more irreversible, and features higher entropy production as it becomes warmer."
"Until now the analysis of long wave runup on a plane beach has been focused on finding its maximum value, failing to capture the existence of resonant regimes. One-dimensional numerical simulations in the framework of the Nonlinear Shallow Water Equations (NSWE) are used to investigate the Boundary Value Problem (BVP) for plane and non-trivial beaches. Monochromatic waves, as well as virtual wave-gage recordings from real tsunami simulations, are used as forcing conditions to the BVP. Resonant phenomena between the incident wavelength and the beach slope are found to occur, which result in enhanced runup of non-leading waves. The evolution of energy reveals the existence of a quasi-periodic state for the case of sinusoidal waves, the energy level of which, as well as the time required to reach that state, depend on the incident wavelength for a given beach slope. Dispersion is found to slightly reduce the value of maximum runup, but not to change the overall picture. Runup amplification occurs for both leading elevation and depression waves."
"The primary emphasis of this study has been to explain how modifying a cake recipe by changing either the dimensions of the cake or the amount of cake batter alters the baking time. Restricting our consideration to the genoise, one of the basic cakes of classic French cuisine, we have obtained a semi-empirical formula for its baking time as a function of oven temperature, initial temperature of the cake batter, and dimensions of the unbaked cake. The formula, which is based on the Diffusion equation, has three adjustable parameters whose values are estimated from data obtained by baking genoises in cylindrical pans of various diameters. The resulting formula for the baking time exhibits the scaling behavior typical of diffusion processes, i.e. the baking time is proportional to the (characteristic length scale)^2 of the cake. It also takes account of evaporation of moisture at the top surface of the cake, which appears to be a dominant factor affecting the baking time of a cake. In solving this problem we have obtained solutions of the Diffusion equation which are interpreted naturally and straightforwardly in the context of heat transfer; however, when interpreted in the context of the Schrodinger equation, they are somewhat peculiar. The solutions describe a system whose mass assumes different values in two different regions of space. Furthermore, the solutions exhibit characteristics similar to the evanescent modes associated with light waves propagating in a wave guide. When we consider the Schrodinger equation as a non-relativistic limit of the Klein-Gordon equation so that it includes a mass term, these are no longer solutions."
"We present an analysis on the geometrical place formed by the set of maxima of the trajectories of a projectile launched in a media with linear drag. Such a place, the locus of apexes, is written in term of the Lambert $W$ function in polar coordinates, confirming the special role played by this function in the problem. In order to characterize the locus, a study of its curvature is presented in two parameterizations, in terms of the launch angle and in the polar one. The angles of maximum curvature are compared with other important angles in the projectile problem. As an addendum, we find that the synchronous curve in this problem is a circle as in the drag-free case."
"The Principle of Least Action has evolved and established itself as the most basic law of physics. This allows us to see how this fundamental law of nature determines the development of the system towards states with less action, i.e., organized states. A system undergoing a natural process is formulated as a game that tends to organize the system in the least possible time. Also, other concepts of game theory are related to their profound physical counterparts. Although no fundamentally new findings are provided, it is quite interesting to see certain important properties of a complex system and their far-reaching consequences."
"The Ingenuity helicopter test flights on Mars in April 2021 marked the first time a powered aircraft has flown on another world. Students who have access to model helicopters and drones may wonder, how well could those hover on the Red Planet? The answer can be found in this journal [The Physics Teacher], using appropriate scaling of the surface gravity and atmospheric density to Martian values."
"We describe an experiment involving a mass oscillating in a viscous fluid and analyze viscous damping of harmonic motion. The mechanical oscillator is tracked using a simple webcam and an image processing algorithm records the position of the geometrical center as a function of time. Interesting information can be extracted from the displacement-time graphs, in particular for the underdamped case. For example, we use these oscillations to determine the viscosity of the fluid. Our mean value of 1.08 \pm 0.07 mPa s for distilled water is in good agreement with the accepted value at 20\circC. This experiment has been successfully employed in the freshman lab setting."
"Resonance is an ubiquitous phenomenon present in many systems. In particular, air resonance in cavities was studied by Hermann von Helmholtz in the 1850s. Originally used as acoustic filters, Helmholtz resonators are rigid-wall cavities which reverberate at given fixed frequencies. An adjustable type of resonator is the so-called universal Helmholtz resonator, a device consisting of two sliding cylinders capable of producing sounds over a continuous range of frequencies. Here we propose a simple experiment using a smartphone and normal bottle of tea, with a nearly uniform cylindrical section, which, filled with water at different levels, mimics a universal Helmholtz resonator. Blowing over the bottle, different sounds are produced. Taking advantage of the great processing capacity of smartphones, sound spectra together with frequencies of resonance are obtained in real time."
"The recent advent of deep artificial neural networks has resulted in a dramatic increase in performance for object classification and detection. While pre-trained with everyday objects, we find that a state-of-the-art object detection architecture can very efficiently be fine-tuned to work on a variety of object detection tasks in a high-power laser laboratory. In this manuscript, three exemplary applications are presented. We show that the plasma waves in a laser-plasma accelerator can be detected and located on the optical shadowgrams. The plasma wavelength and plasma density are estimated accordingly. Furthermore, we present the detection of all the peaks in an electron energy spectrum of the accelerated electron beam, and the beam charge of each peak is estimated accordingly. Lastly, we demonstrate the detection of optical damage in a high-power laser system. The reliability of the object detector is demonstrated over one thousand laser shots in each application. Our study shows that deep object detection networks are suitable to assist online and offline experiment analysis, even with small training sets. We believe that the presented methodology is adaptable yet robust, and we encourage further applications in high-power laser facilities regarding the control and diagnostic tools, especially for those involving image data."
"In this article, I present an elementary introduction to the theory of gravitational waves. This article is meant for students who have had an exposure to general relativity, but, results from general relativity used in the main discussion have been derived and discussed in the appendices. The weak gravitational field approximation is first considered and the linearized Einstein's equations are obtained. We discuss the plane wave solutions to these equations and consider the transverse-traceless (TT) gauge. We then discuss the motion of test particles in the presence of a gravitational wave and their polarization. The method of Green's functions is applied to obtain the solutions to the linearized field equations in presence of a nonrelativistic, isolated source."
"The discovery of quantum mechanics at the beginning of the last century led to a revolution of the physical world view. Modern experiments, made possible by new techniques on the border of the classical and the quantum regimes offer new insights and better understanding of the quantum world and have impact on technical development. Therefore it seems important that students gain appreciation of the principles of quantum mechanics. A suitable way seems be the treatment of the EPR-experiment at a prominent place."
"An experimental study of strings, woodwinds (organ pipe, flute, clarinet, saxophone and recorder), and the voice was undertaken to illustrate the basic principles of sound production in music instruments. The setup used is simple and consists of common laboratory equipment. Although the canonical examples (standing wave on a string, in an open and closed pipe) are easily reproduced, they fail to explain the majority of the measurements. The reasons for these deviations are outlined and discussed."
"I use simple thermodynamic reasoning to argue that at temperatures of order a trillion kelvin, QCD, the theory which describes strongly interacting particles such as protons and neutrons under normal conditions, undergoes a phase transition to a plasma of more elementary constituents called quarks and gluons. I review what is known about the plasma phase both from theoretical calculations and from experiments involving the collisions of large atomic nuclei moving at relativistic speeds. Finally I consider the behaviour of nuclear material under conditions of extreme density, and discuss possible exotic phenomena such as quark matter and color superconductivity."
"This two-part article considers certain fundamental symmetries of nature, namely the discrete symmetries of parity (P), charge conjugation (C) and time reversal (T), and their possible violation. Recent experimental results are discussed in some depth. In the first part of this article we present a general background and discuss parity violation."
"Atmospheric refraction is responsible for the bending of light-rays in the atmosphere. It is a result of the continuous decrease in the refractive index of the air as a function of altitude. A well-known consequence of this phenomenon is the apparently elliptic shape of the setting or rising Sun (or full-Moon). In the present paper we systematically investigate this phenomenon in a standard atmosphere. Theoretical and numerical calculations are compared with experimental data. The asymmetric rim of the Sun is computed as a function of its inclination angle, observational height and meteorological conditions characterized by pressure, temperature and lapse-rate. We reveal and illustrate some extreme and highly unusual situations."
"""Bernoulli"" levitation is the basis of many popular counter-intuitive physics demonstrations. However, few of these lend themselves to a quantitative description without recourse to computational fluid dynamics. Levitation of a flat plate is the exception, and we present here a straightforward analysis which illustrates several principles of fluid mechanics in a pedagogically useful way."
"The 'elastic capacitor' (EC) model was first introduced in studies of lipid bilayers (the major components of biological membranes). This electro-elastic model accounted for the compression of a membrane under applied voltage and allowed obtaining information about the membrane's elastic properties from the measurements of its capacitance. Later on, ECs were used to analyze the electrical breakdown of biological membranes. The EC model was also helpful in studies of electric double layers in various electrified interfaces (of which the electrode/ electrolyte interface is the most common example). This comparatively simple model, which analysis requires only high-school physics, has a close relationship to some real-life problems in physics, chemistry and biology. I hope that both teachers and students will find its discussion interesting, challenging and instructive."
The traceroute utility on any computer connected to the Internet can be used to record the roundtrip time for small Internet packets between major Internet traffic hubs. Some of the routes include transmission over transoceanic fiber optic cable. We report on traceroute's use by students to quickly and easily estimate the size of the Earth. This is an inexpensive way to involve introductory physics students in a hands-on use of scientific notation and to teach them about systematics in data.
"The reality of neutrino oscillations has not really sunk in yet. The phenomenon presents us with purely quantum mechanical effects over macroscopic time and distance scales (milliseconds and 1000s of km). In order to help with the pedagogical difficulties this poses, I attempt here to present the physics in words and pictures rather than math. No disrespect is implied by the title; I am merely borrowing a term used by a popular series of self-help books."
"Over the past year and a half we have developed an innovative approach to the teaching of `Physical Science', a general education course typically found in the curricula of nearly every college and university. The new approach uses popular movies to illustrate the principles of physical science, analyzing individual scenes against the background of the fundamental physical laws. The impact of being able to understand why, in reality, the scene could or could not have occurred as depicted in the film, what the director got right and what he got wrong, has excited student interest enormously in a course that, when taught in the traditional mode, is usually considered to be `too hard and boring'. The performance of students on exams reflected the increased attention to and retention of basic physical concepts, a result that was a primary goal of the `Physics in Films' approach. Following the first offering of the revitalization of the Physical Science course, in which action and sci-fi films were the primary source of the scene clips used in class, the instructors have demonstrated the versatility of the approach by building variations of the course around other genres, as well --`Physics in Films: Superheroes' and `Physics in Films: Pseudoscience'. A parallel approach to the general education course in astronomy is currently being discussed; many others are in our thoughts."
"In order to understand the physics phenomea on the fundamental aspects, the software simulations are a good exercise to succed in this way. Some work of heat transport and molecular physics laboratory are studied in a comparative mode experiments and software applications. The main ojbects structure ans some program interfaces are presented."
A simple methodical derivation for Landauer quantization of the conductivity is derived as a simple consequence of the Bohr quantization laws. The level of explanation corresponds to high-school level of Physics education and can be used as popular lecture for students. The purpose of the work is to introduce students in an achievement of nano-technology which is relevant to the future electronics. Using only the fundamental laws of quantization students can understand a contemporary experimental research and to follow future development in the field.
"It has been found that some text-books show LC-oscillators that may not work as assumed. Thus, the typical example showing a LC-oscillator driven by a voltage operational-amplifier is simply wrong. The difficulty stems from the fact that such oscillators are normally built to work with transconductance- not with voltage-amplifiers. Such a difficulty however, can be readily solved by connecting a resistor in series with the so-called frequency-determining network."
"A popular problem asks for the equilibrium separation between two identical (mutually repelling) charges suspended by strings fastened to a common point. We slightly modify this problem by considering two opposite (mutually attracting) charges and adding a finite separation between the suspension points. The discussion leading to the solution introduces important physical phenomena including ""catastrophic"" behavior and hysterisis."
"I survey motivations for education and outreach initiatives in the American context and explore the value of communicating physics for physicists and for the wider society. I describe the roles of large institutions, professional organizations, and funding agencies and cite some individual actions, local activities, and coordinated national programs. I note the emergence of transnational enterprises--not only to carry out research, but to communicate physics. A brief appendix collects some useful internet resources."

# Scientific Abstract Classification Using Bert and LR Classifiers
# 1. Introduction 1.1 Background
We live in an age of rapid information growth, and with it the amount of textual data. Text classification technology can be very important when you need to help organize and manage this data. Especially in the field of scientific research, it can help students or researchers quickly sift through a large number of papers to select relevant literature. The objective of this project is to build or use a pre-trained model to make a classification system to effectively classify scientific abstracts into the corresponding subject area.

## 1.2 Text Classification
Text classification is important in natural language processing (NLP), where text data can be grouped into one or more categories. This is crucial when you encounter large amounts of text data and want to manage and retrieve it. Text classification is very versatile and has applications in many fields, such as spam detection, sentiment analysis, and subject recognition.

## 1.3 Project Objectives
The goal of this project is to develop a text classification model that can automatically classify scientific abstracts into three specific fields: physics, mathematics and economics. These three fields will be similar to some extent and basically involve mathematical content, so it is very important to classify them accurately.

# 2. Methodology

## 2.1 Data Acquisition and Preprocessing 2.1.1 Data Collection:
The project uses the API provided by arxiv.org for web scraping. Data is mainly collected from arxiv. Abstract data were gathered primarily from three fields: physics, mathematics, and economics. At least 300 abstracts were collected from each field to ensure the representativeness and balance of the data.

### 2.1.2 Preprocessing of Text Data
The objective of text data preprocessing is to clean and prepare the raw text data, reducing irrelevant elements that might impact the accuracy of model training. This step is crucial for subsequent feature extraction and model training. Since I implemented two models for text classification, their preprocessing steps are different.
First, I'll introduce the preprocessing for the logistic regression model. Using the word_tokenize function from the Natural Language Toolkit (NLTK) library, each abstract text was
split into words (tokens). This step transforms the text into a word-level representation that can be processed individually. To ensure consistency in text data, all words were converted to lowercase. The case of words in text often does not affect their meaning but could lead the model to treat the same word in different cases as different entities. Converting text to a uniform lowercase format improves model consistency. Additionally, to clean the text from noise, punctuation and non- alphabetical characters were removed. Common stop words, which frequently appear in text but usually do not contribute much to text classification tasks, like â€œa,â€ â€œan,â€ â€œthe,â€ etc., were eliminated using the stop word collection provided by the NLTK library. Removing stop words can reduce the dimension of the text data while preserving key words.
In the data preprocessing of BERT models, additional steps are needed to adapt to the input format requirements of BERT models. First, the Tokenizer of the BERT model is used to split each summary text into tokens. This step is to convert text data into a form that the BERT model can understand. Add special tags at the beginning and end of each text sequence, such as CLS for sequence beginning and SEP for sequence separation. These labels are also part of the BERT model input. In order to satisfy the input length limit of BERT model, the text sequence is truncated or filled. Typically, the input length of a BERT model is set to 512 tags. Finally, an attention mask is generated to distinguish the actual text from the padding. This helps BERT models handle text of different lengths correctly.

## 2.2 Feature Extraction: TF-IDF
TF-IDF (Term Frequency-Inverse Document Frequency) is a common weighting technique used in information retrieval and text mining. It is a statistical method to evaluate the importance of a word to a document in a collection or a corpus. The importance of a word increases proportionally with its frequency in the document but is offset by its frequency in the corpus.
TF-IDF is composed of two parts: Term Frequency (TF) and Inverse Document Frequency (IDF). TF refers to the frequency with which a term (keyword) appears in a text. It is a measure of how often a word appears in a document.
ğ‘‡ğ¹ = ğ‘ğ‘¢ğ‘šğ‘ğ‘’ğ‘Ÿ ğ‘œğ‘“ ğ‘¡ğ‘–ğ‘šğ‘’ğ‘  ğ‘ ğ‘¡ğ‘’ğ‘Ÿğ‘š ğ‘ğ‘ğ‘ğ‘’ğ‘ğ‘Ÿğ‘  ğ‘–ğ‘› ğ‘ ğ‘‘ğ‘œğ‘ğ‘¢ğ‘šğ‘’ğ‘›ğ‘¡ ğ‘‡ğ‘œğ‘¡ğ‘ğ‘™ ğ‘›ğ‘¢ğ‘šğ‘ğ‘’ğ‘Ÿ ğ‘œğ‘“ ğ‘¡ğ‘’ğ‘Ÿğ‘šğ‘  ğ‘–ğ‘› ğ‘¡hğ‘’ ğ‘‘ğ‘œğ‘ğ‘¢ğ‘šğ‘’ğ‘›ğ‘¡
IDF is calculated by dividing the total number of documents by the number of documents containing the term, and then taking the logarithm of that quotient. The more unique the term is (i.e., the fewer documents it appears in), the higher its IDF value will be, indicating good class- differentiation power.
By multiplying TF and IDF, we obtain the TF-IDF value of a word. The higher the TF-IDF value of a word in a document, the greater its importance in that document. Therefore, by
   ğ¼ğ·ğ¹ = ğ‘™ğ‘œğ‘” ğ‘ğ‘¢ğ‘šğ‘ğ‘’ğ‘Ÿ ğ‘œğ‘“ ğ‘‘ğ‘œğ‘ğ‘¢ğ‘šğ‘’ğ‘›ğ‘¡ğ‘  ğ‘–ğ‘› ğ‘¡hğ‘’ ğ‘ğ‘œğ‘Ÿğ‘ğ‘¢ğ‘ 
ğ‘‡ğ‘œğ‘¡ğ‘ğ‘™ ğ‘›ğ‘¢ğ‘šğ‘ğ‘’ğ‘Ÿ ğ‘œğ‘“ ğ‘‘ğ‘œğ‘ğ‘¢ğ‘šğ‘’ğ‘›ğ‘¡ğ‘  ğ‘¡hğ‘ğ‘¡ ğ‘ğ‘œğ‘›ğ‘¡ğ‘ğ‘–ğ‘› ğ‘¡hğ‘’ ğ‘¡ğ‘’ğ‘Ÿğ‘š + 1
 
calculating the TF-IDF values of all words in a document and sorting them from high to low, the top-ranked words are considered the key terms of the document.
ğ‘»ğ‘­ âˆ’ ğ‘°ğ‘«ğ‘­ = ğ‘»ğ‘­ Ã— ğ‘°ğ‘«ğ‘­
The TF-IDF algorithm is straightforward to understand and implement. However, its simple structure does not consider the semantic information of words and cannot handle polysemy (multiple meanings for a single word) and synonymy (multiple words with the same meaning).

## 2.3 Model Selection

### 2.3.1 Detailed Explanation of the BERT Model
The architecture of the BERT model is a multi-layer Transformer network that utilizes a self-attention mechanism to understand the contextual relationships between words. Each layer of these Transformer layers is composed of two main sub-structures: a multi-head self-attention mechanism and a simple position-wise feed-forward network. In the multi-head self-attention mechanism of the transformation model, multiple different contextual representations of each word are learned simultaneously through several "attention heads." This allows the model to focus on the parts of the input sequence at the same time. By considering a large number of word relations at the same time, this method is very effective in grasping the various possible meanings and applications of words. The transformer layer also includes a position feedforward network, which is a dense neural network that can be refined from the output of the attention layer. In this network, the output is location specific but independent, allowing all locations within the same layer to be processed simultaneously and in parallel. This independence of position treatment is another feature that improves the efficiency of transformer models.

### 2.3.2 BERT Pre-training Process
What sets BERT apart from other models is its pre-training approach, which uses two new methods to train the model: Mask Language Model (MLM) and Next Sentence Prediction (NSP).
In the MLM task, there is a randomly blocked word, and the model is mainly asked to fill in this word, which forces BERT to learn to use the surrounding bidirectional context to predict the missing information. In this way, the model not only learns one-way context from both directions, left to right or right to left, but also learns to integrate context from both directions, resulting in a richer and deeper understanding of the language. The NSP task further trains the model to understand the logical relationships between sentences and enhances the model's grasp of coherence and narrative flow by predicting whether one sentence is a follow-up to another. These pre-training tasks are typically performed on large corpora containing multiple texts and rich linguistic structures, such as Wikipedia, which allows BERT to acquire a broad vocabulary and grammar knowledge during the pre-training phase. After pre-training, BERT translates this deep language understanding into a specific application with task-specific fine-tuning, demonstrating its powerful performance and flexibility in a variety of natural language processing tasks.
 
### 2.3.3 Fine-Tuning of BERT
The fine-tuning of the bidirectional encoder representation is important in natural language processing (NLP) to adapt the universal pretrained BERT model to a specific NLP task. This includes steps such as data preparation, loading pre-trained models, building task-specific structures, defining loss functions, fine-tuning training, hyperparameter tuning, and evaluating models.
In the Transformer architecture, BERT can effectively model multiple downstream tasks through a self-focused mechanism, which also simplifies the fine-tuning process. Previously, models such as those proposed by Parikh et al. (2016) and Seo et al. (2017) required independently encoded text pairs before applying bidirectional cross-attention. BERT innovatively uses the self- attention mechanism to process both steps simultaneously, naturally integrating two-way cross- attention between two sentences by encoding linked text pairs. To fine-tune, we simply embed task-specific inputs and outputs into BERT, and then make end-to-end adjustments to all parameters such as learning rate, batch size, and training cycles. For example, at the input end, sentences A and B can represent sentence pairs in paraphrasing tasks, hypothesis and premise in entailment tasks, question and paragraph in question-answering tasks, and single texts in text classification or sequence tagging tasks. At the output end, token-level representations are used for sequence tagging or question-answering tasks, while representations of the [CLS] token are used for classification tasks like entailment judgment or sentiment analysis. Compared to pre- training, fine-tuning is less costly, and all experimental results can be replicated in one hour using a Cloud TPU or in a few hours using a GPU, all based on the same pre-trained model.
2.3.4 Detailed Explanation of Logistic Regression:
Logistic Regression (LR), despite the word "regression" in its name, is actually a classification model widely used across various fields. It transforms the result of a linear regression model through a non-linear sigmoid function, producing values in the range of [0, 1]. By setting a threshold at 0.5, the model can achieve binary classification based on whether the output value is above or below this threshold. The model is built on the sigmoid function, whose formula is as
      follows:
ğ‘”(ğ‘§) = 1 1+ğ‘’!"
 
Sigmoid function graph:
Logistic regression is an effective choice for literature abstract classification because logistic regression is very effective when dealing with linear separable data, and the model is simple and easy to implement and interpret. Logistic regression can output results in the form of probabilities rather than just 0,1 decisions, which makes classification results more explanatory. Combined with TF-IDF features, logistic regression can process and classify text data well.
2.4 K-Fold Cross-Validation
K-Fold cross-validation is a technique that can be commonly used in statistical analysis and machine learning to evaluate a model's ability to predict new data. The main aim is to ensure the reliability of the assessment by dividing the data set into multiple groups (often called "folds") so that each group has the opportunity to act as a test set to avoid overfitting the model. In machine learning projects, there is usually not a lot of data available to train and test models. K-Fold cross- validation ensures a comprehensive evaluation of model performance by rotating different subsets of data as training and test data. This approach helps reveal the volatility and stability of the model when dealing with different data, providing a more comprehensive measure of performance. By averaging the performance indicators of each Fold, the comprehensive performance of the model on the entire data set can be obtained. In addition, analyzing the fluctuations of these indicators between different multiples can help us understand the robustness of the model. For example, if the performance of the model is similar across all folds, it indicates that the model has high robustness. If the performance fluctuates significantly between different folds, it may indicate an overfitting problem or an imbalance in the data set itself.
